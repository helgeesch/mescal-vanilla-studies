{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the MESCAL Vanilla Documentation","text":"<p>This documentation provides a detailed overview of the mescal ecosystem,  including tutorials, examples, and a complete API reference.</p>"},{"location":"mescal-package-documentation/api_reference/datasets/","title":"Dataset References","text":""},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset","title":"dataset","text":""},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset.Dataset","title":"Dataset","text":"<p>               Bases: <code>Generic[DatasetConfigType, FlagType, FlagIndexType]</code>, <code>ABC</code></p> <p>Abstract base class for all datasets in the MESCAL framework.</p> <p>The Dataset class provides the fundamental interface for data access and manipulation in MESCAL. It implements the core principle \"Everything is a Dataset\" where individual scenarios, collections of scenarios, and scenario comparisons all share the same unified interface.</p> Key Features <ul> <li>Unified <code>.fetch(flag)</code> interface for data access</li> <li>Attribute management for scenario metadata</li> <li>KPI calculation integration</li> <li>Database caching support</li> <li>Dot notation fetching via <code>dotfetch</code> property</li> <li>Type-safe generic implementation</li> </ul> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>DatasetConfigType</code> <p>Configuration class for dataset behavior</p> required <code>FlagType</code> <p>Type used for data flag identification (typically str)</p> required <code>FlagIndexType</code> <p>Flag index implementation for flag mapping</p> required <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Human-readable identifier for the dataset</p> <code>kpi_collection</code> <code>KPICollection</code> <p>Collection of KPIs associated with this dataset</p> <code>dotfetch</code> <code>_DotNotationFetcher</code> <p>Enables dot notation data access</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Basic usage pattern\n&gt;&gt;&gt; data = dataset.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; flags = dataset.accepted_flags\n&gt;&gt;&gt; if dataset.flag_is_accepted('generators_t.p'):\n...     gen_data = dataset.fetch('generators_t.p')\n</code></pre> Source code in <code>submodules/mescal/mescal/datasets/dataset.py</code> <pre><code>class Dataset(Generic[DatasetConfigType, FlagType, FlagIndexType], ABC):\n    \"\"\"\n    Abstract base class for all datasets in the MESCAL framework.\n\n    The Dataset class provides the fundamental interface for data access and manipulation\n    in MESCAL. It implements the core principle \"Everything is a Dataset\" where individual\n    scenarios, collections of scenarios, and scenario comparisons all share the same\n    unified interface.\n\n    Key Features:\n        - Unified `.fetch(flag)` interface for data access\n        - Attribute management for scenario metadata\n        - KPI calculation integration\n        - Database caching support\n        - Dot notation fetching via `dotfetch` property\n        - Type-safe generic implementation\n\n    Type Parameters:\n        DatasetConfigType: Configuration class for dataset behavior\n        FlagType: Type used for data flag identification (typically str)\n        FlagIndexType: Flag index implementation for flag mapping\n\n    Attributes:\n        name (str): Human-readable identifier for the dataset\n        kpi_collection (KPICollection): Collection of KPIs associated with this dataset\n        dotfetch (_DotNotationFetcher): Enables dot notation data access\n\n    Example:\n\n        &gt;&gt;&gt; # Basic usage pattern\n        &gt;&gt;&gt; data = dataset.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; flags = dataset.accepted_flags\n        &gt;&gt;&gt; if dataset.flag_is_accepted('generators_t.p'):\n        ...     gen_data = dataset.fetch('generators_t.p')\n    \"\"\"\n\n    def __init__(\n            self,\n            name: str = None,\n            parent_dataset: Dataset = None,\n            flag_index: FlagIndexType = None,\n            attributes: dict = None,\n            database: Database = None,\n            config: DatasetConfigType = None\n    ):\n        \"\"\"\n        Initialize a new Dataset instance.\n\n        Args:\n            name: Human-readable identifier. If None, auto-generates from class name\n            parent_dataset: Optional parent dataset for hierarchical relationships\n            flag_index: Index for mapping and validating data flags\n            attributes: Dictionary of metadata attributes for the dataset\n            database: Optional database for caching expensive computations\n            config: Configuration object controlling dataset behavior\n        \"\"\"\n        self.name = name or f'{self.__class__.__name__}_{str(id(self))}'\n        self._flag_index = flag_index or EmptyFlagIndex()\n        self._parent_dataset = parent_dataset\n        self._attributes: dict = attributes or dict()\n        self._database = database\n        self._config = config\n        self.dotfetch = _DotNotationFetcher(self)\n\n        from mescal.kpis.kpi_collection import KPICollection\n        self.kpi_collection: KPICollection = KPICollection()\n\n    @property\n    def flag_index(self) -&gt; FlagIndexType:\n        if isinstance(self._flag_index, EmptyFlagIndex):\n            logger.info(\n                f\"Dataset {self.name}: \"\n                \"You're trying to use functionality of the FlagIndex but didn't define one. \"\n                \"The current FlagIndex in use is empty. \"\n                \"Make sure to set a flag_index in case you want to use full functionality of the flag_index.\"\n            )\n        return self._flag_index\n\n    @property\n    def database(self) -&gt; Database | None:\n        return self._database\n\n    def add_kpis(self, kpis: Iterable[KPI | KPIFactory | Type[KPI]]):\n        \"\"\"\n        Add multiple KPIs to this dataset's KPI collection.\n\n        Args:\n            kpis: Iterable of KPI instances, factories, or classes to add\n        \"\"\"\n        for kpi in kpis:\n            self.add_kpi(kpi)\n\n    def add_kpi(self, kpi: KPI | KPIFactory | Type[KPI]):\n        \"\"\"\n        Add a single KPI to this dataset's KPI collection.\n\n        Automatically handles different KPI input types by converting factories\n        and classes to KPI instances.\n\n        Args:\n            kpi: KPI instance, factory, or class to add\n        \"\"\"\n        from mescal.kpis.kpi_base import KPI\n        from mescal.kpis.kpis_from_aggregations import KPIFactory\n        if isinstance(kpi, KPIFactory):\n            kpi = kpi.get_kpi(self)\n        elif isinstance(kpi, type) and issubclass(kpi, KPI):\n            kpi = kpi.from_factory(self)\n        self.kpi_collection.add_kpi(kpi)\n\n    def clear_kpi_collection(self):\n        from mescal.kpis import KPICollection\n        self.kpi_collection = KPICollection()\n\n    @property\n    def attributes(self) -&gt; dict:\n        return self._attributes\n\n    def get_attributes_series(self) -&gt; pd.Series:\n        att_series = pd.Series(self.attributes, name=self.name)\n        return att_series\n\n    def set_attributes(self, **kwargs):\n        for key, value in kwargs.items():\n            if not isinstance(key, str):\n                raise TypeError(f'Attribute keys must be of type str. Your key {key} is of type {type(key)}.')\n            if not isinstance(value, (bool, int, float, str)):\n                raise TypeError(\n                    f'Attribute values must be of type (bool, int, flaot, str). '\n                    f'Your value for {key} ({value}) is of type {type(value)}.'\n                )\n            self._attributes[key] = value\n\n    @property\n    def parent_dataset(self) -&gt; 'DatasetLinkCollection':\n        if self._parent_dataset is None:\n            raise RuntimeError(f\"Parent dataset called without / before assignment.\")\n        return self._parent_dataset\n\n    @parent_dataset.setter\n    def parent_dataset(self, parent_dataset: 'DatasetLinkCollection'):\n        from mescal.datasets.dataset_collection import DatasetLinkCollection\n        if not isinstance(parent_dataset, DatasetLinkCollection):\n            raise TypeError(f\"Parent parent_dataset must be of type {DatasetLinkCollection.__name__}\")\n        self._parent_dataset = parent_dataset\n\n    @property\n    @abstractmethod\n    def accepted_flags(self) -&gt; set[FlagType]:\n        \"\"\"\n        Set of all flags accepted by this dataset.\n\n        This abstract property must be implemented by all concrete dataset classes\n        to define which data flags can be fetched from the dataset.\n\n        Returns:\n            Set of flags that can be used with the fetch() method\n\n        Example:\n\n            &gt;&gt;&gt; print(dataset.accepted_flags)\n                {'buses', 'buses_t.marginal_price', 'generators', 'generators_t.p', ...}\n        \"\"\"\n        return set()\n\n    def get_accepted_flags_containing_x(self, x: str, match_case: bool = False) -&gt; set[FlagType]:\n        \"\"\"\n        Find all accepted flags containing a specific substring.\n\n        Useful for discovering related data flags or filtering flags by category.\n\n        Args:\n            x: Substring to search for in flag names\n            match_case: If True, performs case-sensitive search. Default is False.\n\n        Returns:\n            Set of accepted flags containing the substring\n\n        Example:\n\n            &gt;&gt;&gt; ds = PyPSADataset()\n            &gt;&gt;&gt; ds.get_accepted_flags_containing_x('generators')\n                {'generators', 'generators_t.p', 'generators_t.efficiency', ...}\n            &gt;&gt;&gt; ds.get_accepted_flags_containing_x('BUSES', match_case=True)\n                set()  # Empty because case doesn't match\n        \"\"\"\n        if match_case:\n            return {f for f in self.accepted_flags if x in str(f)}\n        x_lower = x.lower()\n        return {f for f in self.accepted_flags if x_lower in str(f).lower()}\n\n    def flag_is_accepted(self, flag: FlagType) -&gt; bool:\n        \"\"\"\n        Boolean check whether a flag is accepted by the Dataset.\n\n        This method can be optionally overridden in any child-class\n        in case you want to follow logic instead of the explicit set of accepted_flags.\n        \"\"\"\n        return flag in self.accepted_flags\n\n    @flag_must_be_accepted\n    def required_flags_for_flag(self, flag: FlagType) -&gt; set[FlagType]:\n        return self._required_flags_for_flag(flag)\n\n    @abstractmethod\n    def _required_flags_for_flag(self, flag: FlagType) -&gt; set[FlagType]:\n        return set()\n\n    @flag_must_be_accepted\n    def fetch(self, flag: FlagType, config: dict | DatasetConfigType = None, **kwargs) -&gt; pd.Series | pd.DataFrame:\n        \"\"\"\n        Fetch data associated with a specific flag.\n\n        This is the primary method for data access in MESCAL datasets. It provides\n        a unified interface for retrieving data regardless of the underlying source\n        or dataset type. The method includes automatic caching, post-processing,\n        and configuration management.\n\n        Args:\n            flag: Data identifier flag (must be in accepted_flags)\n            config: Optional configuration to override dataset defaults.\n                   Can be a dict or DatasetConfig instance.\n            **kwargs: Additional keyword arguments passed to the underlying\n                     data fetching implementation\n\n        Returns:\n            DataFrame or Series containing the requested data\n\n        Raises:\n            ValueError: If the flag is not accepted by this dataset\n\n        Examples:\n\n            &gt;&gt;&gt; # Basic usage\n            &gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price')\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With custom configuration\n            &gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price', config={'use_database': False})\n        \"\"\"\n        effective_config = self._prepare_config(config)\n        use_database = self._database is not None and effective_config.use_database\n\n        if use_database:\n            if self._database.key_is_up_to_date(self, flag, config=effective_config, **kwargs):\n                return self._database.get(self, flag, config=effective_config, **kwargs)\n\n        raw_data = self._fetch(flag, effective_config, **kwargs)\n        processed_data = self._post_process_data(raw_data, flag, effective_config)\n\n        if use_database:\n            self._database.set(self, flag, config=effective_config, value=processed_data, **kwargs)\n\n        return processed_data.copy()\n\n    def _post_process_data(\n            self,\n            data: pd.Series | pd.DataFrame,\n            flag: FlagType,\n            config: DatasetConfigType\n    ) -&gt; pd.Series | pd.DataFrame:\n        if config.remove_duplicate_indices and any(data.index.duplicated()):\n            logger.info(\n                f'For some reason your data-set {self.name} returns an object with duplicate indices for flag {flag}.\\n'\n                f'We manually remove duplicate indices. Please make sure your data importer / converter is set up '\n                f'appropriately and that your raw data does not contain duplicate indices. \\n'\n                f'We will keep the first element of every duplicated index.'\n            )\n            data = data.loc[~data.index.duplicated()]\n        if config.auto_sort_datetime_index and isinstance(data.index, pd.DatetimeIndex):\n            data = data.sort_index()\n        return data\n\n    def _prepare_config(self, config: dict | DatasetConfigType = None) -&gt; DatasetConfigType:\n        if config is None:\n            return self.instance_config\n\n        if isinstance(config, dict):\n            temp_config = self.get_config_type()()\n            temp_config.__dict__.update(config)\n            return self.instance_config.merge(temp_config)\n\n        from mescal.datasets.dataset_config import DatasetConfig\n        if isinstance(config, DatasetConfig):\n            return self.instance_config.merge(config)\n\n        raise TypeError(f\"Config must be dict or {DatasetConfig.__name__}, got {type(config)}\")\n\n    @abstractmethod\n    def _fetch(self, flag: FlagType, effective_config: DatasetConfigType, **kwargs) -&gt; pd.Series | pd.DataFrame:\n        return pd.DataFrame()\n\n    def fetch_multiple_flags_and_concat(\n            self,\n            flags: Iterable[FlagType],\n            concat_axis: int = 1,\n            concat_level_name: str = 'variable',\n            concat_level_at_top: bool = True,\n            config: dict | DatasetConfigType = None,\n            **kwargs\n    ) -&gt; Union[pd.Series, pd.DataFrame]:\n        dfs = {\n            str(flag): self.fetch(flag, config, **kwargs)\n            for flag in flags\n        }\n        df = pd.concat(\n            dfs,\n            axis=concat_axis,\n            names=[concat_level_name],\n        )\n        if not concat_level_at_top:\n            ax = df.axes[concat_axis]\n            ax = ax.reorder_levels(list(range(1, ax.nlevels)) + [0])\n            df.axes[concat_axis] = ax\n        return df\n\n    def fetch_filter_groupby_agg(\n            self,\n            flag: FlagType,\n            model_filter_query: str = None,\n            prop_groupby: str | list[str] = None,\n            prop_groupby_agg: str = None,\n            config: dict | DatasetConfigType = None,\n            **kwargs\n    ) -&gt; pd.Series | pd.DataFrame:\n        model_flag = self.flag_index.get_linked_model_flag(flag)\n        if not model_flag:\n            raise RuntimeError(f'FlagIndex could not successfully map flag {flag} to a model flag.')\n\n        from mescal.utils import pandas_utils\n\n        data = self.fetch(flag, config, **kwargs)\n        model_df = self.fetch(model_flag, config, **kwargs)\n\n        if model_filter_query:\n            data = pandas_utils.filter_by_model_query(data, model_df, query=model_filter_query)\n\n        if prop_groupby:\n            if isinstance(prop_groupby, str):\n                prop_groupby = [prop_groupby]\n            data = pandas_utils.prepend_model_prop_levels(data, model_df, *prop_groupby)\n            data = data.groupby(prop_groupby)\n            if prop_groupby_agg:\n                data = data.agg(prop_groupby_agg)\n        elif prop_groupby_agg:\n            logger.warning(\n                f\"You provided a prop_groupby_agg operation, but didn't provide prop_groupby. \"\n                f\"No aggregation performed.\"\n            )\n        return data\n\n    @classmethod\n    def get_flag_type(cls) -&gt; Type[FlagType]:\n        from mescal.flag.flag import FlagTypeProtocol\n        return FlagTypeProtocol\n\n    @classmethod\n    def get_flag_index_type(cls) -&gt; Type[FlagIndexType]:\n        from mescal.flag.flag_index import FlagIndex\n        return FlagIndex\n\n    @classmethod\n    def get_config_type(cls) -&gt; Type[DatasetConfigType]:\n        from mescal.datasets.dataset_config import DatasetConfig\n        return DatasetConfig\n\n    @property\n    def instance_config(self) -&gt; DatasetConfigType:\n        from mescal.datasets.dataset_config import DatasetConfigManager\n        return DatasetConfigManager.get_effective_config(self.__class__, self._config)\n\n    def set_instance_config(self, config: DatasetConfigType) -&gt; None:\n        self._config = config\n\n    def set_instance_config_kwargs(self, **kwargs) -&gt; None:\n        for key, value in kwargs.items():\n            setattr(self._config, key, value)\n\n    @classmethod\n    def set_class_config(cls, config: DatasetConfigType) -&gt; None:\n        from mescal.datasets.dataset_config import DatasetConfigManager\n        DatasetConfigManager.set_class_config(cls, config)\n\n    @classmethod\n    def _get_class_name_lower_snake(cls) -&gt; str:\n        return to_lower_snake(cls.__name__)\n\n    def __str__(self) -&gt; str:\n        return self.name\n\n    def __hash__(self):\n        return hash((self.name, self._config))\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset.Dataset.accepted_flags","title":"accepted_flags  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>accepted_flags: set[FlagType]\n</code></pre> <p>Set of all flags accepted by this dataset.</p> <p>This abstract property must be implemented by all concrete dataset classes to define which data flags can be fetched from the dataset.</p> <p>Returns:</p> Type Description <code>set[FlagType]</code> <p>Set of flags that can be used with the fetch() method</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; print(dataset.accepted_flags)\n    {'buses', 'buses_t.marginal_price', 'generators', 'generators_t.p', ...}\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset.Dataset.__init__","title":"__init__","text":"<pre><code>__init__(name: str = None, parent_dataset: Dataset = None, flag_index: FlagIndexType = None, attributes: dict = None, database: Database = None, config: DatasetConfigType = None)\n</code></pre> <p>Initialize a new Dataset instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Human-readable identifier. If None, auto-generates from class name</p> <code>None</code> <code>parent_dataset</code> <code>Dataset</code> <p>Optional parent dataset for hierarchical relationships</p> <code>None</code> <code>flag_index</code> <code>FlagIndexType</code> <p>Index for mapping and validating data flags</p> <code>None</code> <code>attributes</code> <code>dict</code> <p>Dictionary of metadata attributes for the dataset</p> <code>None</code> <code>database</code> <code>Database</code> <p>Optional database for caching expensive computations</p> <code>None</code> <code>config</code> <code>DatasetConfigType</code> <p>Configuration object controlling dataset behavior</p> <code>None</code> Source code in <code>submodules/mescal/mescal/datasets/dataset.py</code> <pre><code>def __init__(\n        self,\n        name: str = None,\n        parent_dataset: Dataset = None,\n        flag_index: FlagIndexType = None,\n        attributes: dict = None,\n        database: Database = None,\n        config: DatasetConfigType = None\n):\n    \"\"\"\n    Initialize a new Dataset instance.\n\n    Args:\n        name: Human-readable identifier. If None, auto-generates from class name\n        parent_dataset: Optional parent dataset for hierarchical relationships\n        flag_index: Index for mapping and validating data flags\n        attributes: Dictionary of metadata attributes for the dataset\n        database: Optional database for caching expensive computations\n        config: Configuration object controlling dataset behavior\n    \"\"\"\n    self.name = name or f'{self.__class__.__name__}_{str(id(self))}'\n    self._flag_index = flag_index or EmptyFlagIndex()\n    self._parent_dataset = parent_dataset\n    self._attributes: dict = attributes or dict()\n    self._database = database\n    self._config = config\n    self.dotfetch = _DotNotationFetcher(self)\n\n    from mescal.kpis.kpi_collection import KPICollection\n    self.kpi_collection: KPICollection = KPICollection()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset.Dataset.add_kpis","title":"add_kpis","text":"<pre><code>add_kpis(kpis: Iterable[KPI | KPIFactory | Type[KPI]])\n</code></pre> <p>Add multiple KPIs to this dataset's KPI collection.</p> <p>Parameters:</p> Name Type Description Default <code>kpis</code> <code>Iterable[KPI | KPIFactory | Type[KPI]]</code> <p>Iterable of KPI instances, factories, or classes to add</p> required Source code in <code>submodules/mescal/mescal/datasets/dataset.py</code> <pre><code>def add_kpis(self, kpis: Iterable[KPI | KPIFactory | Type[KPI]]):\n    \"\"\"\n    Add multiple KPIs to this dataset's KPI collection.\n\n    Args:\n        kpis: Iterable of KPI instances, factories, or classes to add\n    \"\"\"\n    for kpi in kpis:\n        self.add_kpi(kpi)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset.Dataset.add_kpi","title":"add_kpi","text":"<pre><code>add_kpi(kpi: KPI | KPIFactory | Type[KPI])\n</code></pre> <p>Add a single KPI to this dataset's KPI collection.</p> <p>Automatically handles different KPI input types by converting factories and classes to KPI instances.</p> <p>Parameters:</p> Name Type Description Default <code>kpi</code> <code>KPI | KPIFactory | Type[KPI]</code> <p>KPI instance, factory, or class to add</p> required Source code in <code>submodules/mescal/mescal/datasets/dataset.py</code> <pre><code>def add_kpi(self, kpi: KPI | KPIFactory | Type[KPI]):\n    \"\"\"\n    Add a single KPI to this dataset's KPI collection.\n\n    Automatically handles different KPI input types by converting factories\n    and classes to KPI instances.\n\n    Args:\n        kpi: KPI instance, factory, or class to add\n    \"\"\"\n    from mescal.kpis.kpi_base import KPI\n    from mescal.kpis.kpis_from_aggregations import KPIFactory\n    if isinstance(kpi, KPIFactory):\n        kpi = kpi.get_kpi(self)\n    elif isinstance(kpi, type) and issubclass(kpi, KPI):\n        kpi = kpi.from_factory(self)\n    self.kpi_collection.add_kpi(kpi)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset.Dataset.get_accepted_flags_containing_x","title":"get_accepted_flags_containing_x","text":"<pre><code>get_accepted_flags_containing_x(x: str, match_case: bool = False) -&gt; set[FlagType]\n</code></pre> <p>Find all accepted flags containing a specific substring.</p> <p>Useful for discovering related data flags or filtering flags by category.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>Substring to search for in flag names</p> required <code>match_case</code> <code>bool</code> <p>If True, performs case-sensitive search. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>set[FlagType]</code> <p>Set of accepted flags containing the substring</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; ds = PyPSADataset()\n&gt;&gt;&gt; ds.get_accepted_flags_containing_x('generators')\n    {'generators', 'generators_t.p', 'generators_t.efficiency', ...}\n&gt;&gt;&gt; ds.get_accepted_flags_containing_x('BUSES', match_case=True)\n    set()  # Empty because case doesn't match\n</code></pre> Source code in <code>submodules/mescal/mescal/datasets/dataset.py</code> <pre><code>def get_accepted_flags_containing_x(self, x: str, match_case: bool = False) -&gt; set[FlagType]:\n    \"\"\"\n    Find all accepted flags containing a specific substring.\n\n    Useful for discovering related data flags or filtering flags by category.\n\n    Args:\n        x: Substring to search for in flag names\n        match_case: If True, performs case-sensitive search. Default is False.\n\n    Returns:\n        Set of accepted flags containing the substring\n\n    Example:\n\n        &gt;&gt;&gt; ds = PyPSADataset()\n        &gt;&gt;&gt; ds.get_accepted_flags_containing_x('generators')\n            {'generators', 'generators_t.p', 'generators_t.efficiency', ...}\n        &gt;&gt;&gt; ds.get_accepted_flags_containing_x('BUSES', match_case=True)\n            set()  # Empty because case doesn't match\n    \"\"\"\n    if match_case:\n        return {f for f in self.accepted_flags if x in str(f)}\n    x_lower = x.lower()\n    return {f for f in self.accepted_flags if x_lower in str(f).lower()}\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset.Dataset.flag_is_accepted","title":"flag_is_accepted","text":"<pre><code>flag_is_accepted(flag: FlagType) -&gt; bool\n</code></pre> <p>Boolean check whether a flag is accepted by the Dataset.</p> <p>This method can be optionally overridden in any child-class in case you want to follow logic instead of the explicit set of accepted_flags.</p> Source code in <code>submodules/mescal/mescal/datasets/dataset.py</code> <pre><code>def flag_is_accepted(self, flag: FlagType) -&gt; bool:\n    \"\"\"\n    Boolean check whether a flag is accepted by the Dataset.\n\n    This method can be optionally overridden in any child-class\n    in case you want to follow logic instead of the explicit set of accepted_flags.\n    \"\"\"\n    return flag in self.accepted_flags\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset.Dataset.fetch","title":"fetch","text":"<pre><code>fetch(flag: FlagType, config: dict | DatasetConfigType = None, **kwargs) -&gt; Series | DataFrame\n</code></pre> <p>Fetch data associated with a specific flag.</p> <p>This is the primary method for data access in MESCAL datasets. It provides a unified interface for retrieving data regardless of the underlying source or dataset type. The method includes automatic caching, post-processing, and configuration management.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>FlagType</code> <p>Data identifier flag (must be in accepted_flags)</p> required <code>config</code> <code>dict | DatasetConfigType</code> <p>Optional configuration to override dataset defaults.    Can be a dict or DatasetConfig instance.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the underlying      data fetching implementation</p> <code>{}</code> <p>Returns:</p> Type Description <code>Series | DataFrame</code> <p>DataFrame or Series containing the requested data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the flag is not accepted by this dataset</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic usage\n&gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With custom configuration\n&gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price', config={'use_database': False})\n</code></pre> Source code in <code>submodules/mescal/mescal/datasets/dataset.py</code> <pre><code>@flag_must_be_accepted\ndef fetch(self, flag: FlagType, config: dict | DatasetConfigType = None, **kwargs) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"\n    Fetch data associated with a specific flag.\n\n    This is the primary method for data access in MESCAL datasets. It provides\n    a unified interface for retrieving data regardless of the underlying source\n    or dataset type. The method includes automatic caching, post-processing,\n    and configuration management.\n\n    Args:\n        flag: Data identifier flag (must be in accepted_flags)\n        config: Optional configuration to override dataset defaults.\n               Can be a dict or DatasetConfig instance.\n        **kwargs: Additional keyword arguments passed to the underlying\n                 data fetching implementation\n\n    Returns:\n        DataFrame or Series containing the requested data\n\n    Raises:\n        ValueError: If the flag is not accepted by this dataset\n\n    Examples:\n\n        &gt;&gt;&gt; # Basic usage\n        &gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With custom configuration\n        &gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price', config={'use_database': False})\n    \"\"\"\n    effective_config = self._prepare_config(config)\n    use_database = self._database is not None and effective_config.use_database\n\n    if use_database:\n        if self._database.key_is_up_to_date(self, flag, config=effective_config, **kwargs):\n            return self._database.get(self, flag, config=effective_config, **kwargs)\n\n    raw_data = self._fetch(flag, effective_config, **kwargs)\n    processed_data = self._post_process_data(raw_data, flag, effective_config)\n\n    if use_database:\n        self._database.set(self, flag, config=effective_config, value=processed_data, **kwargs)\n\n    return processed_data.copy()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset.flag_must_be_accepted","title":"flag_must_be_accepted","text":"<pre><code>flag_must_be_accepted(method)\n</code></pre> <p>Decorator that validates flag acceptance before method execution.</p> <p>Ensures that only accepted flags are processed by dataset methods, providing clear error messages for invalid flag usage.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <p>The method to decorate</p> required <p>Returns:</p> Type Description <p>Decorated method that validates flag acceptance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the flag is not accepted by the dataset</p> Source code in <code>submodules/mescal/mescal/datasets/dataset.py</code> <pre><code>def flag_must_be_accepted(method):\n    \"\"\"\n    Decorator that validates flag acceptance before method execution.\n\n    Ensures that only accepted flags are processed by dataset methods,\n    providing clear error messages for invalid flag usage.\n\n    Args:\n        method: The method to decorate\n\n    Returns:\n        Decorated method that validates flag acceptance\n\n    Raises:\n        ValueError: If the flag is not accepted by the dataset\n    \"\"\"\n    def raise_if_flag_not_accepted(self: Dataset, flag: FlagType, config: DatasetConfigType = None, **kwargs):\n        if not self.flag_is_accepted(flag):\n            raise ValueError(f'Flag {flag} not accepted by Dataset \"{self.name}\" of type {type(self)}.')\n        return method(self, flag, config, **kwargs)\n    return raise_if_flag_not_accepted\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset_collection","title":"dataset_collection","text":""},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset_collection.DatasetCollection","title":"DatasetCollection","text":"<p>               Bases: <code>Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>Dataset[DatasetConfigType, FlagType, FlagIndexType]</code>, <code>ABC</code></p> <p>Abstract base class for collections of datasets.</p> <p>DatasetCollection extends the Dataset interface to handle multiple child datasets while maintaining the same unified API. This enables complex hierarchical structures where collections themselves can be treated as datasets.</p> Key Features <ul> <li>Inherits all Dataset functionality</li> <li>Manages collections of child datasets</li> <li>Provides iteration and access methods</li> <li>Aggregates accepted flags from all children</li> <li>Supports KPI operations across all sub-datasets</li> </ul> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>DatasetType</code> <p>Type of datasets that can be collected</p> required <code>DatasetConfigType</code> <p>Configuration class for dataset behavior</p> required <code>FlagType</code> <p>Type used for data flag identification</p> required <code>FlagIndexType</code> <p>Flag index implementation for flag mapping</p> required <p>Attributes:</p> Name Type Description <code>datasets</code> <code>list[DatasetType]</code> <p>List of child datasets in this collection</p> Note <p>This class follows the \"Everything is a Dataset\" principle, allowing collections to be used anywhere a Dataset is expected.</p> Source code in <code>submodules/mescal/mescal/datasets/dataset_collection.py</code> <pre><code>class DatasetCollection(\n    Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    Dataset[DatasetConfigType, FlagType, FlagIndexType],\n    ABC\n):\n    \"\"\"\n    Abstract base class for collections of datasets.\n\n    DatasetCollection extends the Dataset interface to handle multiple child datasets\n    while maintaining the same unified API. This enables complex hierarchical structures\n    where collections themselves can be treated as datasets.\n\n    Key Features:\n        - Inherits all Dataset functionality\n        - Manages collections of child datasets\n        - Provides iteration and access methods\n        - Aggregates accepted flags from all children\n        - Supports KPI operations across all sub-datasets\n\n    Type Parameters:\n        DatasetType: Type of datasets that can be collected\n        DatasetConfigType: Configuration class for dataset behavior\n        FlagType: Type used for data flag identification\n        FlagIndexType: Flag index implementation for flag mapping\n\n    Attributes:\n        datasets (list[DatasetType]): List of child datasets in this collection\n\n    Note:\n        This class follows the \"Everything is a Dataset\" principle, allowing\n        collections to be used anywhere a Dataset is expected.\n    \"\"\"\n\n    def __init__(\n            self,\n            datasets: list[DatasetType] = None,\n            name: str = None,\n            parent_dataset: Dataset = None,\n            flag_index: FlagIndex = None,\n            attributes: dict = None,\n            database: Database = None,\n            config: DatasetConfigType = None\n    ):\n        super().__init__(\n            name=name,\n            parent_dataset=parent_dataset,\n            flag_index=flag_index,\n            attributes=attributes,\n            database=database,\n            config=config,\n        )\n        self.datasets: list[DatasetType] = datasets if datasets else []\n\n    @property\n    def dataset_iterator(self) -&gt; Iterator[DatasetType]:\n        for ds in self.datasets:\n            yield ds\n\n    @property\n    def flag_index(self) -&gt; FlagIndex:\n        from mescal.flag.flag_index import EmptyFlagIndex\n        if (self._flag_index is None) or isinstance(self._flag_index, EmptyFlagIndex):\n            from mescal.utils.check_all_same import all_same_object\n            if all_same_object(ds.flag_index for ds in self.datasets) and len(self.datasets):\n                return self.get_dataset().flag_index\n        return self._flag_index\n\n    @property\n    def attributes(self) -&gt; dict:\n        child_dataset_atts = [ds.attributes for ds in self.datasets]\n        attributes_that_all_childs_have_in_common = get_intersection_of_dicts(child_dataset_atts)\n        return {**attributes_that_all_childs_have_in_common, **self._attributes.copy()}\n\n    def get_merged_kpi_collection(self, deep: bool = True) -&gt; 'KPICollection':\n        from mescal.kpis.kpi_collection import KPICollection\n        all_kpis = set()\n        for ds in self.datasets:\n            for kpi in ds.kpi_collection:\n                all_kpis.add(kpi)\n            if deep and isinstance(ds, DatasetCollection):\n                for kpi in ds.get_merged_kpi_collection(deep=deep):\n                    all_kpis.add(kpi)\n\n        return KPICollection(all_kpis)\n\n    def add_kpis_to_all_sub_datasets(self, kpis: Iterable[KPIFactory]):\n        for kpi in kpis:\n            self.add_kpi_to_all_sub_datasets(kpi)\n\n    def add_kpi_to_all_sub_datasets(self, kpi: KPIFactory):\n        for ds in self.datasets:\n            ds.add_kpi(kpi)\n\n    def clear_kpi_collection_for_all_sub_datasets(self, deep: bool = True):\n        for ds in self.datasets:\n            ds.clear_kpi_collection()\n            if deep and isinstance(ds, DatasetCollection):\n                ds.clear_kpi_collection_for_all_sub_datasets(deep=deep)\n\n    @abstractmethod\n    def _fetch(\n            self,\n            flag: FlagType,\n            effective_config: DatasetConfigType,\n            **kwargs\n    ) -&gt; pd.Series | pd.DataFrame:\n        pass\n\n    def flag_is_accepted(self, flag: FlagType) -&gt; bool:\n        return any(ds.flag_is_accepted(flag) for ds in self.datasets)\n\n    @property\n    def accepted_flags(self) -&gt; set[FlagType]:\n        return nested_union([ds.accepted_flags for ds in self.datasets])\n\n    def _required_flags_for_flag(self, flag: FlagType) -&gt; set[FlagType]:\n        return nested_union([ds.accepted_flags for ds in self.datasets])\n\n    def get_dataset(self, key: str = None) -&gt; DatasetType:\n        if key is None:\n            if not self.datasets:\n                raise ValueError(\"No datasets available\")\n            return self.datasets[0]\n\n        for ds in self.datasets:\n            if ds.name == key:\n                return ds\n\n        raise KeyError(f\"Dataset with name '{key}' not found\")\n\n    def add_datasets(self, datasets: Iterable[DatasetType]):\n        for ds in datasets:\n            self.add_dataset(ds)\n\n    def add_dataset(self, dataset: DatasetType):\n        if not isinstance(dataset, self.get_child_dataset_type()):\n            raise TypeError(f\"Can only add data sets of type {self.get_child_dataset_type().__name__}.\")\n\n        for i, existing in enumerate(self.datasets):\n            if existing.name == dataset.name:\n                logger.warning(\n                    f\"Dataset {self.name}: \"\n                    f\"dataset {dataset.name} already in this collection. Replacing it.\"\n                )\n                self.datasets[i] = dataset\n                return\n\n        self.datasets.append(dataset)\n\n    @classmethod\n    def get_child_dataset_type(cls) -&gt; type[DatasetType]:\n        return Dataset\n\n    def fetch_merged(\n            self,\n            flag: FlagType,\n            config: dict | DatasetConfigType = None,\n            keep_first: bool = True,\n            **kwargs\n    ) -&gt; pd.Series | pd.DataFrame:\n        \"\"\"Fetch method that merges dataframes from all child datasets, similar to DatasetMergeCollection.\"\"\"\n        temp_merge_collection = self.get_merged_dataset_collection(keep_first)\n        return temp_merge_collection.fetch(flag, config, **kwargs)\n\n    def get_merged_dataset_collection(self, keep_first: bool = True) -&gt; 'DatasetMergeCollection':\n        return DatasetMergeCollection(\n            datasets=self.datasets,\n            name=f\"{self.name} merged\",\n            keep_first=keep_first\n        )\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset_collection.DatasetCollection.fetch_merged","title":"fetch_merged","text":"<pre><code>fetch_merged(flag: FlagType, config: dict | DatasetConfigType = None, keep_first: bool = True, **kwargs) -&gt; Series | DataFrame\n</code></pre> <p>Fetch method that merges dataframes from all child datasets, similar to DatasetMergeCollection.</p> Source code in <code>submodules/mescal/mescal/datasets/dataset_collection.py</code> <pre><code>def fetch_merged(\n        self,\n        flag: FlagType,\n        config: dict | DatasetConfigType = None,\n        keep_first: bool = True,\n        **kwargs\n) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"Fetch method that merges dataframes from all child datasets, similar to DatasetMergeCollection.\"\"\"\n    temp_merge_collection = self.get_merged_dataset_collection(keep_first)\n    return temp_merge_collection.fetch(flag, config, **kwargs)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset_collection.DatasetLinkCollection","title":"DatasetLinkCollection","text":"<p>               Bases: <code>Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code></p> <p>Links multiple datasets to provide unified data access with automatic routing.</p> <p>DatasetLinkCollection acts as a unified interface to multiple child datasets, automatically routing data requests to the appropriate child dataset that  accepts the requested flag. This is the foundation for platform datasets that combine multiple data interpreters.</p> Key Features <ul> <li>Automatic flag routing to appropriate child dataset</li> <li>Bidirectional parent-child relationships</li> <li>First-match-wins routing strategy</li> <li>Overlap detection and warnings</li> <li>Maintains all Dataset interface compatibility</li> </ul> Routing Logic <p>When fetch() is called, iterates through child datasets in order and returns data from the first dataset that accepts the flag.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Platform dataset with multiple interpreters\n&gt;&gt;&gt; link_collection = DatasetLinkCollection([\n...     ModelInterpreter(network),\n...     TimeSeriesInterpreter(network),\n...     ObjectiveInterpreter(network)\n... ])\n&gt;&gt;&gt; # Automatically routes to appropriate interpreter\n&gt;&gt;&gt; buses = link_collection.fetch('buses')  # -&gt; ModelInterpreter\n&gt;&gt;&gt; prices = link_collection.fetch('buses_t.marginal_price')  # -&gt; TimeSeriesInterpreter\n</code></pre> Warning <p>If multiple child datasets accept the same flag, only the first one will be used. The constructor logs warnings for such overlaps.</p> Source code in <code>submodules/mescal/mescal/datasets/dataset_collection.py</code> <pre><code>class DatasetLinkCollection(\n    Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]\n):\n    \"\"\"\n    Links multiple datasets to provide unified data access with automatic routing.\n\n    DatasetLinkCollection acts as a unified interface to multiple child datasets,\n    automatically routing data requests to the appropriate child dataset that \n    accepts the requested flag. This is the foundation for platform datasets\n    that combine multiple data interpreters.\n\n    Key Features:\n        - Automatic flag routing to appropriate child dataset\n        - Bidirectional parent-child relationships\n        - First-match-wins routing strategy\n        - Overlap detection and warnings\n        - Maintains all Dataset interface compatibility\n\n    Routing Logic:\n        When fetch() is called, iterates through child datasets in order and\n        returns data from the first dataset that accepts the flag.\n\n    Example:\n\n        &gt;&gt;&gt; # Platform dataset with multiple interpreters\n        &gt;&gt;&gt; link_collection = DatasetLinkCollection([\n        ...     ModelInterpreter(network),\n        ...     TimeSeriesInterpreter(network),\n        ...     ObjectiveInterpreter(network)\n        ... ])\n        &gt;&gt;&gt; # Automatically routes to appropriate interpreter\n        &gt;&gt;&gt; buses = link_collection.fetch('buses')  # -&gt; ModelInterpreter\n        &gt;&gt;&gt; prices = link_collection.fetch('buses_t.marginal_price')  # -&gt; TimeSeriesInterpreter\n\n    Warning:\n        If multiple child datasets accept the same flag, only the first one\n        will be used. The constructor logs warnings for such overlaps.\n    \"\"\"\n\n    def __init__(\n            self,\n            datasets: list[DatasetType],\n            name: str = None,\n            parent_dataset: Dataset = None,\n            flag_index: FlagIndex = None,\n            attributes: dict = None,\n            database: Database = None,\n            config: DatasetConfigType = None,\n    ):\n        super().__init__(\n            datasets=datasets,\n            name=name,\n            parent_dataset=parent_dataset,\n            flag_index=flag_index,\n            attributes=attributes,\n            database=database,\n            config=config,\n        )\n        self._warn_if_flags_overlap()\n\n    def _fetch(self, flag: FlagType, effective_config: DatasetConfigType, **kwargs) -&gt; pd.Series | pd.DataFrame:\n        for ds in self.datasets:\n            if ds.flag_is_accepted(flag):\n                return ds.fetch(flag, effective_config, **kwargs)\n        raise KeyError(f\"Key '{flag}' not recognized by any of the linked Datasets.\")\n\n    def _warn_if_flags_overlap(self):\n        from collections import Counter\n\n        accepted_flags = list()\n        for ds in self.datasets:\n            accepted_flags += list(ds.accepted_flags)\n\n        counts = Counter(accepted_flags)\n        duplicates = {k: v for k, v in counts.items() if v &gt; 1}\n        if any(duplicates.values()):\n            logger.warning(\n                f\"Dataset {self.name}: \"\n                f\"The following keys have multiple Dataset sources: {duplicates.keys()}. \\n\"\n                f\"Only the first one will be used! This might lead to unexpected behavior. \\n\"\n                f\"A potential reason could be the use of an inappropriate DatasetCollection Type.\"\n            )\n\n    def get_dataset_by_type(self, ds_type: type[Dataset]) -&gt; DatasetType:\n        \"\"\"Returns instance of child dataset that matches the ds_type.\"\"\"\n        for ds in self.datasets:\n            if isinstance(ds, ds_type):\n                return ds\n        raise KeyError(f'No Dataset of type {ds_type.__name__} found in {self.name}.')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset_collection.DatasetLinkCollection.get_dataset_by_type","title":"get_dataset_by_type","text":"<pre><code>get_dataset_by_type(ds_type: type[Dataset]) -&gt; DatasetType\n</code></pre> <p>Returns instance of child dataset that matches the ds_type.</p> Source code in <code>submodules/mescal/mescal/datasets/dataset_collection.py</code> <pre><code>def get_dataset_by_type(self, ds_type: type[Dataset]) -&gt; DatasetType:\n    \"\"\"Returns instance of child dataset that matches the ds_type.\"\"\"\n    for ds in self.datasets:\n        if isinstance(ds, ds_type):\n            return ds\n    raise KeyError(f'No Dataset of type {ds_type.__name__} found in {self.name}.')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset_collection.DatasetMergeCollection","title":"DatasetMergeCollection","text":"<p>               Bases: <code>Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code></p> <p>Fetch method will merge fragmented Datasets for same flag, e.g.:     - fragmented simulation runs, e.g. CW1, CW2, CW3, CWn.     - fragmented data sources, e.g. mapping from Excel file with model from simulation platform.</p> Source code in <code>submodules/mescal/mescal/datasets/dataset_collection.py</code> <pre><code>class DatasetMergeCollection(\n    Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]\n):\n    \"\"\"\n    Fetch method will merge fragmented Datasets for same flag, e.g.:\n        - fragmented simulation runs, e.g. CW1, CW2, CW3, CWn.\n        - fragmented data sources, e.g. mapping from Excel file with model from simulation platform.\n    \"\"\"\n    def __init__(\n            self,\n            datasets: list[DatasetType],\n            name: str = None,\n            parent_dataset: Dataset = None,\n            flag_index: FlagIndex = None,\n            attributes: dict = None,\n            database: Database = None,\n            config: DatasetConfigType = None,\n            keep_first: bool = True,\n    ):\n        super().__init__(\n            datasets=datasets,\n            name=name,\n            parent_dataset=parent_dataset,\n            flag_index=flag_index,\n            attributes=attributes,\n            database=database,\n            config=config,\n        )\n        self.keep_first = keep_first\n\n    def _fetch(self, flag: FlagType, effective_config: DatasetConfigType, **kwargs) -&gt; pd.Series | pd.DataFrame:\n        data_frames = []\n        for ds in self.datasets:\n            if ds.flag_is_accepted(flag):\n                data_frames.append(ds.fetch(flag, effective_config, **kwargs))\n\n        if not data_frames:\n            raise KeyError(f\"Flag '{flag}' not recognized by any of the datasets.\")\n\n        from mescal.utils.pandas_utils.combine_df import combine_dfs\n        df = combine_dfs(data_frames, keep_first=self.keep_first)\n        return df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset_collection.DatasetConcatCollection","title":"DatasetConcatCollection","text":"<p>               Bases: <code>Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code></p> <p>Concatenates data from multiple datasets with MultiIndex structure.</p> <p>DatasetConcatCollection is fundamental to MESCAL's multi-scenario analysis capabilities. It fetches the same flag from multiple child datasets and concatenates the results into a single DataFrame/Series with an additional index level identifying the source dataset.</p> Key Features <ul> <li>Automatic MultiIndex creation with dataset names</li> <li>Configurable concatenation axis and level positioning  </li> <li>Preserves all dimensional relationships</li> <li>Supports scenario and comparison collections</li> <li>Enables unified analysis across multiple datasets</li> </ul> MultiIndex Structure <p>The resulting data structure includes an additional index level (typically named 'dataset') that identifies the source dataset for each data point.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Collection of scenario datasets\n&gt;&gt;&gt; scenarios = DatasetConcatCollection([\n...     PyPSADataset(base_network, name='base'),\n...     PyPSADataset(high_res_network, name='high_res'),\n...     PyPSADataset(low_gas_network, name='low_gas')\n... ])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Fetch creates MultiIndex DataFrame\n&gt;&gt;&gt; prices = scenarios.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; print(prices.columns.names)\n    ['dataset', 'Bus']  # Original Bus index + dataset level\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Access specific scenario data\n&gt;&gt;&gt; base_prices = prices['base']\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Analyze across scenarios\n&gt;&gt;&gt; mean_prices = prices.mean()  # Mean across all scenarios\n</code></pre> Source code in <code>submodules/mescal/mescal/datasets/dataset_collection.py</code> <pre><code>class DatasetConcatCollection(\n    Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]\n):\n    \"\"\"\n    Concatenates data from multiple datasets with MultiIndex structure.\n\n    DatasetConcatCollection is fundamental to MESCAL's multi-scenario analysis\n    capabilities. It fetches the same flag from multiple child datasets and\n    concatenates the results into a single DataFrame/Series with an additional\n    index level identifying the source dataset.\n\n    Key Features:\n        - Automatic MultiIndex creation with dataset names\n        - Configurable concatenation axis and level positioning  \n        - Preserves all dimensional relationships\n        - Supports scenario and comparison collections\n        - Enables unified analysis across multiple datasets\n\n    MultiIndex Structure:\n        The resulting data structure includes an additional index level\n        (typically named 'dataset') that identifies the source dataset\n        for each data point.\n\n    Example:\n\n        &gt;&gt;&gt; # Collection of scenario datasets\n        &gt;&gt;&gt; scenarios = DatasetConcatCollection([\n        ...     PyPSADataset(base_network, name='base'),\n        ...     PyPSADataset(high_res_network, name='high_res'),\n        ...     PyPSADataset(low_gas_network, name='low_gas')\n        ... ])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Fetch creates MultiIndex DataFrame\n        &gt;&gt;&gt; prices = scenarios.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; print(prices.columns.names)\n            ['dataset', 'Bus']  # Original Bus index + dataset level\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Access specific scenario data\n        &gt;&gt;&gt; base_prices = prices['base']\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Analyze across scenarios\n        &gt;&gt;&gt; mean_prices = prices.mean()  # Mean across all scenarios\n    \"\"\"\n    DEFAULT_CONCAT_LEVEL_NAME = 'dataset'\n    DEFAULT_ATT_LEVEL_NAME = 'attribute'\n\n    def __init__(\n            self,\n            datasets: list[DatasetType],\n            name: str = None,\n            parent_dataset: Dataset = None,\n            flag_index: FlagIndex = None,\n            attributes: dict = None,\n            database: Database = None,\n            config: DatasetConfigType = None,\n            default_concat_axis: int = 1,\n            concat_top: bool = True,\n            concat_level_name: str = None,\n    ):\n        super().__init__(\n            datasets=datasets,\n            name=name,\n            parent_dataset=parent_dataset,\n            flag_index=flag_index,\n            attributes=attributes,\n            database=database,\n            config=config,\n        )\n        super().__init__(datasets=datasets, name=name)\n        self.default_concat_axis = default_concat_axis\n        self.concat_top = concat_top\n        self.concat_level_name = concat_level_name or self.DEFAULT_CONCAT_LEVEL_NAME\n\n    def get_attributes_concat_df(self) -&gt; pd.DataFrame:\n        if all(isinstance(ds, DatasetConcatCollection) for ds in self.datasets):\n            use_att_df_instead_of_series = True\n        else:\n            use_att_df_instead_of_series = False\n\n        atts_per_dataset = dict()\n        for ds in self.datasets:\n            atts = ds.get_attributes_concat_df().T if use_att_df_instead_of_series else ds.get_attributes_series()\n            atts_per_dataset[ds.name] = atts\n\n        return pd.concat(\n            atts_per_dataset,\n            axis=1,\n            names=[self.concat_level_name]\n        ).rename_axis(self.DEFAULT_ATT_LEVEL_NAME).T\n\n    def _fetch(\n            self,\n            flag: FlagType,\n            effective_config: DatasetConfigType,\n            concat_axis: int = None,\n            **kwargs\n    ) -&gt; pd.Series | pd.DataFrame:\n        if concat_axis is None:\n            concat_axis = self.default_concat_axis\n\n        dfs = {}\n        for ds in self.datasets:\n            if ds.flag_is_accepted(flag):\n                dfs[ds.name] = ds.fetch(flag, effective_config, **kwargs)\n\n        if not dfs:\n            raise KeyError(f\"Flag '{flag}' not recognized by any of the datasets in {type(self)} {self.name}.\")\n\n        df0 = list(dfs.values())[0]\n        if not all(len(df.axes) == len(df0.axes) for df in dfs.values()):\n            raise NotImplementedError(f'Axes lengths do not match between dfs.')\n\n        for ax in range(len(df0.axes)):\n            if not all(set(df.axes[ax].names) == set(df0.axes[ax].names) for df in dfs.values()):\n                raise NotImplementedError(f'Axes names do not match between dfs.')\n\n        df = pd.concat(dfs, join='outer', axis=concat_axis, names=[self.concat_level_name])\n\n        if not self.concat_top:\n            ax = df.axes[concat_axis]\n            df.axes[concat_axis] = ax.reorder_levels([ax.nlevels - 1] + list(range(ax.nlevels - 1)))\n\n        return df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset_comparison","title":"dataset_comparison","text":""},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset_comparison.DatasetComparison","title":"DatasetComparison","text":"<p>               Bases: <code>Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code></p> <p>Computes and provides access to differences between two datasets.</p> <p>DatasetComparison is a core component of MESCAL's scenario comparison capabilities. It automatically calculates deltas, ratios, or side-by-side comparisons between a variation dataset and a reference dataset, enabling systematic analysis of scenario differences.</p> Key Features <ul> <li>Automatic delta computation between datasets</li> <li>Multiple comparison types (DELTA, VARIATION, BOTH)</li> <li>Handles numeric and non-numeric data appropriately</li> <li>Preserves data structure and index relationships</li> <li>Configurable unchanged value handling</li> <li>Inherits full Dataset interface</li> </ul> Comparison Types <ul> <li>DELTA: Variation - Reference (default)</li> <li>VARIATION: Returns variation data with optional NaN for unchanged values</li> <li>BOTH: Side-by-side variation and reference data</li> </ul> <p>Attributes:</p> Name Type Description <code>variation_dataset</code> <p>The dataset representing the scenario being compared</p> <code>reference_dataset</code> <p>The dataset representing the baseline for comparison</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Compare high renewable scenario to base case\n&gt;&gt;&gt; comparison = DatasetComparison(\n...     variation_dataset=high_res_dataset,\n...     reference_dataset=base_dataset\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Get price differences\n&gt;&gt;&gt; price_deltas = comparison.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Get both datasets side-by-side (often used to show model changes)\n&gt;&gt;&gt; price_both = comparison.fetch('buses', comparison_type=ComparisonTypeEnum.BOTH)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Highlight only changes (often used to show model changes)\n&gt;&gt;&gt; price_changes = comparison.fetch('buses', replace_unchanged_values_by_nan=True)\n</code></pre> Source code in <code>submodules/mescal/mescal/datasets/dataset_comparison.py</code> <pre><code>class DatasetComparison(\n    Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]\n):\n    \"\"\"\n    Computes and provides access to differences between two datasets.\n\n    DatasetComparison is a core component of MESCAL's scenario comparison capabilities.\n    It automatically calculates deltas, ratios, or side-by-side comparisons between\n    a variation dataset and a reference dataset, enabling systematic analysis of\n    scenario differences.\n\n    Key Features:\n        - Automatic delta computation between datasets\n        - Multiple comparison types (DELTA, VARIATION, BOTH)\n        - Handles numeric and non-numeric data appropriately\n        - Preserves data structure and index relationships\n        - Configurable unchanged value handling\n        - Inherits full Dataset interface\n\n    Comparison Types:\n        - DELTA: Variation - Reference (default)\n        - VARIATION: Returns variation data with optional NaN for unchanged values\n        - BOTH: Side-by-side variation and reference data\n\n    Attributes:\n        variation_dataset: The dataset representing the scenario being compared\n        reference_dataset: The dataset representing the baseline for comparison\n\n    Example:\n\n        &gt;&gt;&gt; # Compare high renewable scenario to base case\n        &gt;&gt;&gt; comparison = DatasetComparison(\n        ...     variation_dataset=high_res_dataset,\n        ...     reference_dataset=base_dataset\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Get price differences\n        &gt;&gt;&gt; price_deltas = comparison.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Get both datasets side-by-side (often used to show model changes)\n        &gt;&gt;&gt; price_both = comparison.fetch('buses', comparison_type=ComparisonTypeEnum.BOTH)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Highlight only changes (often used to show model changes)\n        &gt;&gt;&gt; price_changes = comparison.fetch('buses', replace_unchanged_values_by_nan=True)\n    \"\"\"\n    COMPARISON_ATTRIBUTES_SOURCE = ComparisonAttributesSourceEnum.USE_VARIATION_ATTS\n    COMPARISON_NAME_JOIN = ' vs '\n    VARIATION_DS_ATT_KEY = 'variation_dataset'\n    REFERENCE_DS_ATT_KEY = 'reference_dataset'\n\n    def __init__(\n            self,\n            variation_dataset: Dataset,\n            reference_dataset: Dataset,\n            name: str = None,\n            attributes: dict = None,\n            config: DatasetConfigType = None,\n    ):\n        name = name or self._get_auto_generated_name(variation_dataset, reference_dataset)\n\n        super().__init__(\n            [reference_dataset, variation_dataset],\n            name=name,\n            attributes=attributes,\n            config=config\n        )\n\n        self.variation_dataset = variation_dataset\n        self.reference_dataset = reference_dataset\n\n    def _get_auto_generated_name(self, variation_dataset: Dataset, reference_dataset: Dataset) -&gt; str:\n        return variation_dataset.name + self.COMPARISON_NAME_JOIN + reference_dataset.name\n\n    @property\n    def attributes(self) -&gt; dict:\n        match self.COMPARISON_ATTRIBUTES_SOURCE:\n            case ComparisonAttributesSourceEnum.USE_VARIATION_ATTS:\n                atts = self.variation_dataset.attributes.copy()\n            case ComparisonAttributesSourceEnum.USE_REFERENCE_ATTS:\n                atts = self.reference_dataset.attributes.copy()\n            case _:\n                atts = super().attributes\n        atts[self.VARIATION_DS_ATT_KEY] = self.variation_dataset.name\n        atts[self.REFERENCE_DS_ATT_KEY] = self.reference_dataset.name\n        return atts\n\n    def fetch(\n            self,\n            flag: FlagType,\n            config: dict | DatasetConfigType = None,\n            comparison_type: ComparisonTypeEnum = ComparisonTypeEnum.DELTA,\n            replace_unchanged_values_by_nan: bool = False,\n            fill_value: float | int | None = None,\n            **kwargs\n    ) -&gt; pd.Series | pd.DataFrame:\n        \"\"\"\n        Fetch comparison data between variation and reference datasets.\n\n        Extends the base Dataset.fetch() method with comparison-specific parameters\n        for controlling how the comparison is computed and formatted.\n\n        Args:\n            flag: Data identifier flag to fetch from both datasets\n            config: Optional configuration overrides\n            comparison_type: How to compare the datasets:\n                - DELTA: variation - reference (default)\n                - VARIATION: variation data only, optionally with NaN for unchanged\n                - BOTH: concatenated variation and reference data\n            replace_unchanged_values_by_nan: If True, replaces values that are\n                identical between datasets with NaN (useful for highlighting changes)\n            fill_value: Value to use for missing data in subtraction operations\n            **kwargs: Additional arguments passed to child dataset fetch methods\n\n        Returns:\n            DataFrame or Series with comparison results\n\n        Example:\n\n            &gt;&gt;&gt; # Basic delta comparison\n            &gt;&gt;&gt; deltas = comparison.fetch('buses_t.marginal_price')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Highlight only changed values\n            &gt;&gt;&gt; changes_only = comparison.fetch(\n            ...     'buses_t.marginal_price',\n            ...     replace_unchanged_values_by_nan=True\n            ... )\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Side-by-side comparison\n            &gt;&gt;&gt; both = comparison.fetch(\n            ...     'buses_t.marginal_price',\n            ...     comparison_type=ComparisonTypeEnum.BOTH\n            ... )\n        \"\"\"\n        return super().fetch(\n            flag=flag,\n            config=config,\n            comparison_type=comparison_type,\n            replace_unchanged_values_by_nan=replace_unchanged_values_by_nan,\n            fill_value=fill_value,\n            **kwargs\n        )\n\n    def _fetch(\n            self,\n            flag: FlagType,\n            effective_config: DatasetConfigType,\n            comparison_type: ComparisonTypeEnum = ComparisonTypeEnum.DELTA,\n            replace_unchanged_values_by_nan: bool = False,\n            fill_value: float | int | None = None,\n            **kwargs\n    ) -&gt; pd.Series | pd.DataFrame:\n        df_var = self.variation_dataset.fetch(flag, effective_config, **kwargs)\n        df_ref = self.reference_dataset.fetch(flag, effective_config, **kwargs)\n\n        match comparison_type:\n            case ComparisonTypeEnum.VARIATION:\n                return self._get_variation_comparison(df_var, df_ref, replace_unchanged_values_by_nan)\n            case ComparisonTypeEnum.BOTH:\n                return self._get_both_comparison(df_var, df_ref, replace_unchanged_values_by_nan)\n            case ComparisonTypeEnum.DELTA:\n                return self._get_delta_comparison(df_var, df_ref, replace_unchanged_values_by_nan, fill_value)\n        raise ValueError(f\"Unsupported comparison_type: {comparison_type}\")\n\n    def _values_are_equal(self, val1, val2) -&gt; bool:\n        if pd.isna(val1) and pd.isna(val2):\n            return True\n        try:\n            return val1 == val2\n        except:\n            pass\n        try:\n            if str(val1) == str(val2):\n                return True\n        except:\n            pass\n        return False\n\n    def _get_variation_comparison(\n            self,\n            df_var: pd.DataFrame,\n            df_ref: pd.DataFrame,\n            replace_unchanged_values_by_nan: bool\n    ) -&gt; pd.DataFrame:\n        result = df_var.copy()\n\n        if replace_unchanged_values_by_nan:\n            common_indices = df_var.index.intersection(df_ref.index)\n            common_columns = df_var.columns.intersection(df_ref.columns)\n\n            for idx in common_indices:\n                for col in common_columns:\n                    if self._values_are_equal(df_var.loc[idx, col], df_ref.loc[idx, col]):\n                        result.loc[idx, col] = float('nan')\n\n        return result\n\n    def _get_both_comparison(\n            self,\n            df_var: pd.DataFrame,\n            df_ref: pd.DataFrame,\n            replace_unchanged_values_by_nan: bool\n    ) -&gt; pd.DataFrame:\n        var_name = self.variation_dataset.name\n        ref_name = self.reference_dataset.name\n\n        result = pd.concat([df_var, df_ref], keys=[var_name, ref_name])\n        result = result.sort_index(level=1)\n\n        if replace_unchanged_values_by_nan:\n            common_indices = df_var.index.intersection(df_ref.index)\n            common_columns = df_var.columns.intersection(df_ref.columns)\n\n            for idx in common_indices:\n                for col in common_columns:\n                    if self._values_are_equal(df_var.loc[idx, col], df_ref.loc[idx, col]):\n                        result.loc[(var_name, idx), col] = float('nan')\n                        result.loc[(ref_name, idx), col] = float('nan')\n\n        return result\n\n    def _get_delta_comparison(\n            self,\n            df_var: pd.DataFrame,\n            df_ref: pd.DataFrame,\n            replace_unchanged_values_by_nan: bool,\n            fill_value: float | int | None\n    ) -&gt; pd.DataFrame:\n        if pd_is_numeric(df_var) and pd_is_numeric(df_ref):\n            result = df_var.subtract(df_ref, fill_value=fill_value)\n\n            if replace_unchanged_values_by_nan:\n                result = result.replace(0, float('nan'))\n\n            return result\n\n        all_columns = df_var.columns.union(df_ref.columns)\n        all_indices = df_var.index.union(df_ref.index)\n\n        result = pd.DataFrame(index=all_indices, columns=all_columns)\n\n        for col in all_columns:\n            if col in df_var.columns and col in df_ref.columns:\n                var_col = df_var[col]\n                ref_col = df_ref[col]\n\n                # Special handling for boolean columns\n                if pd.api.types.is_bool_dtype(var_col) and pd.api.types.is_bool_dtype(ref_col):\n                    # For booleans, we can mark where they differ\n                    common_indices = var_col.index.intersection(ref_col.index)\n                    delta = pd.Series(index=all_indices)\n\n                    for idx in common_indices:\n                        if var_col.loc[idx] != ref_col.loc[idx]:\n                            delta.loc[idx] = f\"{var_col.loc[idx]} (was {ref_col.loc[idx]})\"\n                        elif not replace_unchanged_values_by_nan:\n                            delta.loc[idx] = var_col.loc[idx]\n\n                    # Handle indices only in variation\n                    for idx in var_col.index.difference(ref_col.index):\n                        delta.loc[idx] = f\"{var_col.loc[idx]} (new)\"\n\n                    # Handle indices only in reference\n                    for idx in ref_col.index.difference(var_col.index):\n                        delta.loc[idx] = f\"DELETED: {ref_col.loc[idx]}\"\n\n                    result[col] = delta\n\n                elif pd.api.types.is_numeric_dtype(var_col) and pd.api.types.is_numeric_dtype(ref_col):\n                    delta = var_col.subtract(ref_col, fill_value=fill_value)\n                    result[col] = delta\n\n                    if replace_unchanged_values_by_nan:\n                        result.loc[delta == 0, col] = float('nan')\n                else:\n                    common_indices = var_col.index.intersection(ref_col.index)\n                    var_only_indices = var_col.index.difference(ref_col.index)\n                    ref_only_indices = ref_col.index.difference(var_col.index)\n\n                    for idx in common_indices:\n                        if not self._values_are_equal(var_col.loc[idx], ref_col.loc[idx]):\n                            result.loc[idx, col] = f\"{var_col.loc[idx]} (was {ref_col.loc[idx]})\"\n                        elif not replace_unchanged_values_by_nan:\n                            result.loc[idx, col] = var_col.loc[idx]\n\n                    for idx in var_only_indices:\n                        result.loc[idx, col] = f\"{var_col.loc[idx]} (new)\"\n\n                    for idx in ref_only_indices:\n                        val = ref_col.loc[idx]\n                        if not pd.isna(val):\n                            result.loc[idx, col] = f\"DELETED: {val}\"\n\n            elif col in df_var.columns:\n                for idx in df_var.index:\n                    result.loc[idx, col] = f\"{df_var.loc[idx, col]} (new column)\"\n\n            else:  # Column only in reference\n                for idx in df_ref.index:\n                    val = df_ref.loc[idx, col]\n                    if not pd.isna(val):\n                        result.loc[idx, col] = f\"REMOVED: {val}\"\n\n        return result\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset_comparison.DatasetComparison.fetch","title":"fetch","text":"<pre><code>fetch(flag: FlagType, config: dict | DatasetConfigType = None, comparison_type: ComparisonTypeEnum = DELTA, replace_unchanged_values_by_nan: bool = False, fill_value: float | int | None = None, **kwargs) -&gt; Series | DataFrame\n</code></pre> <p>Fetch comparison data between variation and reference datasets.</p> <p>Extends the base Dataset.fetch() method with comparison-specific parameters for controlling how the comparison is computed and formatted.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>FlagType</code> <p>Data identifier flag to fetch from both datasets</p> required <code>config</code> <code>dict | DatasetConfigType</code> <p>Optional configuration overrides</p> <code>None</code> <code>comparison_type</code> <code>ComparisonTypeEnum</code> <p>How to compare the datasets: - DELTA: variation - reference (default) - VARIATION: variation data only, optionally with NaN for unchanged - BOTH: concatenated variation and reference data</p> <code>DELTA</code> <code>replace_unchanged_values_by_nan</code> <code>bool</code> <p>If True, replaces values that are identical between datasets with NaN (useful for highlighting changes)</p> <code>False</code> <code>fill_value</code> <code>float | int | None</code> <p>Value to use for missing data in subtraction operations</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to child dataset fetch methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>Series | DataFrame</code> <p>DataFrame or Series with comparison results</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Basic delta comparison\n&gt;&gt;&gt; deltas = comparison.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Highlight only changed values\n&gt;&gt;&gt; changes_only = comparison.fetch(\n...     'buses_t.marginal_price',\n...     replace_unchanged_values_by_nan=True\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Side-by-side comparison\n&gt;&gt;&gt; both = comparison.fetch(\n...     'buses_t.marginal_price',\n...     comparison_type=ComparisonTypeEnum.BOTH\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/datasets/dataset_comparison.py</code> <pre><code>def fetch(\n        self,\n        flag: FlagType,\n        config: dict | DatasetConfigType = None,\n        comparison_type: ComparisonTypeEnum = ComparisonTypeEnum.DELTA,\n        replace_unchanged_values_by_nan: bool = False,\n        fill_value: float | int | None = None,\n        **kwargs\n) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"\n    Fetch comparison data between variation and reference datasets.\n\n    Extends the base Dataset.fetch() method with comparison-specific parameters\n    for controlling how the comparison is computed and formatted.\n\n    Args:\n        flag: Data identifier flag to fetch from both datasets\n        config: Optional configuration overrides\n        comparison_type: How to compare the datasets:\n            - DELTA: variation - reference (default)\n            - VARIATION: variation data only, optionally with NaN for unchanged\n            - BOTH: concatenated variation and reference data\n        replace_unchanged_values_by_nan: If True, replaces values that are\n            identical between datasets with NaN (useful for highlighting changes)\n        fill_value: Value to use for missing data in subtraction operations\n        **kwargs: Additional arguments passed to child dataset fetch methods\n\n    Returns:\n        DataFrame or Series with comparison results\n\n    Example:\n\n        &gt;&gt;&gt; # Basic delta comparison\n        &gt;&gt;&gt; deltas = comparison.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Highlight only changed values\n        &gt;&gt;&gt; changes_only = comparison.fetch(\n        ...     'buses_t.marginal_price',\n        ...     replace_unchanged_values_by_nan=True\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Side-by-side comparison\n        &gt;&gt;&gt; both = comparison.fetch(\n        ...     'buses_t.marginal_price',\n        ...     comparison_type=ComparisonTypeEnum.BOTH\n        ... )\n    \"\"\"\n    return super().fetch(\n        flag=flag,\n        config=config,\n        comparison_type=comparison_type,\n        replace_unchanged_values_by_nan=replace_unchanged_values_by_nan,\n        fill_value=fill_value,\n        **kwargs\n    )\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.platform_dataset","title":"platform_dataset","text":""},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.platform_dataset.PlatformDataset","title":"PlatformDataset","text":"<p>               Bases: <code>Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>DatasetLinkCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>ABC</code></p> <p>Base class for platform-specific datasets with automatic interpreter management.</p> <p>PlatformDataset provides the foundation for integrating MESCAL with specific energy modeling platforms (PyPSA, PLEXOS, etc.). It manages a registry of data interpreters and automatically instantiates them to handle different types of platform data.</p> Key Features <ul> <li>Automatic interpreter registration and instantiation</li> <li>Type-safe interpreter management through generics</li> <li>Flexible argument passing to interpreter constructors</li> <li>Support for study-specific interpreter extensions</li> <li>Unified data access through DatasetLinkCollection routing</li> </ul> Architecture <ul> <li>Uses DatasetLinkCollection for automatic flag routing</li> <li>Manages interpreter registry at class level</li> <li>Auto-instantiates all registered interpreters on construction</li> <li>Supports inheritance and interpreter registration on subclasses</li> </ul> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>DatasetType</code> <p>Base type for all interpreters (must be Dataset subclass)</p> required <code>DatasetConfigType</code> <p>Configuration class for dataset behavior</p> required <code>FlagType</code> <p>Type used for data flag identification</p> required <code>FlagIndexType</code> <p>Flag index implementation for flag mapping</p> required Class Attributes <p>_interpreter_registry: List of registered interpreter classes</p> Usage Pattern <ol> <li>Create platform dataset class inheriting from PlatformDataset</li> <li>Define get_child_dataset_type() to specify interpreter base class</li> <li>Create interpreter classes inheriting from the base interpreter</li> <li>Register interpreters using @PlatformDataset.register_interpreter</li> <li>Instantiate platform dataset - interpreters are auto-created</li> </ol> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Define platform dataset\n&gt;&gt;&gt; class PyPSADataset(PlatformDataset[PyPSAInterpreter, ...]):\n...     @classmethod\n...     def get_child_dataset_type(cls):\n...         return PyPSAInterpreter\n...\n&gt;&gt;&gt; # Register core interpreters\n&gt;&gt;&gt; @PyPSADataset.register_interpreter\n... class PyPSAModelInterpreter(PyPSAInterpreter):\n...     @property\n...     def accepted_flags(self):\n...         return {'buses', 'generators', 'lines'}\n...\n&gt;&gt;&gt; @PyPSADataset.register_interpreter  \n... class PyPSATimeSeriesInterpreter(PyPSAInterpreter):\n...     @property\n...     def accepted_flags(self):\n...         return {'buses_t.marginal_price', 'generators_t.p'}\n...\n&gt;&gt;&gt; # Register study-specific interpreter\n&gt;&gt;&gt; @PyPSADataset.register_interpreter\n... class CustomVariableInterpreter(PyPSAInterpreter):\n...     @property\n...     def accepted_flags(self):\n...         return {'custom_metric'}\n...\n&gt;&gt;&gt; # Use platform dataset\n&gt;&gt;&gt; dataset = PyPSADataset(network=my_network)\n&gt;&gt;&gt; buses = dataset.fetch('buses')  # Routes to ModelInterpreter\n&gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price')  # Routes to TimeSeriesInterpreter\n&gt;&gt;&gt; custom = dataset.fetch('custom_metric')  # Routes to CustomVariableInterpreter\n</code></pre> Notes <ul> <li>Interpreters are registered at class level and shared across instances</li> <li>Registration order affects routing (last registered = first checked)</li> <li>All registered interpreters are instantiated for each platform dataset</li> <li>Constructor arguments are automatically extracted and passed to interpreters</li> </ul> Source code in <code>submodules/mescal/mescal/datasets/platform_dataset.py</code> <pre><code>class PlatformDataset(\n    Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    DatasetLinkCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    ABC\n):\n    \"\"\"\n    Base class for platform-specific datasets with automatic interpreter management.\n\n    PlatformDataset provides the foundation for integrating MESCAL with specific\n    energy modeling platforms (PyPSA, PLEXOS, etc.). It manages a registry of\n    data interpreters and automatically instantiates them to handle different\n    types of platform data.\n\n    Key Features:\n        - Automatic interpreter registration and instantiation\n        - Type-safe interpreter management through generics\n        - Flexible argument passing to interpreter constructors\n        - Support for study-specific interpreter extensions\n        - Unified data access through DatasetLinkCollection routing\n\n    Architecture:\n        - Uses DatasetLinkCollection for automatic flag routing\n        - Manages interpreter registry at class level\n        - Auto-instantiates all registered interpreters on construction\n        - Supports inheritance and interpreter registration on subclasses\n\n    Type Parameters:\n        DatasetType: Base type for all interpreters (must be Dataset subclass)\n        DatasetConfigType: Configuration class for dataset behavior\n        FlagType: Type used for data flag identification\n        FlagIndexType: Flag index implementation for flag mapping\n\n    Class Attributes:\n        _interpreter_registry: List of registered interpreter classes\n\n    Usage Pattern:\n        1. Create platform dataset class inheriting from PlatformDataset\n        2. Define get_child_dataset_type() to specify interpreter base class\n        3. Create interpreter classes inheriting from the base interpreter\n        4. Register interpreters using @PlatformDataset.register_interpreter\n        5. Instantiate platform dataset - interpreters are auto-created\n\n    Example:\n\n        &gt;&gt;&gt; # Define platform dataset\n        &gt;&gt;&gt; class PyPSADataset(PlatformDataset[PyPSAInterpreter, ...]):\n        ...     @classmethod\n        ...     def get_child_dataset_type(cls):\n        ...         return PyPSAInterpreter\n        ...\n        &gt;&gt;&gt; # Register core interpreters\n        &gt;&gt;&gt; @PyPSADataset.register_interpreter\n        ... class PyPSAModelInterpreter(PyPSAInterpreter):\n        ...     @property\n        ...     def accepted_flags(self):\n        ...         return {'buses', 'generators', 'lines'}\n        ...\n        &gt;&gt;&gt; @PyPSADataset.register_interpreter  \n        ... class PyPSATimeSeriesInterpreter(PyPSAInterpreter):\n        ...     @property\n        ...     def accepted_flags(self):\n        ...         return {'buses_t.marginal_price', 'generators_t.p'}\n        ...\n        &gt;&gt;&gt; # Register study-specific interpreter\n        &gt;&gt;&gt; @PyPSADataset.register_interpreter\n        ... class CustomVariableInterpreter(PyPSAInterpreter):\n        ...     @property\n        ...     def accepted_flags(self):\n        ...         return {'custom_metric'}\n        ...\n        &gt;&gt;&gt; # Use platform dataset\n        &gt;&gt;&gt; dataset = PyPSADataset(network=my_network)\n        &gt;&gt;&gt; buses = dataset.fetch('buses')  # Routes to ModelInterpreter\n        &gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price')  # Routes to TimeSeriesInterpreter\n        &gt;&gt;&gt; custom = dataset.fetch('custom_metric')  # Routes to CustomVariableInterpreter\n\n    Notes:\n        - Interpreters are registered at class level and shared across instances\n        - Registration order affects routing (last registered = first checked)\n        - All registered interpreters are instantiated for each platform dataset\n        - Constructor arguments are automatically extracted and passed to interpreters\n    \"\"\"\n\n    _interpreter_registry: list[Type[DatasetType]] = []\n\n    def __init__(\n            self,\n            name: str = None,\n            flag_index: FlagIndexType = None,\n            attributes: dict = None,\n            database: Database = None,\n            config: DatasetConfigType = None,\n            **kwargs\n    ):\n        super().__init__(\n            datasets=[],\n            name=name,\n            flag_index=flag_index,\n            attributes=attributes,\n            database=database,\n            config=config,\n        )\n        interpreter_args = self._prepare_interpreter_initialization_args(kwargs)\n        datasets = self._initialize_registered_interpreters(interpreter_args)\n        self.add_datasets(datasets)\n\n    @classmethod\n    def register_interpreter(cls, interpreter: Type[DatasetType]) -&gt; Type['DatasetType']:\n        \"\"\"\n        Register a data interpreter class with this platform dataset.\n\n        This method is typically used as a decorator to register interpreter classes\n        that handle specific types of platform data. Registered interpreters are\n        automatically instantiated when the platform dataset is created.\n\n        Args:\n            interpreter: Interpreter class that must inherit from get_child_dataset_type()\n\n        Returns:\n            The interpreter class (unchanged) to support decorator usage\n\n        Raises:\n            TypeError: If interpreter doesn't inherit from the required base class\n\n        Example:\n\n            &gt;&gt;&gt; @PyPSADataset.register_interpreter\n            ... class CustomInterpreter(PyPSAInterpreter):\n            ...     @property\n            ...     def accepted_flags(self):\n            ...         return {'custom_flag'}\n            ...     \n            ...     def _fetch(self, flag, config, **kwargs):\n            ...         return compute_custom_data()\n        \"\"\"\n        cls._validate_interpreter_type(interpreter)\n        if interpreter not in cls._interpreter_registry:\n            cls._add_interpreter_to_registry(interpreter)\n        return interpreter\n\n    @classmethod\n    def get_registered_interpreters(cls) -&gt; list[Type[DatasetType]]:\n        return cls._interpreter_registry.copy()\n\n    def get_interpreter_instance(self, interpreter_type: Type[DatasetType]) -&gt; DatasetType:\n        interpreter = self._find_interpreter_instance(interpreter_type)\n        if interpreter is None:\n            raise ValueError(\n                f'No interpreter instance found for type {interpreter_type.__name__}'\n            )\n        return interpreter\n\n    def get_flags_by_interpreter(self) -&gt; dict[Type[DatasetType], set[FlagType]]:\n        return {\n            type(interpreter): interpreter.accepted_flags\n            for interpreter in self.datasets.values()\n        }\n\n    def _prepare_interpreter_initialization_args(self, kwargs: dict) -&gt; dict:\n        interpreter_signature = InterpreterSignature.from_interpreter(self.get_child_dataset_type())\n        return {\n            arg: kwargs.get(arg, default)\n            for arg, default in zip(interpreter_signature.args, interpreter_signature.defaults)\n        }\n\n    def _initialize_registered_interpreters(self, interpreter_args: dict) -&gt; list[DatasetType]:\n        return [\n            interpreter(**interpreter_args, parent_dataset=self)\n            for interpreter in self._interpreter_registry\n        ]\n\n    @classmethod\n    def _validate_interpreter_type(cls, interpreter: Type[DatasetType]) -&gt; None:\n        if not issubclass(interpreter, cls.get_child_dataset_type()):\n            raise TypeError(\n                f'Interpreter must be subclass of {cls.get_child_dataset_type().__name__}'\n            )\n\n    @classmethod\n    def _validate_interpreter_not_registered(cls, interpreter: Type[DatasetType]) -&gt; None:\n        if interpreter in cls._interpreter_registry:\n            raise ValueError(f'Interpreter {interpreter.__name__} already registered')\n\n    @classmethod\n    def _add_interpreter_to_registry(cls, interpreter: Type[DatasetType]) -&gt; None:\n        cls._interpreter_registry.insert(0, interpreter)\n\n    def _find_interpreter_instance(self, interpreter_type: Type[DatasetType]) -&gt; DatasetType | None:\n        for interpreter in self.datasets.values():\n            if isinstance(interpreter, interpreter_type):\n                return interpreter\n        return None\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.platform_dataset.PlatformDataset.register_interpreter","title":"register_interpreter  <code>classmethod</code>","text":"<pre><code>register_interpreter(interpreter: Type[DatasetType]) -&gt; Type['DatasetType']\n</code></pre> <p>Register a data interpreter class with this platform dataset.</p> <p>This method is typically used as a decorator to register interpreter classes that handle specific types of platform data. Registered interpreters are automatically instantiated when the platform dataset is created.</p> <p>Parameters:</p> Name Type Description Default <code>interpreter</code> <code>Type[DatasetType]</code> <p>Interpreter class that must inherit from get_child_dataset_type()</p> required <p>Returns:</p> Type Description <code>Type['DatasetType']</code> <p>The interpreter class (unchanged) to support decorator usage</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interpreter doesn't inherit from the required base class</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; @PyPSADataset.register_interpreter\n... class CustomInterpreter(PyPSAInterpreter):\n...     @property\n...     def accepted_flags(self):\n...         return {'custom_flag'}\n...     \n...     def _fetch(self, flag, config, **kwargs):\n...         return compute_custom_data()\n</code></pre> Source code in <code>submodules/mescal/mescal/datasets/platform_dataset.py</code> <pre><code>@classmethod\ndef register_interpreter(cls, interpreter: Type[DatasetType]) -&gt; Type['DatasetType']:\n    \"\"\"\n    Register a data interpreter class with this platform dataset.\n\n    This method is typically used as a decorator to register interpreter classes\n    that handle specific types of platform data. Registered interpreters are\n    automatically instantiated when the platform dataset is created.\n\n    Args:\n        interpreter: Interpreter class that must inherit from get_child_dataset_type()\n\n    Returns:\n        The interpreter class (unchanged) to support decorator usage\n\n    Raises:\n        TypeError: If interpreter doesn't inherit from the required base class\n\n    Example:\n\n        &gt;&gt;&gt; @PyPSADataset.register_interpreter\n        ... class CustomInterpreter(PyPSAInterpreter):\n        ...     @property\n        ...     def accepted_flags(self):\n        ...         return {'custom_flag'}\n        ...     \n        ...     def _fetch(self, flag, config, **kwargs):\n        ...         return compute_custom_data()\n    \"\"\"\n    cls._validate_interpreter_type(interpreter)\n    if interpreter not in cls._interpreter_registry:\n        cls._add_interpreter_to_registry(interpreter)\n    return interpreter\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/datasets/#mescal.datasets.dataset_config","title":"dataset_config","text":""},{"location":"mescal-package-documentation/api_reference/study_manager/","title":"StudyManager References","text":""},{"location":"mescal-package-documentation/api_reference/study_manager/#mescal.study_manager.StudyManager","title":"StudyManager","text":"<p>Central orchestrator for multi-scenario energy systems analysis using MESCAL's three-tier collection system.</p> <p>StudyManager provides unified access to scenarios, scenario comparisons, and combined datasets, implementing MESCAL's core architectural principle that \"Everything is a Dataset\". It manages the complete lifecycle of multi-scenario studies including data organization, automatic delta computation, and export functionality.</p> The class implements three primary data access patterns <ul> <li><code>.scen</code>: Access to individual scenario data (DatasetConcatCollection)</li> <li><code>.comp</code>: Access to scenario comparison deltas (DatasetConcatCollectionOfComparisons)</li> <li><code>.scen_comp</code>: Unified access to both scenarios and comparisons with type distinction</li> </ul> <p>Attributes:</p> Name Type Description <code>scen</code> <code>DatasetConcatCollection</code> <p>Collection of scenario datasets with consistent .fetch() interface</p> <code>comp</code> <code>DatasetConcatCollectionOfComparisons</code> <p>Collection of scenario comparisons with automatic delta calculation</p> <code>scen_comp</code> <code>DatasetConcatCollection</code> <p>Unified collection combining scenarios and comparisons</p> <p>Examples:</p> <p>Basic multi-scenario study setup:</p> <pre><code>&gt;&gt;&gt; from mescal import StudyManager\n&gt;&gt;&gt; from mescal_pypsa import PyPSADataset\n&gt;&gt;&gt; \n&gt;&gt;&gt; study = StudyManager.factory_from_scenarios(\n...     scenarios=[\n...         PyPSADataset(base_network, name='base'),\n...         PyPSADataset(high_res_network, name='high_res'),\n...         PyPSADataset(low_gas_network, name='cheap_gas'),\n...     ],\n...     comparisons=[('high_res', 'base'), ('cheap_gas', 'base')],  # StudyManager handles automatic naming with '*variation_dataset_name* vs *reference_dataset_name*'\n... )\n</code></pre> <p>Three-tier data access:</p> <pre><code>&gt;&gt;&gt; # Access scenario data across all scenarios\n&gt;&gt;&gt; scenario_prices = study.scen.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; print(scenario_prices)  # Print price time-series df for all scenarios concatenated with MultiIndex\n    dataset              base        ... n_cheap_gas\n    Bus                     1    10  ...          99 99_220kV\n    snapshot                         ...\n    2011-01-01 00:00:00 -0.44  5.77  ...       23.83    23.79\n    2011-01-01 01:00:00 -0.58  6.10  ...       22.38    22.33\n    ...                   ...   ...  ...         ...      ...\n    2011-01-01 22:00:00 20.98 20.44  ...       22.72    22.67\n    2011-01-01 23:00:00 13.43 18.94  ...       22.59    22.54\n</code></pre> <pre><code>&gt;&gt;&gt; # Access comparison deltas automatically calculated across all comparisons\n&gt;&gt;&gt; price_changes = study.comp.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; print(price_changes)  # Print price time-series df for all comparisons concatenated with MultiIndex\n    dataset             high_res vs base        ... n_cheap_gas vs base\n    Bus                                1    10  ...                  99 99_220kV\n    snapshot                                    ...\n    2011-01-01 00:00:00             0.21  0.02  ...                0.10     0.11\n    2011-01-01 01:00:00             0.02  0.06  ...               -0.81    -0.82\n    ...                              ...   ...  ...                 ...      ...\n    2011-01-01 22:00:00             0.04 -0.05  ...               -2.10    -2.14\n    2011-01-01 23:00:00             2.33  0.46  ...               -1.74    -1.78\n</code></pre> <pre><code>&gt;&gt;&gt; # Access unified view with type-level distinction\n&gt;&gt;&gt; unified_scen_and_comp_price_data = study.scen_comp.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; print(unified_scen_and_comp_price_data)  # Print price time-series df for all scenarios and comparisons concatenated with additional level on MultiIndex:\n    type                scenario        ...          comparison\n    dataset                 base        ... n_cheap_gas vs base\n    Bus                        1    10  ...                  99 99_220kV\n    snapshot                            ...\n    2011-01-01 00:00:00    -0.44  5.77  ...                0.10     0.11\n    2011-01-01 01:00:00    -0.58  6.10  ...               -0.81    -0.82\n    ..                       ...   ...  ...                 ...      ...\n    2011-01-01 22:00:00    20.98 20.44  ...               -2.10    -2.14\n    2011-01-01 23:00:00    13.43 18.94  ...               -1.74    -1.78\n</code></pre> <p>Access individual datasets:</p> <pre><code>&gt;&gt;&gt; # Access to data from individual scenario\n&gt;&gt;&gt; ds_base = study.scen.get_dataset('base')\n&gt;&gt;&gt; base_prices = ds_base.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; print(base_prices)  # Print price time-series df for base scenario\n    Bus                     1    10  ...    99  99_220kV\n    snapshot                         ...\n    2011-01-01 00:00:00 -0.44  5.77  ... 23.72     23.69\n    2011-01-01 01:00:00 -0.58  6.10  ... 23.19     23.14\n    ...                   ...   ...  ...   ...       ...\n    2011-01-01 22:00:00 20.98 20.44  ... 24.81     24.81\n    2011-01-01 23:00:00 13.43 18.94  ... 24.33     24.32\n</code></pre> <pre><code>&gt;&gt;&gt; base_buses_model_df = ds_base.fetch('buses')\n&gt;&gt;&gt; print(base_buses_model_df)  # Print bus model df for base scenario\n               v_nom type  ...  v_mag_pu_max  control\n    Bus                    ...\n    1         220.00       ...           inf    Slack\n    2         380.00       ...           inf       PQ\n    ...          ...  ...  ...           ...      ...\n    450_220kV 220.00       ...           inf       PQ\n    458_220kV 220.00       ...           inf       PQ\n</code></pre> <pre><code>&gt;&gt;&gt; # Access to individual comparison dataset\n&gt;&gt;&gt; ds_comp_high_res = study.comp.get_dataset('high_res vs base')\n&gt;&gt;&gt; price_changes_high_res = ds_comp_high_res.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; print(price_changes_high_res)  # Print price time-series df changes (deltas) comparing high_res to base\n    Bus                    1    10  ...    99  99_220kV\n    snapshot                        ...\n    2011-01-01 00:00:00 0.21  0.02  ...  0.01      0.01\n    2011-01-01 01:00:00 0.02  0.06  ... -0.07     -0.07\n    ...                  ...   ...  ...   ...       ...\n    2011-01-01 22:00:00 0.04 -0.05  ... -0.00     -0.00\n    2011-01-01 23:00:00 2.33  0.46  ... -0.09     -0.09\n</code></pre> <pre><code>&gt;&gt;&gt; bus_model_changes_high_res = ds_comp_high_res.fetch('buses')\n&gt;&gt;&gt; print(bus_model_changes_high_res)  # Print bus model df changes comparing high_res to base\n               v_nom type  ...  v_mag_pu_max  control\n    Bus                    ...\n    1           0.00       ...           NaN    Slack\n    2           0.00       ...           NaN       PQ\n    ...          ...  ...  ...           ...      ...\n    450_220kV   0.00       ...           NaN       PQ\n    458_220kV   0.00       ...           NaN       PQ\n</code></pre> <p>Access to merged (model) dfs for all sub datasets:</p> <pre><code>&gt;&gt;&gt; # Access merged dataframe (useful to get unified model_df in case of different objects across scenarios)\n&gt;&gt;&gt; bus_model_df_merged = study.scen.fetch_merged('buses')\n&gt;&gt;&gt; print(bus_model_df_merged)  # Print merged bus model df for all scenarios\n               v_nom type  ...  v_mag_pu_max  control\n    Bus                    ...\n    1         220.00       ...           inf    Slack\n    2         380.00       ...           inf       PQ\n    ...          ...  ...  ...           ...      ...\n    450_220kV 220.00       ...           inf       PQ\n    458_220kV 220.00       ...           inf       PQ\n</code></pre> Notes <ul> <li>All collections share the same .fetch(flag) interface for consistent data access</li> <li>Scenario comparisons are computed automatically as deltas (variation - reference)</li> <li>The unified collection uses 'type' as the concat_level_name to distinguish data sources</li> <li>Supports dynamic addition of scenarios and comparisons after initialization</li> </ul> Source code in <code>submodules/mescal/mescal/study_manager.py</code> <pre><code>class StudyManager:\n    \"\"\"\n    Central orchestrator for multi-scenario energy systems analysis using MESCAL's three-tier collection system.\n\n    StudyManager provides unified access to scenarios, scenario comparisons, and combined datasets,\n    implementing MESCAL's core architectural principle that \"Everything is a Dataset\". It manages\n    the complete lifecycle of multi-scenario studies including data organization, automatic delta\n    computation, and export functionality.\n\n    The class implements three primary data access patterns:\n        - `.scen`: Access to individual scenario data (DatasetConcatCollection)\n        - `.comp`: Access to scenario comparison deltas (DatasetConcatCollectionOfComparisons)\n        - `.scen_comp`: Unified access to both scenarios and comparisons with type distinction\n\n    Attributes:\n        scen (DatasetConcatCollection): Collection of scenario datasets with consistent .fetch() interface\n        comp (DatasetConcatCollectionOfComparisons): Collection of scenario comparisons with automatic delta calculation\n        scen_comp (DatasetConcatCollection): Unified collection combining scenarios and comparisons\n\n    Examples:\n        Basic multi-scenario study setup:\n\n        &gt;&gt;&gt; from mescal import StudyManager\n        &gt;&gt;&gt; from mescal_pypsa import PyPSADataset\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; study = StudyManager.factory_from_scenarios(\n        ...     scenarios=[\n        ...         PyPSADataset(base_network, name='base'),\n        ...         PyPSADataset(high_res_network, name='high_res'),\n        ...         PyPSADataset(low_gas_network, name='cheap_gas'),\n        ...     ],\n        ...     comparisons=[('high_res', 'base'), ('cheap_gas', 'base')],  # StudyManager handles automatic naming with '*variation_dataset_name* vs *reference_dataset_name*'\n        ... )\n\n\n        Three-tier data access:\n\n        &gt;&gt;&gt; # Access scenario data across all scenarios\n        &gt;&gt;&gt; scenario_prices = study.scen.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; print(scenario_prices)  # Print price time-series df for all scenarios concatenated with MultiIndex\n            dataset              base        ... n_cheap_gas\n            Bus                     1    10  ...          99 99_220kV\n            snapshot                         ...\n            2011-01-01 00:00:00 -0.44  5.77  ...       23.83    23.79\n            2011-01-01 01:00:00 -0.58  6.10  ...       22.38    22.33\n            ...                   ...   ...  ...         ...      ...\n            2011-01-01 22:00:00 20.98 20.44  ...       22.72    22.67\n            2011-01-01 23:00:00 13.43 18.94  ...       22.59    22.54\n\n        &gt;&gt;&gt; # Access comparison deltas automatically calculated across all comparisons\n        &gt;&gt;&gt; price_changes = study.comp.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; print(price_changes)  # Print price time-series df for all comparisons concatenated with MultiIndex\n            dataset             high_res vs base        ... n_cheap_gas vs base\n            Bus                                1    10  ...                  99 99_220kV\n            snapshot                                    ...\n            2011-01-01 00:00:00             0.21  0.02  ...                0.10     0.11\n            2011-01-01 01:00:00             0.02  0.06  ...               -0.81    -0.82\n            ...                              ...   ...  ...                 ...      ...\n            2011-01-01 22:00:00             0.04 -0.05  ...               -2.10    -2.14\n            2011-01-01 23:00:00             2.33  0.46  ...               -1.74    -1.78\n\n        &gt;&gt;&gt; # Access unified view with type-level distinction\n        &gt;&gt;&gt; unified_scen_and_comp_price_data = study.scen_comp.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; print(unified_scen_and_comp_price_data)  # Print price time-series df for all scenarios and comparisons concatenated with additional level on MultiIndex:\n            type                scenario        ...          comparison\n            dataset                 base        ... n_cheap_gas vs base\n            Bus                        1    10  ...                  99 99_220kV\n            snapshot                            ...\n            2011-01-01 00:00:00    -0.44  5.77  ...                0.10     0.11\n            2011-01-01 01:00:00    -0.58  6.10  ...               -0.81    -0.82\n            ..                       ...   ...  ...                 ...      ...\n            2011-01-01 22:00:00    20.98 20.44  ...               -2.10    -2.14\n            2011-01-01 23:00:00    13.43 18.94  ...               -1.74    -1.78\n\n\n        Access individual datasets:\n\n        &gt;&gt;&gt; # Access to data from individual scenario\n        &gt;&gt;&gt; ds_base = study.scen.get_dataset('base')\n        &gt;&gt;&gt; base_prices = ds_base.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; print(base_prices)  # Print price time-series df for base scenario\n            Bus                     1    10  ...    99  99_220kV\n            snapshot                         ...\n            2011-01-01 00:00:00 -0.44  5.77  ... 23.72     23.69\n            2011-01-01 01:00:00 -0.58  6.10  ... 23.19     23.14\n            ...                   ...   ...  ...   ...       ...\n            2011-01-01 22:00:00 20.98 20.44  ... 24.81     24.81\n            2011-01-01 23:00:00 13.43 18.94  ... 24.33     24.32\n\n        &gt;&gt;&gt; base_buses_model_df = ds_base.fetch('buses')\n        &gt;&gt;&gt; print(base_buses_model_df)  # Print bus model df for base scenario\n                       v_nom type  ...  v_mag_pu_max  control\n            Bus                    ...\n            1         220.00       ...           inf    Slack\n            2         380.00       ...           inf       PQ\n            ...          ...  ...  ...           ...      ...\n            450_220kV 220.00       ...           inf       PQ\n            458_220kV 220.00       ...           inf       PQ\n\n        &gt;&gt;&gt; # Access to individual comparison dataset\n        &gt;&gt;&gt; ds_comp_high_res = study.comp.get_dataset('high_res vs base')\n        &gt;&gt;&gt; price_changes_high_res = ds_comp_high_res.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; print(price_changes_high_res)  # Print price time-series df changes (deltas) comparing high_res to base\n            Bus                    1    10  ...    99  99_220kV\n            snapshot                        ...\n            2011-01-01 00:00:00 0.21  0.02  ...  0.01      0.01\n            2011-01-01 01:00:00 0.02  0.06  ... -0.07     -0.07\n            ...                  ...   ...  ...   ...       ...\n            2011-01-01 22:00:00 0.04 -0.05  ... -0.00     -0.00\n            2011-01-01 23:00:00 2.33  0.46  ... -0.09     -0.09\n\n        &gt;&gt;&gt; bus_model_changes_high_res = ds_comp_high_res.fetch('buses')\n        &gt;&gt;&gt; print(bus_model_changes_high_res)  # Print bus model df changes comparing high_res to base\n                       v_nom type  ...  v_mag_pu_max  control\n            Bus                    ...\n            1           0.00       ...           NaN    Slack\n            2           0.00       ...           NaN       PQ\n            ...          ...  ...  ...           ...      ...\n            450_220kV   0.00       ...           NaN       PQ\n            458_220kV   0.00       ...           NaN       PQ\n\n\n        Access to merged (model) dfs for all sub datasets:\n\n        &gt;&gt;&gt; # Access merged dataframe (useful to get unified model_df in case of different objects across scenarios)\n        &gt;&gt;&gt; bus_model_df_merged = study.scen.fetch_merged('buses')\n        &gt;&gt;&gt; print(bus_model_df_merged)  # Print merged bus model df for all scenarios\n                       v_nom type  ...  v_mag_pu_max  control\n            Bus                    ...\n            1         220.00       ...           inf    Slack\n            2         380.00       ...           inf       PQ\n            ...          ...  ...  ...           ...      ...\n            450_220kV 220.00       ...           inf       PQ\n            458_220kV 220.00       ...           inf       PQ\n\n    Notes:\n        - All collections share the same .fetch(flag) interface for consistent data access\n        - Scenario comparisons are computed automatically as deltas (variation - reference)\n        - The unified collection uses 'type' as the concat_level_name to distinguish data sources\n        - Supports dynamic addition of scenarios and comparisons after initialization\n    \"\"\"\n    def __init__(\n            self,\n            scenarios: DatasetConcatCollection,\n            comparisons: DatasetConcatCollectionOfComparisons,\n            export_folder: str = None,\n    ):\n        self._scenarios = scenarios\n        self._comparisons = comparisons\n        self._scenarios_and_comparisons: DatasetConcatCollection = DatasetConcatCollection(\n            [\n                scenarios,\n                comparisons\n            ],\n            name='scenarios_and_comparisons',\n            concat_level_name='type',\n        )\n        self._export_folder: str = export_folder\n        if export_folder is not None:\n            self._ensure_folder_exists(export_folder)\n\n    @property\n    def scen(self) -&gt; DatasetConcatCollection:\n        return self._scenarios\n\n    def add_scenario(self, dataset: Dataset):\n        self._scenarios.add_dataset(dataset)\n\n    @property\n    def comp(self) -&gt; DatasetConcatCollectionOfComparisons:\n        return self._comparisons\n\n    def add_comparison(self, dataset: DatasetComparison):\n        self._comparisons.add_dataset(dataset)\n\n    @property\n    def scen_comp(self) -&gt; DatasetConcatCollection:\n        return self._scenarios_and_comparisons\n\n    @property\n    def export_folder(self) -&gt; str:\n        return self._export_folder\n\n    @export_folder.setter\n    def export_folder(self, folder_path: str):\n        self._ensure_folder_exists(folder_path)\n        self._export_folder = folder_path\n\n    @staticmethod\n    def _ensure_folder_exists(folder: str):\n        os.makedirs(folder, exist_ok=True)\n\n    def export_path(self, file_name: str) -&gt; str:\n        if self._export_folder is None:\n            raise RuntimeError(f'Export folder must be assigned first.')\n        return os.path.join(self._export_folder, file_name)\n\n    @classmethod\n    def factory_from_scenarios(\n            cls,\n            scenarios: list[Dataset],\n            comparisons: list[tuple[str, str]],\n            export_folder: str = None\n    ) -&gt; 'StudyManager':\n        scen = DatasetConcatCollection(scenarios, name='scenario', concat_level_name='dataset',)\n        comp = DatasetConcatCollectionOfComparisons(\n            datasets=[\n                DatasetComparison(scen.get_dataset(var), scen.get_dataset(ref))\n                for var, ref in comparisons\n            ],\n            name='comparison',\n            concat_level_name='dataset',\n        )\n        return cls(scen, comp, export_folder)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/","title":"Energy Data Handling Intro","text":"<p>Energy Data Handling Module.</p> <p>This comprehensive module provides utilities for handling energy-specific time series data operations, network flow analysis and area-level aggregations within the MESCAL framework.</p> <p>The module is designed to work with pandas time series data and energy system networks, particularly for energy market analysis where different data streams may have varying temporal granularities and require specific handling based on whether they represent intensive (prices, power levels) or extensive (volumes, energy amounts) quantities.</p>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/#mescal.energy_data_handling--core-components","title":"Core Components","text":"Time Series Processing <ul> <li>Granularity analysis and conversion between different temporal resolutions</li> <li>Gap detection and handling in time series data</li> <li>Support for intensive vs. extensive quantity conversions</li> </ul> Network Flow Analysis <ul> <li>Bidirectional transmission line flow data structures</li> <li>Network capacity modeling with loss considerations</li> <li>Flow direction conventions for complex network topologies</li> </ul> Area-Level Aggregations <ul> <li>Node-to-area aggregation modules with geographic modeling</li> <li>Cross-border flow analysis and capacity calculation modules</li> <li>Price aggregation modules using volume-weighted methods</li> </ul> Variable Utilities <ul> <li>Regional trade balance calculations</li> <li>Volume-weighted price aggregations</li> <li>Congestion rent analysis</li> <li>Directional data processing (up/down, net flows)</li> </ul> Model Handling <ul> <li>DataFrame enrichment with model properties</li> <li>Membership-based property propagation</li> <li>Combination identifier creation for paired relationships</li> </ul>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/granularity_modules/","title":"MESCAL Granularity Modules","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/granularity_modules/#mescal.energy_data_handling.granularity_analyzer.TimeSeriesGranularityAnalyzer","title":"TimeSeriesGranularityAnalyzer","text":"<p>Analyzes and validates time granularity in DatetimeIndex sequences.</p> <p>This class provides tools for working with time series data that may have varying granularities (e.g., hourly, quarter-hourly). It's particularly useful for electricity market data analysis where different market products can have different time resolutions.</p> Features <ul> <li>Granularity detection for time series data</li> <li>Support for mixed granularities within the same series</li> <li>Strict mode for validation scenarios</li> <li>Per-day granularity analysis</li> </ul> <p>Parameters:</p> Name Type Description Default <code>strict_mode</code> <code>bool</code> <p>If True, raises GranularityError when multiple granularities are detected. If False, only issues warnings. Default is True.</p> <code>True</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; analyzer = TimeSeriesGranularityAnalyzer()\n&gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=24, freq='h')\n&gt;&gt;&gt; analyzer.get_granularity_as_timedelta(index)\n    Timedelta('1 hours')\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/granularity_analyzer.py</code> <pre><code>class TimeSeriesGranularityAnalyzer:\n    \"\"\"Analyzes and validates time granularity in DatetimeIndex sequences.\n\n    This class provides tools for working with time series data that may have varying\n    granularities (e.g., hourly, quarter-hourly). It's particularly useful for\n    electricity market data analysis where different market products can have\n    different time resolutions.\n\n    Features:\n        - Granularity detection for time series data\n        - Support for mixed granularities within the same series\n        - Strict mode for validation scenarios\n        - Per-day granularity analysis\n\n    Args:\n        strict_mode: If True, raises GranularityError when multiple granularities\n            are detected. If False, only issues warnings. Default is True.\n\n    Example:\n\n        &gt;&gt;&gt; analyzer = TimeSeriesGranularityAnalyzer()\n        &gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=24, freq='h')\n        &gt;&gt;&gt; analyzer.get_granularity_as_timedelta(index)\n            Timedelta('1 hours')\n    \"\"\"\n\n    def __init__(self, strict_mode: bool = True):\n        self._strict_mode = strict_mode\n\n    @property\n    def strict_mode(self) -&gt; bool:\n        return self._strict_mode\n\n    @strict_mode.setter\n    def strict_mode(self, value: bool):\n        self._strict_mode = value\n\n    @_validate_dt_index\n    def get_granularity_as_series_of_timedeltas(self, dt_index: pd.DatetimeIndex) -&gt; pd.Series:\n        s = pd.Series(index=dt_index)\n        s = s.groupby(dt_index.date).apply(lambda g: self._get_granularity_for_day(g.index))\n        s = s.droplevel(0, axis=0)\n        if dt_index.name is not None:\n            s.index.name = dt_index.name\n        return s\n\n    @_validate_dt_index\n    def get_granularity_as_series_of_minutes(self, dt_index: pd.DatetimeIndex) -&gt; pd.Series:\n        return self.get_granularity_as_series_of_timedeltas(dt_index).apply(lambda x: x.total_seconds() / 60)\n\n    @_validate_dt_index\n    def get_granularity_as_series_of_hours(self, dt_index: pd.DatetimeIndex) -&gt; pd.Series:\n        return self.get_granularity_as_series_of_timedeltas(dt_index).apply(lambda x: x.total_seconds() / 3600)\n\n    def _get_granularity_for_day(self, dt_index: pd.DatetimeIndex) -&gt; pd.Series:\n        gran = dt_index.to_series().diff().shift(-1)\n        if len(gran) &gt; 1:\n            gran.iloc[-1] = gran.iloc[-2]\n\n        if gran.nunique() &gt; 1:\n            msg = f'Multiple granularities identified within a day: {gran.unique()}'\n            if self._strict_mode:\n                raise GranularityError(msg)\n            warnings.warn(msg)\n        return gran\n\n    @_validate_dt_index\n    def get_granularity_as_timedelta(self, dt_index: pd.DatetimeIndex) -&gt; pd.Timedelta:\n        if len(dt_index) == 0:\n            return pd.Timedelta(0)\n\n        gran = self.get_granularity_as_series_of_timedeltas(dt_index)\n        first_gran = gran.iloc[0]\n\n        if gran.nunique() &gt; 1:\n            msg = (f'Multiple granularities found: {gran.unique()}. '\n                   f'Using {first_gran} as the reference granularity.')\n            if self._strict_mode:\n                raise GranularityError(msg)\n            warnings.warn(msg)\n        return first_gran\n\n    @_validate_dt_index\n    def get_granularity_as_hours(self, dt_index: pd.DatetimeIndex) -&gt; float:\n        return self.get_granularity_as_timedelta(dt_index).total_seconds() / 3600\n\n    @_validate_dt_index\n    def get_granularity_as_minutes(self, dt_index: pd.DatetimeIndex) -&gt; float:\n        return self.get_granularity_as_timedelta(dt_index).total_seconds() / 60\n\n    @_validate_dt_index\n    def validate_constant_granularity(self, dt_index: pd.DatetimeIndex, expected_hours: float) -&gt; bool:\n        actual_hours = self.get_granularity_as_hours(dt_index)\n        return abs(actual_hours - expected_hours) &lt; 1e-10\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/granularity_modules/#mescal.energy_data_handling.granularity_converter.TimeSeriesGranularityConverter","title":"TimeSeriesGranularityConverter","text":"<p>Converts time series between different granularities while respecting the nature of the quantity.</p> <p>This class handles the conversion of time series data between different granularities (e.g., hourly to 15-min or vice versa) while properly accounting for the physical nature of the quantity being converted:</p> <ul> <li> <p>Intensive quantities (e.g., prices, power levels) are replicated when increasing   granularity and averaged when decreasing granularity.</p> </li> <li> <p>Extensive quantities (e.g., volumes, welfare) are split when increasing   granularity and summed when decreasing granularity.</p> </li> </ul> Features <ul> <li>Automatic granularity detection using TimeGranularityAnalyzer</li> <li>Per-day processing to handle missing periods properly and prevent incorrect autofilling of missing days</li> <li>Support for both intensive and extensive quantities</li> <li>Timezone-aware processing including daylight saving transitions</li> </ul> Source code in <code>submodules/mescal/mescal/energy_data_handling/granularity_converter.py</code> <pre><code>class TimeSeriesGranularityConverter:\n    \"\"\"Converts time series between different granularities while respecting the nature of the quantity.\n\n    This class handles the conversion of time series data between different granularities\n    (e.g., hourly to 15-min or vice versa) while properly accounting for the physical\n    nature of the quantity being converted:\n\n    - Intensive quantities (e.g., prices, power levels) are replicated when increasing\n      granularity and averaged when decreasing granularity.\n\n    - Extensive quantities (e.g., volumes, welfare) are split when increasing\n      granularity and summed when decreasing granularity.\n\n    Features:\n        - Automatic granularity detection using TimeGranularityAnalyzer\n        - Per-day processing to handle missing periods properly and prevent incorrect autofilling of missing days\n        - Support for both intensive and extensive quantities\n        - Timezone-aware processing including daylight saving transitions\n    \"\"\"\n    def __init__(self):\n        \"\"\"Initialize the granularity converter with analyzer instances.\n\n        Creates both strict and non-strict granularity analyzers for different\n        validation requirements during conversion operations.\n        \"\"\"\n        self._strict_gran_analyzer = TimeSeriesGranularityAnalyzer(strict_mode=True)\n        self._non_strict_gran_analyzer = TimeSeriesGranularityAnalyzer(strict_mode=False)\n\n    def _validate_series_format(self, series: pd.Series) -&gt; None:\n        \"\"\"Validate that the input series has the required DatetimeIndex format.\n\n        Args:\n            series: Time series to validate\n\n        Raises:\n            TypeError: If series index is not a DatetimeIndex\n        \"\"\"\n        if not isinstance(series.index, pd.DatetimeIndex):\n            raise TypeError(f\"Series index must be DatetimeIndex, got {type(series.index)}\")\n\n    def upsample_through_fillna(\n            self,\n            data: pd.DataFrame | pd.Series,\n            quantity_type: QuantityTypeEnum\n    ) -&gt; pd.DataFrame | pd.Series:\n        \"\"\"Upsample data using forward-fill strategy with quantity-type-aware scaling.\n\n        This method handles upsampling of sparse data where some values are missing.\n        It uses forward-fill to propagate values and applies appropriate scaling\n        based on the quantity type:\n\n        - For INTENSIVE quantities: Values are replicated without scaling\n        - For EXTENSIVE quantities: Values are divided by the number of periods\n          they are spread across within each hour-segment group\n\n        The method processes data per day and hour to handle missing periods properly\n        and prevent incorrect auto-filling across day boundaries.\n\n        Args:\n            data: Time series data to upsample (Series or DataFrame)\n            quantity_type: Type of quantity being converted (INTENSIVE or EXTENSIVE)\n\n        Returns:\n            Upsampled data with same type as input\n\n        Example:\n\n            &gt;&gt;&gt; # For extensive quantities (energy), values are divided\n            &gt;&gt;&gt; series = pd.Series([100, np.nan, np.nan, np.nan, 200, np.nan, np.nan, np.nan],\n            ...                   index=pd.date_range('2024-01-01', freq='15min', periods=5))\n            &gt;&gt;&gt; converter.upsample_through_fillna(series, QuantityTypeEnum.EXTENSIVE)\n                # Results in [25, 25, 25, 25, 50, 50, 50, 50]\n        \"\"\"\n        if isinstance(data, pd.Series):\n            return self._upsample_series(data, quantity_type)\n\n        tmp = data.copy().sort_index()\n        idx = tmp.index.tz_convert('UTC') if tmp.index.tz is not None else tmp.index\n\n        if quantity_type == QuantityTypeEnum.EXTENSIVE:\n            segment_patterns = tmp.notna().cumsum()\n\n            # Group columns by their segment pattern\n            pattern_to_cols = {}\n            for col in tmp.columns:\n                pattern = tuple(segment_patterns[col].values)  # Convert to tuple to make it hashable\n                pattern_to_cols.setdefault(pattern, []).append(col)\n\n            # Process each group of columns with same pattern\n            result_pieces = []\n            for pattern, cols in pattern_to_cols.items():\n                segments = segment_patterns[cols[0]]  # Take segments from first column (all are same)\n                piece = (\n                    tmp[cols]\n                    .groupby([idx.date, idx.hour, segments])\n                    .transform(lambda s: s.ffill() / len(s))\n                )\n                result_pieces.append(piece)\n\n            return pd.concat(result_pieces, axis=1).loc[data.index].rename_axis(data.columns.names, axis=1)\n        else:\n            return tmp.groupby([idx.date, idx.hour]).ffill().loc[data.index]\n\n    def _upsample_series(self, series: pd.Series, quantity_type: QuantityTypeEnum) -&gt; pd.Series:\n        \"\"\"Helper method to upsample a Series using DataFrame-based upsampling.\n\n        Args:\n            series: Time series to upsample\n            quantity_type: Type of quantity being converted\n\n        Returns:\n            Upsampled series\n        \"\"\"\n        return self.upsample_through_fillna(\n            series.to_frame(),\n            quantity_type\n        ).iloc[:, 0]\n\n    def convert_to_target_index(\n            self,\n            series: pd.Series,\n            target_index: pd.DatetimeIndex,\n            quantity_type: QuantityTypeEnum\n    ) -&gt; pd.Series:\n        \"\"\"Convert a time series to match a specific target DatetimeIndex.\n\n        This method converts the granularity of a time series to match the granularity\n        of a target index. The target index must have consistent granularity within\n        each day and consistent granularity across all days.\n\n        Args:\n            series: Source time series to convert\n            target_index: DatetimeIndex defining the target granularity and timestamps\n            quantity_type: Type of quantity (INTENSIVE or EXTENSIVE) for proper scaling\n\n        Returns:\n            Series converted to match target index granularity and timestamps\n\n        Raises:\n            ValueError: If target index has multiple granularities within days\n                       or inconsistent granularity across days\n\n        Example:\n\n            &gt;&gt;&gt; # Convert hourly to 15-min data\n            &gt;&gt;&gt; hourly_series = pd.Series([100, 150, 200], \n            ...                          index=pd.date_range('2024-01-01', freq='1H', periods=3))\n            &gt;&gt;&gt; target_idx = pd.date_range('2024-01-01', freq='15min', periods=12)\n            &gt;&gt;&gt; result = converter.convert_to_target_index(hourly_series, target_idx,\n            ...                                          QuantityTypeEnum.INTENSIVE)\n        \"\"\"\n        self._validate_series_format(series)\n        target_gran_series = self._strict_gran_analyzer.get_granularity_as_series_of_timedeltas(target_index)\n        _grouped = target_gran_series.groupby(target_gran_series.index.date)\n        if _grouped.nunique().max() &gt; 1:\n            raise ValueError(f\"Found some dates with multiple granularities within same day. Can't handle that!\")\n        if _grouped.first().nunique() &gt; 1:\n            raise ValueError(f\"Found multiple granularities. Can't handle that!\")\n        target_granularity = pd.Timedelta(target_gran_series.values[0])\n        return series.groupby(series.index.date).apply(\n            lambda x: self._convert_date_to_target_granularity(x, target_granularity, quantity_type)\n        ).droplevel(0).rename_axis(series.index.name).rename(series.name)\n\n    def convert_to_target_granularity(\n            self,\n            series: pd.Series,\n            target_granularity: pd.Timedelta,\n            quantity_type: QuantityTypeEnum\n    ) -&gt; pd.Series:\n        \"\"\"Convert a time series to a specific target granularity.\n\n        This method converts the temporal granularity of a time series while properly\n        handling the physical nature of the quantity. The conversion is performed\n        day-by-day to prevent incorrect handling of missing days or daylight saving\n        time transitions.\n\n        Args:\n            series: Source time series to convert\n            target_granularity: Target granularity as a pandas Timedelta\n                               (e.g., pd.Timedelta(minutes=15) for 15-minute data)\n            quantity_type: Type of quantity for proper scaling:\n                          - INTENSIVE: Values are averaged/replicated (prices, power)\n                          - EXTENSIVE: Values are summed/split (volumes, energy)\n\n        Returns:\n            Series with converted granularity, maintaining original naming and metadata\n\n        Raises:\n            GranularityConversionError: If conversion cannot be performed due to\n                                       unsupported granularities or data issues\n\n        Example:\n\n            &gt;&gt;&gt; # Convert 15-minute to hourly data (downsampling)\n            &gt;&gt;&gt; quarter_hourly = pd.Series([25, 30, 35, 40], \n            ...                           index=pd.date_range('2024-01-01', freq='15min', periods=4))\n            &gt;&gt;&gt; hourly = converter.convert_to_target_granularity(\n            ...     quarter_hourly, pd.Timedelta(hours=1), QuantityTypeEnum.EXTENSIVE)\n            &gt;&gt;&gt; print(hourly)  # Result: [130] (25+30+35+40)\n        \"\"\"\n        self._validate_series_format(series)\n        return series.groupby(series.index.date).apply(\n            lambda x: self._convert_date_to_target_granularity(x, target_granularity, quantity_type)\n        ).droplevel(0).rename_axis(series.index.name).rename(series.name)\n\n    def _convert_date_to_target_granularity(\n        self,\n        series: pd.Series,\n        target_granularity: pd.Timedelta,\n        quantity_type: QuantityTypeEnum\n    ) -&gt; pd.Series:\n        \"\"\"Convert granularity for a single date's worth of data.\n\n        This internal method handles the actual conversion logic for data within\n        a single date range. It determines whether upsampling, downsampling, or\n        no conversion is needed, then applies the appropriate method.\n\n        Args:\n            series: Time series data for a single date (may span into next date)\n            target_granularity: Target granularity as Timedelta\n            quantity_type: Type of quantity for scaling decisions\n\n        Returns:\n            Converted series for the date period\n\n        Raises:\n            GranularityConversionError: If conversion parameters are invalid or\n                                       unsupported granularities are encountered\n        \"\"\"\n        if len(set(series.index.date)) &gt; 2:\n            raise GranularityConversionError('This method is intended for single-date conversion only.')\n        source_gran = self._non_strict_gran_analyzer.get_granularity_as_series_of_minutes(series.index)\n        if len(source_gran.unique()) &gt; 1:\n            raise GranularityConversionError('Cannot convert data with changing granularity within a single day.')\n        source_gran_minutes = source_gran.values[0]\n        target_gran_minutes = target_granularity.total_seconds() / 60\n\n        _allowed_granularities = [1, 5, 15, 30, 60, 24*60]\n        if target_gran_minutes not in _allowed_granularities:\n            raise GranularityConversionError(\n                f'Target granularity {target_gran_minutes} minutes not supported. '\n                f'Allowed granularities: {_allowed_granularities} minutes'\n            )\n\n        if target_gran_minutes &gt; source_gran_minutes:\n            sampling = SamplingMethodEnum.DOWNSAMPLING\n        elif target_gran_minutes &lt; source_gran_minutes:\n            sampling = SamplingMethodEnum.UPSAMPLING\n        else:\n            sampling = SamplingMethodEnum.KEEP\n\n        if sampling == SamplingMethodEnum.UPSAMPLING:\n            scaling_factor = source_gran_minutes / target_gran_minutes\n            if (scaling_factor % 1) != 0:\n                raise GranularityConversionError(\n                    f'Source granularity ({source_gran_minutes} min) is not evenly divisible '\n                    f'by target granularity ({target_gran_minutes} min)'\n                )\n            else:\n                scaling_factor = int(scaling_factor)\n\n            new_index = pd.date_range(\n                start=series.index[0],\n                end=series.index[-1],\n                freq=f\"{target_gran_minutes}min\",\n                tz=series.index.tz\n            )\n\n            # For intensive quantities, replicate values; for extensive, divide by scaling factor\n            if quantity_type == QuantityTypeEnum.INTENSIVE:\n                return series.reindex(new_index, method='ffill')\n            else:  # EXTENSIVE\n                return series.reindex(new_index, method='ffill') / scaling_factor\n\n        elif sampling == SamplingMethodEnum.DOWNSAMPLING:\n            groups = series.groupby(pd.Grouper(freq=f\"{target_gran_minutes}min\"))\n            # For extensive quantities, sum the values; for intensive, take the mean\n            func = 'sum' if quantity_type == QuantityTypeEnum.EXTENSIVE else 'mean'\n            return groups.agg(func)\n\n        else:  # SamplingMethodEnum.KEEP\n            return series\n\n        return series\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/granularity_modules/#mescal.energy_data_handling.granularity_converter.TimeSeriesGranularityConverter.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the granularity converter with analyzer instances.</p> <p>Creates both strict and non-strict granularity analyzers for different validation requirements during conversion operations.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/granularity_converter.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the granularity converter with analyzer instances.\n\n    Creates both strict and non-strict granularity analyzers for different\n    validation requirements during conversion operations.\n    \"\"\"\n    self._strict_gran_analyzer = TimeSeriesGranularityAnalyzer(strict_mode=True)\n    self._non_strict_gran_analyzer = TimeSeriesGranularityAnalyzer(strict_mode=False)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/granularity_modules/#mescal.energy_data_handling.granularity_converter.TimeSeriesGranularityConverter.upsample_through_fillna","title":"upsample_through_fillna","text":"<pre><code>upsample_through_fillna(data: DataFrame | Series, quantity_type: QuantityTypeEnum) -&gt; DataFrame | Series\n</code></pre> <p>Upsample data using forward-fill strategy with quantity-type-aware scaling.</p> <p>This method handles upsampling of sparse data where some values are missing. It uses forward-fill to propagate values and applies appropriate scaling based on the quantity type:</p> <ul> <li>For INTENSIVE quantities: Values are replicated without scaling</li> <li>For EXTENSIVE quantities: Values are divided by the number of periods   they are spread across within each hour-segment group</li> </ul> <p>The method processes data per day and hour to handle missing periods properly and prevent incorrect auto-filling across day boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame | Series</code> <p>Time series data to upsample (Series or DataFrame)</p> required <code>quantity_type</code> <code>QuantityTypeEnum</code> <p>Type of quantity being converted (INTENSIVE or EXTENSIVE)</p> required <p>Returns:</p> Type Description <code>DataFrame | Series</code> <p>Upsampled data with same type as input</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # For extensive quantities (energy), values are divided\n&gt;&gt;&gt; series = pd.Series([100, np.nan, np.nan, np.nan, 200, np.nan, np.nan, np.nan],\n...                   index=pd.date_range('2024-01-01', freq='15min', periods=5))\n&gt;&gt;&gt; converter.upsample_through_fillna(series, QuantityTypeEnum.EXTENSIVE)\n    # Results in [25, 25, 25, 25, 50, 50, 50, 50]\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/granularity_converter.py</code> <pre><code>def upsample_through_fillna(\n        self,\n        data: pd.DataFrame | pd.Series,\n        quantity_type: QuantityTypeEnum\n) -&gt; pd.DataFrame | pd.Series:\n    \"\"\"Upsample data using forward-fill strategy with quantity-type-aware scaling.\n\n    This method handles upsampling of sparse data where some values are missing.\n    It uses forward-fill to propagate values and applies appropriate scaling\n    based on the quantity type:\n\n    - For INTENSIVE quantities: Values are replicated without scaling\n    - For EXTENSIVE quantities: Values are divided by the number of periods\n      they are spread across within each hour-segment group\n\n    The method processes data per day and hour to handle missing periods properly\n    and prevent incorrect auto-filling across day boundaries.\n\n    Args:\n        data: Time series data to upsample (Series or DataFrame)\n        quantity_type: Type of quantity being converted (INTENSIVE or EXTENSIVE)\n\n    Returns:\n        Upsampled data with same type as input\n\n    Example:\n\n        &gt;&gt;&gt; # For extensive quantities (energy), values are divided\n        &gt;&gt;&gt; series = pd.Series([100, np.nan, np.nan, np.nan, 200, np.nan, np.nan, np.nan],\n        ...                   index=pd.date_range('2024-01-01', freq='15min', periods=5))\n        &gt;&gt;&gt; converter.upsample_through_fillna(series, QuantityTypeEnum.EXTENSIVE)\n            # Results in [25, 25, 25, 25, 50, 50, 50, 50]\n    \"\"\"\n    if isinstance(data, pd.Series):\n        return self._upsample_series(data, quantity_type)\n\n    tmp = data.copy().sort_index()\n    idx = tmp.index.tz_convert('UTC') if tmp.index.tz is not None else tmp.index\n\n    if quantity_type == QuantityTypeEnum.EXTENSIVE:\n        segment_patterns = tmp.notna().cumsum()\n\n        # Group columns by their segment pattern\n        pattern_to_cols = {}\n        for col in tmp.columns:\n            pattern = tuple(segment_patterns[col].values)  # Convert to tuple to make it hashable\n            pattern_to_cols.setdefault(pattern, []).append(col)\n\n        # Process each group of columns with same pattern\n        result_pieces = []\n        for pattern, cols in pattern_to_cols.items():\n            segments = segment_patterns[cols[0]]  # Take segments from first column (all are same)\n            piece = (\n                tmp[cols]\n                .groupby([idx.date, idx.hour, segments])\n                .transform(lambda s: s.ffill() / len(s))\n            )\n            result_pieces.append(piece)\n\n        return pd.concat(result_pieces, axis=1).loc[data.index].rename_axis(data.columns.names, axis=1)\n    else:\n        return tmp.groupby([idx.date, idx.hour]).ffill().loc[data.index]\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/granularity_modules/#mescal.energy_data_handling.granularity_converter.TimeSeriesGranularityConverter.convert_to_target_index","title":"convert_to_target_index","text":"<pre><code>convert_to_target_index(series: Series, target_index: DatetimeIndex, quantity_type: QuantityTypeEnum) -&gt; Series\n</code></pre> <p>Convert a time series to match a specific target DatetimeIndex.</p> <p>This method converts the granularity of a time series to match the granularity of a target index. The target index must have consistent granularity within each day and consistent granularity across all days.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>Source time series to convert</p> required <code>target_index</code> <code>DatetimeIndex</code> <p>DatetimeIndex defining the target granularity and timestamps</p> required <code>quantity_type</code> <code>QuantityTypeEnum</code> <p>Type of quantity (INTENSIVE or EXTENSIVE) for proper scaling</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series converted to match target index granularity and timestamps</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If target index has multiple granularities within days        or inconsistent granularity across days</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Convert hourly to 15-min data\n&gt;&gt;&gt; hourly_series = pd.Series([100, 150, 200], \n...                          index=pd.date_range('2024-01-01', freq='1H', periods=3))\n&gt;&gt;&gt; target_idx = pd.date_range('2024-01-01', freq='15min', periods=12)\n&gt;&gt;&gt; result = converter.convert_to_target_index(hourly_series, target_idx,\n...                                          QuantityTypeEnum.INTENSIVE)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/granularity_converter.py</code> <pre><code>def convert_to_target_index(\n        self,\n        series: pd.Series,\n        target_index: pd.DatetimeIndex,\n        quantity_type: QuantityTypeEnum\n) -&gt; pd.Series:\n    \"\"\"Convert a time series to match a specific target DatetimeIndex.\n\n    This method converts the granularity of a time series to match the granularity\n    of a target index. The target index must have consistent granularity within\n    each day and consistent granularity across all days.\n\n    Args:\n        series: Source time series to convert\n        target_index: DatetimeIndex defining the target granularity and timestamps\n        quantity_type: Type of quantity (INTENSIVE or EXTENSIVE) for proper scaling\n\n    Returns:\n        Series converted to match target index granularity and timestamps\n\n    Raises:\n        ValueError: If target index has multiple granularities within days\n                   or inconsistent granularity across days\n\n    Example:\n\n        &gt;&gt;&gt; # Convert hourly to 15-min data\n        &gt;&gt;&gt; hourly_series = pd.Series([100, 150, 200], \n        ...                          index=pd.date_range('2024-01-01', freq='1H', periods=3))\n        &gt;&gt;&gt; target_idx = pd.date_range('2024-01-01', freq='15min', periods=12)\n        &gt;&gt;&gt; result = converter.convert_to_target_index(hourly_series, target_idx,\n        ...                                          QuantityTypeEnum.INTENSIVE)\n    \"\"\"\n    self._validate_series_format(series)\n    target_gran_series = self._strict_gran_analyzer.get_granularity_as_series_of_timedeltas(target_index)\n    _grouped = target_gran_series.groupby(target_gran_series.index.date)\n    if _grouped.nunique().max() &gt; 1:\n        raise ValueError(f\"Found some dates with multiple granularities within same day. Can't handle that!\")\n    if _grouped.first().nunique() &gt; 1:\n        raise ValueError(f\"Found multiple granularities. Can't handle that!\")\n    target_granularity = pd.Timedelta(target_gran_series.values[0])\n    return series.groupby(series.index.date).apply(\n        lambda x: self._convert_date_to_target_granularity(x, target_granularity, quantity_type)\n    ).droplevel(0).rename_axis(series.index.name).rename(series.name)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/granularity_modules/#mescal.energy_data_handling.granularity_converter.TimeSeriesGranularityConverter.convert_to_target_granularity","title":"convert_to_target_granularity","text":"<pre><code>convert_to_target_granularity(series: Series, target_granularity: Timedelta, quantity_type: QuantityTypeEnum) -&gt; Series\n</code></pre> <p>Convert a time series to a specific target granularity.</p> <p>This method converts the temporal granularity of a time series while properly handling the physical nature of the quantity. The conversion is performed day-by-day to prevent incorrect handling of missing days or daylight saving time transitions.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>Source time series to convert</p> required <code>target_granularity</code> <code>Timedelta</code> <p>Target granularity as a pandas Timedelta                (e.g., pd.Timedelta(minutes=15) for 15-minute data)</p> required <code>quantity_type</code> <code>QuantityTypeEnum</code> <p>Type of quantity for proper scaling:           - INTENSIVE: Values are averaged/replicated (prices, power)           - EXTENSIVE: Values are summed/split (volumes, energy)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series with converted granularity, maintaining original naming and metadata</p> <p>Raises:</p> Type Description <code>GranularityConversionError</code> <p>If conversion cannot be performed due to                        unsupported granularities or data issues</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Convert 15-minute to hourly data (downsampling)\n&gt;&gt;&gt; quarter_hourly = pd.Series([25, 30, 35, 40], \n...                           index=pd.date_range('2024-01-01', freq='15min', periods=4))\n&gt;&gt;&gt; hourly = converter.convert_to_target_granularity(\n...     quarter_hourly, pd.Timedelta(hours=1), QuantityTypeEnum.EXTENSIVE)\n&gt;&gt;&gt; print(hourly)  # Result: [130] (25+30+35+40)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/granularity_converter.py</code> <pre><code>def convert_to_target_granularity(\n        self,\n        series: pd.Series,\n        target_granularity: pd.Timedelta,\n        quantity_type: QuantityTypeEnum\n) -&gt; pd.Series:\n    \"\"\"Convert a time series to a specific target granularity.\n\n    This method converts the temporal granularity of a time series while properly\n    handling the physical nature of the quantity. The conversion is performed\n    day-by-day to prevent incorrect handling of missing days or daylight saving\n    time transitions.\n\n    Args:\n        series: Source time series to convert\n        target_granularity: Target granularity as a pandas Timedelta\n                           (e.g., pd.Timedelta(minutes=15) for 15-minute data)\n        quantity_type: Type of quantity for proper scaling:\n                      - INTENSIVE: Values are averaged/replicated (prices, power)\n                      - EXTENSIVE: Values are summed/split (volumes, energy)\n\n    Returns:\n        Series with converted granularity, maintaining original naming and metadata\n\n    Raises:\n        GranularityConversionError: If conversion cannot be performed due to\n                                   unsupported granularities or data issues\n\n    Example:\n\n        &gt;&gt;&gt; # Convert 15-minute to hourly data (downsampling)\n        &gt;&gt;&gt; quarter_hourly = pd.Series([25, 30, 35, 40], \n        ...                           index=pd.date_range('2024-01-01', freq='15min', periods=4))\n        &gt;&gt;&gt; hourly = converter.convert_to_target_granularity(\n        ...     quarter_hourly, pd.Timedelta(hours=1), QuantityTypeEnum.EXTENSIVE)\n        &gt;&gt;&gt; print(hourly)  # Result: [130] (25+30+35+40)\n    \"\"\"\n    self._validate_series_format(series)\n    return series.groupby(series.index.date).apply(\n        lambda x: self._convert_date_to_target_granularity(x, target_granularity, quantity_type)\n    ).droplevel(0).rename_axis(series.index.name).rename(series.name)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/granularity_modules/#mescal.energy_data_handling.granularity_converter.SamplingMethodEnum","title":"SamplingMethodEnum","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of sampling methods for granularity conversion.</p> <p>Attributes:</p> Name Type Description <code>UPSAMPLING</code> <p>Converting from coarser to finer granularity (e.g., hourly to 15-min)</p> <code>DOWNSAMPLING</code> <p>Converting from finer to coarser granularity (e.g., 15-min to hourly)</p> <code>KEEP</code> <p>No conversion needed - source and target granularities are the same</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/granularity_converter.py</code> <pre><code>class SamplingMethodEnum(Enum):\n    \"\"\"Enumeration of sampling methods for granularity conversion.\n\n    Attributes:\n        UPSAMPLING: Converting from coarser to finer granularity (e.g., hourly to 15-min)\n        DOWNSAMPLING: Converting from finer to coarser granularity (e.g., 15-min to hourly)\n        KEEP: No conversion needed - source and target granularities are the same\n    \"\"\"\n    UPSAMPLING = 'upsampling'\n    DOWNSAMPLING = 'downsampling'\n    KEEP = 'keep'\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/","title":"MESCAL Network Lines Data","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/#mescal.energy_data_handling.network_lines_data.NetworkLineFlowsData","title":"NetworkLineFlowsData","text":"<p>Wrapper for bidirectional flow data of network transmission lines.</p> <p>This class encapsulates energy or power flow data for lines in both directions, accounting for transmission losses. It provides a standardized interface for handling complex flow patterns in electrical network analysis.</p> Flow Direction Conventions <ul> <li>sent_up: Flow entering line at node_from (towards node_to)</li> <li>received_up: Flow leaving line at node_from after losses (coming from node_to)</li> <li>sent_down: Flow entering line at node_to (towards node_from)  </li> <li>received_down: Flow leaving line at node_to after losses (coming from node_from)</li> </ul> <p>The distinction between 'sent' and 'received' allows for modeling transmission losses, where received_flow = sent_flow * (1 - loss_rate).</p> <p>Parameters:</p> Name Type Description Default <code>sent_up</code> <code>DataFrame</code> <p>DataFrame with flow data entering at node_from</p> required <code>received_up</code> <code>DataFrame</code> <p>DataFrame with flow data received at node_from after losses</p> required <code>sent_down</code> <code>DataFrame</code> <p>DataFrame with flow data entering at node_to</p> required <code>received_down</code> <code>DataFrame</code> <p>DataFrame with flow data received at node_to after losses</p> required <code>granularity</code> <code>None | float | Series</code> <p>Time granularity of the data (None, float in minutes, or Series)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If indices or columns of the four DataFrames don't match</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=24, freq='1H')\n&gt;&gt;&gt; columns = ['Line_A_B', 'Line_B_C']\n&gt;&gt;&gt; flows_up = pd.DataFrame(100, index=index, columns=columns)\n&gt;&gt;&gt; flows_down = pd.DataFrame(50, index=index, columns=columns)\n&gt;&gt;&gt; line_data = NetworkLineFlowsData.from_up_and_down_flow_without_losses(\n...     flows_up, flows_down)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/network_lines_data.py</code> <pre><code>class NetworkLineFlowsData:\n    \"\"\"Wrapper for bidirectional flow data of network transmission lines.\n\n    This class encapsulates energy or power flow data for lines in both directions,\n    accounting for transmission losses. It provides a standardized interface for\n    handling complex flow patterns in electrical network analysis.\n\n    Flow Direction Conventions:\n        - sent_up: Flow entering line at node_from (towards node_to)\n        - received_up: Flow leaving line at node_from after losses (coming from node_to)\n        - sent_down: Flow entering line at node_to (towards node_from)  \n        - received_down: Flow leaving line at node_to after losses (coming from node_from)\n\n    The distinction between 'sent' and 'received' allows for modeling transmission\n    losses, where received_flow = sent_flow * (1 - loss_rate).\n\n    Args:\n        sent_up: DataFrame with flow data entering at node_from\n        received_up: DataFrame with flow data received at node_from after losses\n        sent_down: DataFrame with flow data entering at node_to\n        received_down: DataFrame with flow data received at node_to after losses\n        granularity: Time granularity of the data (None, float in minutes, or Series)\n\n    Raises:\n        ValueError: If indices or columns of the four DataFrames don't match\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=24, freq='1H')\n        &gt;&gt;&gt; columns = ['Line_A_B', 'Line_B_C']\n        &gt;&gt;&gt; flows_up = pd.DataFrame(100, index=index, columns=columns)\n        &gt;&gt;&gt; flows_down = pd.DataFrame(50, index=index, columns=columns)\n        &gt;&gt;&gt; line_data = NetworkLineFlowsData.from_up_and_down_flow_without_losses(\n        ...     flows_up, flows_down)\n    \"\"\"\n    def __init__(\n            self,\n            sent_up: pd.DataFrame,\n            received_up: pd.DataFrame,\n            sent_down: pd.DataFrame,\n            received_down: pd.DataFrame,\n            granularity: None | float | pd.Series = None\n    ):\n        \"\"\"Initialize NetworkLineFlowsData with flow data in both directions.\n\n        Args:\n            sent_up: Flow data sent in up direction (node_from -&gt; node_to)\n            received_up: Flow data received in up direction after losses\n            sent_down: Flow data sent in down direction (node_to -&gt; node_from)\n            received_down: Flow data received in down direction after losses\n            granularity: Time granularity information for the data\n        \"\"\"\n        self.sent_up = sent_up\n        self.received_up = received_up\n        self.sent_down = sent_down\n        self.received_down = received_down\n        self.granularity = granularity\n        self.__post_init__()\n\n    def __post_init__(self):\n        \"\"\"Validate that all DataFrames have matching indices and columns.\n\n        Raises:\n            ValueError: If any DataFrame has mismatched indices or columns.\n        \"\"\"\n        dataframes = [self.received_up, self.sent_down, self.received_down]\n        for i, df in enumerate(dataframes):\n            df_name = ['received_up', 'sent_down', 'received_down'][i]\n            if not self.sent_up.index.equals(df.index):\n                raise ValueError(f'Index mismatch: sent_up vs {df_name}')\n            if not self.sent_up.columns.equals(df.columns):\n                raise ValueError(f'Columns mismatch: sent_up vs {df_name}')\n\n    def from_mw_to_mwh(self) -&gt; 'NetworkLineFlowsData':\n        \"\"\"Convert flow data from MW (power) to MWh (energy).\n\n        This conversion requires granularity information to properly scale the values.\n\n        Returns:\n            New NetworkLineFlowsData instance with energy values\n\n        Raises:\n            NotImplementedError: Method not yet implemented\n        \"\"\"\n        # TODO: Implement MW to MWh conversion using granularity\n        raise NotImplementedError(\"MW to MWh conversion not yet implemented\")\n\n    def from_mwh_to_mw(self) -&gt; 'NetworkLineFlowsData':\n        \"\"\"Convert flow data from MWh (energy) to MW (power).\n\n        This conversion requires granularity information to properly scale the values.\n\n        Returns:\n            New NetworkLineFlowsData instance with power values\n\n        Raises:\n            NotImplementedError: Method not yet implemented\n        \"\"\"\n        # TODO: Implement MWh to MW conversion using granularity  \n        raise NotImplementedError(\"MWh to MW conversion not yet implemented\")\n\n    @classmethod\n    def from_net_flow_without_losses(cls, net_flow: pd.DataFrame) -&gt; \"NetworkLineFlowsData\":\n        \"\"\"Create NetworkLineFlowsData from net flow data assuming no transmission losses.\n\n        Converts net flow data (where positive values indicate flow in up direction\n        and negative values indicate flow in down direction) into the bidirectional\n        flow representation used by this class.\n\n        Args:\n            net_flow: DataFrame with net flow values. Positive = up direction,\n                negative = down direction\n\n        Returns:\n            NetworkLineFlowsData instance with flows split into up/down directions\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; net_flows = pd.DataFrame({\n            ...     'Line_A_B': [100, -50, 75],\n            ...     'Line_B_C': [200, 150, -100]\n            ... })\n            &gt;&gt;&gt; line_data = NetworkLineFlowsData.from_net_flow_without_losses(net_flows)\n        \"\"\"\n        positive_flow = net_flow.clip(lower=0)\n        negative_flow = -net_flow.clip(upper=0)\n\n        return cls(\n            sent_up=positive_flow,\n            received_up=positive_flow,\n            sent_down=negative_flow,\n            received_down=negative_flow\n        )\n\n    @classmethod\n    def from_up_and_down_flow_without_losses(\n            cls,\n            flow_up: pd.DataFrame,\n            flow_down: pd.DataFrame\n    ) -&gt; \"NetworkLineFlowsData\":\n        \"\"\"Create NetworkLineFlowsData from separate up and down flow data without losses.\n\n        This constructor assumes that there are no transmission losses, so sent and\n        received flows are identical in each direction.\n\n        Args:\n            flow_up: DataFrame with flow data in up direction (node_from -&gt; node_to)\n            flow_down: DataFrame with flow data in down direction (node_to -&gt; node_from)\n\n        Returns:\n            NetworkLineFlowsData instance where sent and received flows are equal\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; up_flows = pd.DataFrame({'Line_A_B': [100, 80, 120]})\n            &gt;&gt;&gt; down_flows = pd.DataFrame({'Line_A_B': [50, 60, 40]})\n            &gt;&gt;&gt; line_data = NetworkLineFlowsData.from_up_and_down_flow_without_losses(\n            ...     up_flows, down_flows)\n        \"\"\"\n        return cls(\n            sent_up=flow_up,\n            received_up=flow_up,\n            sent_down=flow_down,\n            received_down=flow_down\n        )\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/#mescal.energy_data_handling.network_lines_data.NetworkLineFlowsData.__init__","title":"__init__","text":"<pre><code>__init__(sent_up: DataFrame, received_up: DataFrame, sent_down: DataFrame, received_down: DataFrame, granularity: None | float | Series = None)\n</code></pre> <p>Initialize NetworkLineFlowsData with flow data in both directions.</p> <p>Parameters:</p> Name Type Description Default <code>sent_up</code> <code>DataFrame</code> <p>Flow data sent in up direction (node_from -&gt; node_to)</p> required <code>received_up</code> <code>DataFrame</code> <p>Flow data received in up direction after losses</p> required <code>sent_down</code> <code>DataFrame</code> <p>Flow data sent in down direction (node_to -&gt; node_from)</p> required <code>received_down</code> <code>DataFrame</code> <p>Flow data received in down direction after losses</p> required <code>granularity</code> <code>None | float | Series</code> <p>Time granularity information for the data</p> <code>None</code> Source code in <code>submodules/mescal/mescal/energy_data_handling/network_lines_data.py</code> <pre><code>def __init__(\n        self,\n        sent_up: pd.DataFrame,\n        received_up: pd.DataFrame,\n        sent_down: pd.DataFrame,\n        received_down: pd.DataFrame,\n        granularity: None | float | pd.Series = None\n):\n    \"\"\"Initialize NetworkLineFlowsData with flow data in both directions.\n\n    Args:\n        sent_up: Flow data sent in up direction (node_from -&gt; node_to)\n        received_up: Flow data received in up direction after losses\n        sent_down: Flow data sent in down direction (node_to -&gt; node_from)\n        received_down: Flow data received in down direction after losses\n        granularity: Time granularity information for the data\n    \"\"\"\n    self.sent_up = sent_up\n    self.received_up = received_up\n    self.sent_down = sent_down\n    self.received_down = received_down\n    self.granularity = granularity\n    self.__post_init__()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/#mescal.energy_data_handling.network_lines_data.NetworkLineFlowsData.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate that all DataFrames have matching indices and columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any DataFrame has mismatched indices or columns.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/network_lines_data.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate that all DataFrames have matching indices and columns.\n\n    Raises:\n        ValueError: If any DataFrame has mismatched indices or columns.\n    \"\"\"\n    dataframes = [self.received_up, self.sent_down, self.received_down]\n    for i, df in enumerate(dataframes):\n        df_name = ['received_up', 'sent_down', 'received_down'][i]\n        if not self.sent_up.index.equals(df.index):\n            raise ValueError(f'Index mismatch: sent_up vs {df_name}')\n        if not self.sent_up.columns.equals(df.columns):\n            raise ValueError(f'Columns mismatch: sent_up vs {df_name}')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/#mescal.energy_data_handling.network_lines_data.NetworkLineFlowsData.from_mw_to_mwh","title":"from_mw_to_mwh","text":"<pre><code>from_mw_to_mwh() -&gt; NetworkLineFlowsData\n</code></pre> <p>Convert flow data from MW (power) to MWh (energy).</p> <p>This conversion requires granularity information to properly scale the values.</p> <p>Returns:</p> Type Description <code>NetworkLineFlowsData</code> <p>New NetworkLineFlowsData instance with energy values</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Method not yet implemented</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/network_lines_data.py</code> <pre><code>def from_mw_to_mwh(self) -&gt; 'NetworkLineFlowsData':\n    \"\"\"Convert flow data from MW (power) to MWh (energy).\n\n    This conversion requires granularity information to properly scale the values.\n\n    Returns:\n        New NetworkLineFlowsData instance with energy values\n\n    Raises:\n        NotImplementedError: Method not yet implemented\n    \"\"\"\n    # TODO: Implement MW to MWh conversion using granularity\n    raise NotImplementedError(\"MW to MWh conversion not yet implemented\")\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/#mescal.energy_data_handling.network_lines_data.NetworkLineFlowsData.from_mwh_to_mw","title":"from_mwh_to_mw","text":"<pre><code>from_mwh_to_mw() -&gt; NetworkLineFlowsData\n</code></pre> <p>Convert flow data from MWh (energy) to MW (power).</p> <p>This conversion requires granularity information to properly scale the values.</p> <p>Returns:</p> Type Description <code>NetworkLineFlowsData</code> <p>New NetworkLineFlowsData instance with power values</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Method not yet implemented</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/network_lines_data.py</code> <pre><code>def from_mwh_to_mw(self) -&gt; 'NetworkLineFlowsData':\n    \"\"\"Convert flow data from MWh (energy) to MW (power).\n\n    This conversion requires granularity information to properly scale the values.\n\n    Returns:\n        New NetworkLineFlowsData instance with power values\n\n    Raises:\n        NotImplementedError: Method not yet implemented\n    \"\"\"\n    # TODO: Implement MWh to MW conversion using granularity  \n    raise NotImplementedError(\"MWh to MW conversion not yet implemented\")\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/#mescal.energy_data_handling.network_lines_data.NetworkLineFlowsData.from_net_flow_without_losses","title":"from_net_flow_without_losses  <code>classmethod</code>","text":"<pre><code>from_net_flow_without_losses(net_flow: DataFrame) -&gt; NetworkLineFlowsData\n</code></pre> <p>Create NetworkLineFlowsData from net flow data assuming no transmission losses.</p> <p>Converts net flow data (where positive values indicate flow in up direction and negative values indicate flow in down direction) into the bidirectional flow representation used by this class.</p> <p>Parameters:</p> Name Type Description Default <code>net_flow</code> <code>DataFrame</code> <p>DataFrame with net flow values. Positive = up direction, negative = down direction</p> required <p>Returns:</p> Type Description <code>NetworkLineFlowsData</code> <p>NetworkLineFlowsData instance with flows split into up/down directions</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; net_flows = pd.DataFrame({\n...     'Line_A_B': [100, -50, 75],\n...     'Line_B_C': [200, 150, -100]\n... })\n&gt;&gt;&gt; line_data = NetworkLineFlowsData.from_net_flow_without_losses(net_flows)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/network_lines_data.py</code> <pre><code>@classmethod\ndef from_net_flow_without_losses(cls, net_flow: pd.DataFrame) -&gt; \"NetworkLineFlowsData\":\n    \"\"\"Create NetworkLineFlowsData from net flow data assuming no transmission losses.\n\n    Converts net flow data (where positive values indicate flow in up direction\n    and negative values indicate flow in down direction) into the bidirectional\n    flow representation used by this class.\n\n    Args:\n        net_flow: DataFrame with net flow values. Positive = up direction,\n            negative = down direction\n\n    Returns:\n        NetworkLineFlowsData instance with flows split into up/down directions\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; net_flows = pd.DataFrame({\n        ...     'Line_A_B': [100, -50, 75],\n        ...     'Line_B_C': [200, 150, -100]\n        ... })\n        &gt;&gt;&gt; line_data = NetworkLineFlowsData.from_net_flow_without_losses(net_flows)\n    \"\"\"\n    positive_flow = net_flow.clip(lower=0)\n    negative_flow = -net_flow.clip(upper=0)\n\n    return cls(\n        sent_up=positive_flow,\n        received_up=positive_flow,\n        sent_down=negative_flow,\n        received_down=negative_flow\n    )\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/#mescal.energy_data_handling.network_lines_data.NetworkLineFlowsData.from_up_and_down_flow_without_losses","title":"from_up_and_down_flow_without_losses  <code>classmethod</code>","text":"<pre><code>from_up_and_down_flow_without_losses(flow_up: DataFrame, flow_down: DataFrame) -&gt; NetworkLineFlowsData\n</code></pre> <p>Create NetworkLineFlowsData from separate up and down flow data without losses.</p> <p>This constructor assumes that there are no transmission losses, so sent and received flows are identical in each direction.</p> <p>Parameters:</p> Name Type Description Default <code>flow_up</code> <code>DataFrame</code> <p>DataFrame with flow data in up direction (node_from -&gt; node_to)</p> required <code>flow_down</code> <code>DataFrame</code> <p>DataFrame with flow data in down direction (node_to -&gt; node_from)</p> required <p>Returns:</p> Type Description <code>NetworkLineFlowsData</code> <p>NetworkLineFlowsData instance where sent and received flows are equal</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; up_flows = pd.DataFrame({'Line_A_B': [100, 80, 120]})\n&gt;&gt;&gt; down_flows = pd.DataFrame({'Line_A_B': [50, 60, 40]})\n&gt;&gt;&gt; line_data = NetworkLineFlowsData.from_up_and_down_flow_without_losses(\n...     up_flows, down_flows)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/network_lines_data.py</code> <pre><code>@classmethod\ndef from_up_and_down_flow_without_losses(\n        cls,\n        flow_up: pd.DataFrame,\n        flow_down: pd.DataFrame\n) -&gt; \"NetworkLineFlowsData\":\n    \"\"\"Create NetworkLineFlowsData from separate up and down flow data without losses.\n\n    This constructor assumes that there are no transmission losses, so sent and\n    received flows are identical in each direction.\n\n    Args:\n        flow_up: DataFrame with flow data in up direction (node_from -&gt; node_to)\n        flow_down: DataFrame with flow data in down direction (node_to -&gt; node_from)\n\n    Returns:\n        NetworkLineFlowsData instance where sent and received flows are equal\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; up_flows = pd.DataFrame({'Line_A_B': [100, 80, 120]})\n        &gt;&gt;&gt; down_flows = pd.DataFrame({'Line_A_B': [50, 60, 40]})\n        &gt;&gt;&gt; line_data = NetworkLineFlowsData.from_up_and_down_flow_without_losses(\n        ...     up_flows, down_flows)\n    \"\"\"\n    return cls(\n        sent_up=flow_up,\n        received_up=flow_up,\n        sent_down=flow_down,\n        received_down=flow_down\n    )\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/#mescal.energy_data_handling.network_lines_data.NetworkLineCapacitiesData","title":"NetworkLineCapacitiesData  <code>dataclass</code>","text":"<p>Wrapper for bidirectional capacity data of network transmission lines.</p> <p>This dataclass encapsulates transmission capacity limits for network lines in both directions. Capacities can be asymmetric to reflect real-world transmission constraints or operational limits.</p> Capacity Direction Conventions <ul> <li>capacities_up: Maximum transmission capacity from node_from to node_to</li> <li>capacities_down: Maximum transmission capacity from node_to to node_from</li> </ul> <p>Parameters:</p> Name Type Description Default <code>capacities_up</code> <code>DataFrame</code> <p>DataFrame with capacity limits in up direction</p> required <code>capacities_down</code> <code>DataFrame</code> <p>DataFrame with capacity limits in down direction  </p> required <code>granularity</code> <code>None | float | Series</code> <p>Time granularity of capacity data (None, float in minutes, or Series)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If indices or columns of the two DataFrames don't match</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=24, freq='1H')\n&gt;&gt;&gt; columns = ['Line_A_B', 'Line_B_C']\n&gt;&gt;&gt; caps = pd.DataFrame(1000, index=index, columns=columns)\n&gt;&gt;&gt; capacity_data = NetworkLineCapacitiesData.from_symmetric_capacities(caps)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/network_lines_data.py</code> <pre><code>@dataclass\nclass NetworkLineCapacitiesData:\n    \"\"\"Wrapper for bidirectional capacity data of network transmission lines.\n\n    This dataclass encapsulates transmission capacity limits for network lines in both\n    directions. Capacities can be asymmetric to reflect real-world transmission\n    constraints or operational limits.\n\n    Capacity Direction Conventions:\n        - capacities_up: Maximum transmission capacity from node_from to node_to\n        - capacities_down: Maximum transmission capacity from node_to to node_from\n\n    Args:\n        capacities_up: DataFrame with capacity limits in up direction\n        capacities_down: DataFrame with capacity limits in down direction  \n        granularity: Time granularity of capacity data (None, float in minutes, or Series)\n\n    Raises:\n        ValueError: If indices or columns of the two DataFrames don't match\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=24, freq='1H')\n        &gt;&gt;&gt; columns = ['Line_A_B', 'Line_B_C']\n        &gt;&gt;&gt; caps = pd.DataFrame(1000, index=index, columns=columns)\n        &gt;&gt;&gt; capacity_data = NetworkLineCapacitiesData.from_symmetric_capacities(caps)\n    \"\"\"\n    capacities_up: pd.DataFrame\n    capacities_down: pd.DataFrame\n    granularity: None | float | pd.Series = None\n\n    def __post_init__(self):\n        \"\"\"Validate that both capacity DataFrames have matching indices and columns.\n\n        Raises:\n            ValueError: If DataFrames have mismatched indices or columns.\n        \"\"\"\n        if not self.capacities_up.index.equals(self.capacities_down.index):\n            raise ValueError('Index mismatch: capacities_up vs capacities_down')\n        if not self.capacities_up.columns.equals(self.capacities_down.columns):\n            raise ValueError('Columns mismatch: capacities_up vs capacities_down')\n\n    def from_mw_to_mwh(self) -&gt; 'NetworkLineCapacitiesData':\n        \"\"\"Convert capacity data from MW (power) to MWh (energy).\n\n        This conversion requires granularity information to properly scale the values.\n\n        Returns:\n            New NetworkLineCapacitiesData instance with energy capacity values\n\n        Raises:\n            NotImplementedError: Method not yet implemented\n        \"\"\"\n        # TODO: Implement MW to MWh conversion using granularity\n        raise NotImplementedError(\"MW to MWh conversion not yet implemented\")\n\n    def from_mwh_to_mw(self) -&gt; 'NetworkLineCapacitiesData':\n        \"\"\"Convert capacity data from MWh (energy) to MW (power).\n\n        This conversion requires granularity information to properly scale the values.\n\n        Returns:\n            New NetworkLineCapacitiesData instance with power capacity values\n\n        Raises:\n            NotImplementedError: Method not yet implemented\n        \"\"\"\n        # TODO: Implement MWh to MW conversion using granularity\n        raise NotImplementedError(\"MWh to MW conversion not yet implemented\")\n\n    @classmethod\n    def from_symmetric_capacities(cls, capacities: pd.DataFrame) -&gt; \"NetworkLineCapacitiesData\":\n        \"\"\"Create NetworkLineCapacitiesData with identical capacities in both directions.\n\n        This is a convenience constructor for cases where transmission lines have\n        the same capacity limit in both directions.\n\n        Args:\n            capacities: DataFrame with capacity values to use for both directions\n\n        Returns:\n            NetworkLineCapacitiesData instance with symmetric capacities\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; caps = pd.DataFrame({\n            ...     'Line_A_B': [1000, 1200, 800],\n            ...     'Line_B_C': [1500, 1500, 1000]\n            ... })\n            &gt;&gt;&gt; capacity_data = NetworkLineCapacitiesData.from_symmetric_capacities(caps)\n        \"\"\"\n        return cls(capacities_up=capacities, capacities_down=capacities)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/#mescal.energy_data_handling.network_lines_data.NetworkLineCapacitiesData.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate that both capacity DataFrames have matching indices and columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If DataFrames have mismatched indices or columns.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/network_lines_data.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate that both capacity DataFrames have matching indices and columns.\n\n    Raises:\n        ValueError: If DataFrames have mismatched indices or columns.\n    \"\"\"\n    if not self.capacities_up.index.equals(self.capacities_down.index):\n        raise ValueError('Index mismatch: capacities_up vs capacities_down')\n    if not self.capacities_up.columns.equals(self.capacities_down.columns):\n        raise ValueError('Columns mismatch: capacities_up vs capacities_down')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/#mescal.energy_data_handling.network_lines_data.NetworkLineCapacitiesData.from_mw_to_mwh","title":"from_mw_to_mwh","text":"<pre><code>from_mw_to_mwh() -&gt; NetworkLineCapacitiesData\n</code></pre> <p>Convert capacity data from MW (power) to MWh (energy).</p> <p>This conversion requires granularity information to properly scale the values.</p> <p>Returns:</p> Type Description <code>NetworkLineCapacitiesData</code> <p>New NetworkLineCapacitiesData instance with energy capacity values</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Method not yet implemented</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/network_lines_data.py</code> <pre><code>def from_mw_to_mwh(self) -&gt; 'NetworkLineCapacitiesData':\n    \"\"\"Convert capacity data from MW (power) to MWh (energy).\n\n    This conversion requires granularity information to properly scale the values.\n\n    Returns:\n        New NetworkLineCapacitiesData instance with energy capacity values\n\n    Raises:\n        NotImplementedError: Method not yet implemented\n    \"\"\"\n    # TODO: Implement MW to MWh conversion using granularity\n    raise NotImplementedError(\"MW to MWh conversion not yet implemented\")\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/#mescal.energy_data_handling.network_lines_data.NetworkLineCapacitiesData.from_mwh_to_mw","title":"from_mwh_to_mw","text":"<pre><code>from_mwh_to_mw() -&gt; NetworkLineCapacitiesData\n</code></pre> <p>Convert capacity data from MWh (energy) to MW (power).</p> <p>This conversion requires granularity information to properly scale the values.</p> <p>Returns:</p> Type Description <code>NetworkLineCapacitiesData</code> <p>New NetworkLineCapacitiesData instance with power capacity values</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Method not yet implemented</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/network_lines_data.py</code> <pre><code>def from_mwh_to_mw(self) -&gt; 'NetworkLineCapacitiesData':\n    \"\"\"Convert capacity data from MWh (energy) to MW (power).\n\n    This conversion requires granularity information to properly scale the values.\n\n    Returns:\n        New NetworkLineCapacitiesData instance with power capacity values\n\n    Raises:\n        NotImplementedError: Method not yet implemented\n    \"\"\"\n    # TODO: Implement MWh to MW conversion using granularity\n    raise NotImplementedError(\"MWh to MW conversion not yet implemented\")\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/network_lines_data/#mescal.energy_data_handling.network_lines_data.NetworkLineCapacitiesData.from_symmetric_capacities","title":"from_symmetric_capacities  <code>classmethod</code>","text":"<pre><code>from_symmetric_capacities(capacities: DataFrame) -&gt; NetworkLineCapacitiesData\n</code></pre> <p>Create NetworkLineCapacitiesData with identical capacities in both directions.</p> <p>This is a convenience constructor for cases where transmission lines have the same capacity limit in both directions.</p> <p>Parameters:</p> Name Type Description Default <code>capacities</code> <code>DataFrame</code> <p>DataFrame with capacity values to use for both directions</p> required <p>Returns:</p> Type Description <code>NetworkLineCapacitiesData</code> <p>NetworkLineCapacitiesData instance with symmetric capacities</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; caps = pd.DataFrame({\n...     'Line_A_B': [1000, 1200, 800],\n...     'Line_B_C': [1500, 1500, 1000]\n... })\n&gt;&gt;&gt; capacity_data = NetworkLineCapacitiesData.from_symmetric_capacities(caps)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/network_lines_data.py</code> <pre><code>@classmethod\ndef from_symmetric_capacities(cls, capacities: pd.DataFrame) -&gt; \"NetworkLineCapacitiesData\":\n    \"\"\"Create NetworkLineCapacitiesData with identical capacities in both directions.\n\n    This is a convenience constructor for cases where transmission lines have\n    the same capacity limit in both directions.\n\n    Args:\n        capacities: DataFrame with capacity values to use for both directions\n\n    Returns:\n        NetworkLineCapacitiesData instance with symmetric capacities\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; caps = pd.DataFrame({\n        ...     'Line_A_B': [1000, 1200, 800],\n        ...     'Line_B_C': [1500, 1500, 1000]\n        ... })\n        &gt;&gt;&gt; capacity_data = NetworkLineCapacitiesData.from_symmetric_capacities(caps)\n    \"\"\"\n    return cls(capacities_up=capacities, capacities_down=capacities)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/time_series_gap_handling/","title":"MESCAL Time Series Gap Handling","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/time_series_gap_handling/#mescal.energy_data_handling.time_series_gap_handling.TimeSeriesGapHandler","title":"TimeSeriesGapHandler","text":"<p>Handles gaps in time series data by inserting NaN values at detected gaps.</p> <p>This class is useful for identifying and marking gaps in time series data that exceed a specified threshold. The handler inserts NaN values at the beginning of detected gaps to make gaps visible for downstream processing. This is particularly useful for line plots, where you often prefer to have a visual line-gap in a data gap instead of the line bridging the two surrounding values.</p> <p>Parameters:</p> Name Type Description Default <code>max_gap_in_minutes</code> <code>float</code> <p>Maximum allowed gap duration in minutes. Gaps longer than this threshold will have NaN values inserted. Default is 60 minutes.</p> <code>60</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=5, freq='30min')\n&gt;&gt;&gt; dates = dates.delete(2)  # Create a gap\n&gt;&gt;&gt; values = np.random.rand(len(dates))\n&gt;&gt;&gt; series = pd.Series(values, index=dates)\n&gt;&gt;&gt; handler = TimeSeriesGapHandler(max_gap_in_minutes=30)\n&gt;&gt;&gt; result = handler.insert_nans_at_gaps(series)\n&gt;&gt;&gt; print(result)  # Will show NaN inserted at gap location\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/time_series_gap_handling.py</code> <pre><code>class TimeSeriesGapHandler:\n    \"\"\"Handles gaps in time series data by inserting NaN values at detected gaps.\n\n    This class is useful for identifying and marking gaps in time series data that exceed\n    a specified threshold. The handler inserts NaN values at the beginning of detected gaps\n    to make gaps visible for downstream processing. This is particularly useful for line plots,\n    where you often prefer to have a visual line-gap in a data gap instead of the line bridging\n    the two surrounding values.\n\n    Args:\n        max_gap_in_minutes: Maximum allowed gap duration in minutes. Gaps longer than\n            this threshold will have NaN values inserted. Default is 60 minutes.\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=5, freq='30min')\n        &gt;&gt;&gt; dates = dates.delete(2)  # Create a gap\n        &gt;&gt;&gt; values = np.random.rand(len(dates))\n        &gt;&gt;&gt; series = pd.Series(values, index=dates)\n        &gt;&gt;&gt; handler = TimeSeriesGapHandler(max_gap_in_minutes=30)\n        &gt;&gt;&gt; result = handler.insert_nans_at_gaps(series)\n        &gt;&gt;&gt; print(result)  # Will show NaN inserted at gap location\n    \"\"\"\n\n    def __init__(self, max_gap_in_minutes: float = 60):\n        \"\"\"Initialize the gap handler with specified maximum gap threshold.\n\n        Args:\n            max_gap_in_minutes: Maximum allowed gap duration in minutes before\n                inserting NaN markers.\n        \"\"\"\n        self._max_gap_in_minutes = max_gap_in_minutes\n\n    def insert_nans_at_gaps(self, data: pd.Series | pd.DataFrame) -&gt; pd.Series | pd.DataFrame:\n        \"\"\"Insert NaN values at the beginning of gaps that exceed the maximum threshold.\n\n        This method identifies time gaps in the data that are longer than the configured\n        maximum gap duration and inserts NaN values at the start of these gaps. This\n        approach ensures that gaps are explicitly marked in the data rather than being\n        silently filled by interpolation methods.\n\n        Args:\n            data: Time series data with DatetimeIndex. Can be either a Series or DataFrame.\n\n        Returns:\n            Same type as input with NaN values inserted at gap locations. The returned\n            data will be sorted by index.\n\n        Raises:\n            TypeError: If data index is not a DatetimeIndex.\n\n        Example:\n\n            &gt;&gt;&gt; dates = pd.date_range('2024-01-01', freq='1H', periods=5)\n            &gt;&gt;&gt; dates = dates.delete([2, 3])  # Create 2-hour gap\n            &gt;&gt;&gt; series = pd.Series([1, 2, 3], index=dates[[0, 1, 4]])\n            &gt;&gt;&gt; handler = TimeSeriesGapHandler(max_gap_in_minutes=90)\n            &gt;&gt;&gt; result = handler.insert_nans_at_gaps(series)\n            &gt;&gt;&gt; print(result)  # Shows NaN inserted at gap start\n        \"\"\"\n        if not isinstance(data.index, pd.DatetimeIndex):\n            raise TypeError(f\"Data index must be DatetimeIndex, got {type(data.index)}\")\n\n        diffs = data.index.to_series().diff()\n        mask = diffs &gt; pd.Timedelta(minutes=self._max_gap_in_minutes)\n        gap_indices = np.where(mask)[0]\n\n        if len(gap_indices) == 0:\n            return data  # No gaps found\n\n        new_timestamps = data.index[gap_indices - 1] + pd.Timedelta(minutes=self._max_gap_in_minutes)\n\n        if isinstance(data, pd.Series):\n            new_values = pd.Series(np.nan, index=new_timestamps, name=data.name)\n        else:\n            new_values = pd.DataFrame(np.nan, index=new_timestamps, columns=data.columns)\n\n        return pd.concat([data, new_values]).sort_index()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/time_series_gap_handling/#mescal.energy_data_handling.time_series_gap_handling.TimeSeriesGapHandler.__init__","title":"__init__","text":"<pre><code>__init__(max_gap_in_minutes: float = 60)\n</code></pre> <p>Initialize the gap handler with specified maximum gap threshold.</p> <p>Parameters:</p> Name Type Description Default <code>max_gap_in_minutes</code> <code>float</code> <p>Maximum allowed gap duration in minutes before inserting NaN markers.</p> <code>60</code> Source code in <code>submodules/mescal/mescal/energy_data_handling/time_series_gap_handling.py</code> <pre><code>def __init__(self, max_gap_in_minutes: float = 60):\n    \"\"\"Initialize the gap handler with specified maximum gap threshold.\n\n    Args:\n        max_gap_in_minutes: Maximum allowed gap duration in minutes before\n            inserting NaN markers.\n    \"\"\"\n    self._max_gap_in_minutes = max_gap_in_minutes\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/time_series_gap_handling/#mescal.energy_data_handling.time_series_gap_handling.TimeSeriesGapHandler.insert_nans_at_gaps","title":"insert_nans_at_gaps","text":"<pre><code>insert_nans_at_gaps(data: Series | DataFrame) -&gt; Series | DataFrame\n</code></pre> <p>Insert NaN values at the beginning of gaps that exceed the maximum threshold.</p> <p>This method identifies time gaps in the data that are longer than the configured maximum gap duration and inserts NaN values at the start of these gaps. This approach ensures that gaps are explicitly marked in the data rather than being silently filled by interpolation methods.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series | DataFrame</code> <p>Time series data with DatetimeIndex. Can be either a Series or DataFrame.</p> required <p>Returns:</p> Type Description <code>Series | DataFrame</code> <p>Same type as input with NaN values inserted at gap locations. The returned</p> <code>Series | DataFrame</code> <p>data will be sorted by index.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If data index is not a DatetimeIndex.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; dates = pd.date_range('2024-01-01', freq='1H', periods=5)\n&gt;&gt;&gt; dates = dates.delete([2, 3])  # Create 2-hour gap\n&gt;&gt;&gt; series = pd.Series([1, 2, 3], index=dates[[0, 1, 4]])\n&gt;&gt;&gt; handler = TimeSeriesGapHandler(max_gap_in_minutes=90)\n&gt;&gt;&gt; result = handler.insert_nans_at_gaps(series)\n&gt;&gt;&gt; print(result)  # Shows NaN inserted at gap start\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/time_series_gap_handling.py</code> <pre><code>def insert_nans_at_gaps(self, data: pd.Series | pd.DataFrame) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"Insert NaN values at the beginning of gaps that exceed the maximum threshold.\n\n    This method identifies time gaps in the data that are longer than the configured\n    maximum gap duration and inserts NaN values at the start of these gaps. This\n    approach ensures that gaps are explicitly marked in the data rather than being\n    silently filled by interpolation methods.\n\n    Args:\n        data: Time series data with DatetimeIndex. Can be either a Series or DataFrame.\n\n    Returns:\n        Same type as input with NaN values inserted at gap locations. The returned\n        data will be sorted by index.\n\n    Raises:\n        TypeError: If data index is not a DatetimeIndex.\n\n    Example:\n\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', freq='1H', periods=5)\n        &gt;&gt;&gt; dates = dates.delete([2, 3])  # Create 2-hour gap\n        &gt;&gt;&gt; series = pd.Series([1, 2, 3], index=dates[[0, 1, 4]])\n        &gt;&gt;&gt; handler = TimeSeriesGapHandler(max_gap_in_minutes=90)\n        &gt;&gt;&gt; result = handler.insert_nans_at_gaps(series)\n        &gt;&gt;&gt; print(result)  # Shows NaN inserted at gap start\n    \"\"\"\n    if not isinstance(data.index, pd.DatetimeIndex):\n        raise TypeError(f\"Data index must be DatetimeIndex, got {type(data.index)}\")\n\n    diffs = data.index.to_series().diff()\n    mask = diffs &gt; pd.Timedelta(minutes=self._max_gap_in_minutes)\n    gap_indices = np.where(mask)[0]\n\n    if len(gap_indices) == 0:\n        return data  # No gaps found\n\n    new_timestamps = data.index[gap_indices - 1] + pd.Timedelta(minutes=self._max_gap_in_minutes)\n\n    if isinstance(data, pd.Series):\n        new_values = pd.Series(np.nan, index=new_timestamps, name=data.name)\n    else:\n        new_values = pd.DataFrame(np.nan, index=new_timestamps, columns=data.columns)\n\n    return pd.concat([data, new_values]).sort_index()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/","title":"MESCAL Area and AreaBorder Accounting","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/#mescal.energy_data_handling.area_accounting","title":"area_accounting","text":"<p>MESCAL Area and Area Border Accounting Package</p> <p>Tools for transforming node-line topology into area-based models and variables.  Supports multi-level aggregation (countries, bidding zones, market regions) and  cross-border flow analysis for energy market studies.</p>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/#mescal.energy_data_handling.area_accounting--core-components","title":"Core Components:","text":"Model Generators <ul> <li>Area model creation from node-to-area mappings with geographic visualization</li> <li>Border identification from transmission topology with standardized naming</li> <li>Network graph generation and geometric analysis for spatial representation</li> </ul> Variable Calculators <ul> <li>Area variables: Price aggregation (volume-weighted) and sum-based calculations</li> <li>Border variables: Cross-border flows, capacity aggregation, and price spreads</li> <li>Flexible node-to-area mapping with time series support</li> </ul>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/","title":"Area Variable Accounting","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mescal.energy_data_handling.area_accounting.area_variable_price_calculator.AreaPriceCalculator","title":"AreaPriceCalculator","text":"<p>               Bases: <code>AreaVariableCalculatorBase</code></p> <p>Calculates area-level prices from node prices using simple or weighted averaging.</p> <p>This calculator aggregates node-level electricity prices to area-level (e.g., bidding zones, countries) using either simple averaging or weighted averaging based on demand, supply, or other energy quantities. It's particularly useful in energy market analysis where different regions may have multiple price nodes that need to be consolidated into representative area prices.</p> <p>The class inherits from AreaVariableCalculatorBase and provides energy-aware price aggregation that handles edge cases like zero weights and missing data appropriately.</p> <p>Typical use cases: - Aggregating nodal prices to bidding zone prices - Creating country-level price indices from multiple market nodes - Volume-weighted price calculations for regional analysis</p> <p>Parameters:</p> Name Type Description Default <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame with node-area mappings</p> required <code>area_column</code> <code>str</code> <p>Column name containing area identifiers</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Node model with area mapping\n&gt;&gt;&gt; node_model = pd.DataFrame({\n...     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR']\n... }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Price calculator\n&gt;&gt;&gt; calc = AreaPriceCalculator(node_model, 'bidding_zone')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Node prices\n&gt;&gt;&gt; prices = pd.DataFrame({\n...     'DE1': [50.0, 45.0], 'DE2': [52.0, 47.0],\n...     'FR1': [55.0, 48.0], 'FR2': [53.0, 46.0]\n... }, index=pd.date_range('2024-01-01', periods=2, freq='h'))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple average\n&gt;&gt;&gt; area_prices = calc.calculate(prices)\n&gt;&gt;&gt; print(area_prices)\n           bidding_zone  DE_LU FR\n    datetime\n    2024-01-01 00:00:00  51.0  54.0\n    2024-01-01 01:00:00  46.0  47.0\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_variable_price_calculator.py</code> <pre><code>class AreaPriceCalculator(AreaVariableCalculatorBase):\n    \"\"\"Calculates area-level prices from node prices using simple or weighted averaging.\n\n    This calculator aggregates node-level electricity prices to area-level (e.g., bidding zones,\n    countries) using either simple averaging or weighted averaging based on demand, supply, or\n    other energy quantities. It's particularly useful in energy market analysis where different\n    regions may have multiple price nodes that need to be consolidated into representative area\n    prices.\n\n    The class inherits from AreaVariableCalculatorBase and provides energy-aware price aggregation\n    that handles edge cases like zero weights and missing data appropriately.\n\n    Typical use cases:\n    - Aggregating nodal prices to bidding zone prices\n    - Creating country-level price indices from multiple market nodes\n    - Volume-weighted price calculations for regional analysis\n\n    Args:\n        node_model_df: DataFrame with node-area mappings\n        area_column: Column name containing area identifiers\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Node model with area mapping\n        &gt;&gt;&gt; node_model = pd.DataFrame({\n        ...     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR']\n        ... }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Price calculator\n        &gt;&gt;&gt; calc = AreaPriceCalculator(node_model, 'bidding_zone')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Node prices\n        &gt;&gt;&gt; prices = pd.DataFrame({\n        ...     'DE1': [50.0, 45.0], 'DE2': [52.0, 47.0],\n        ...     'FR1': [55.0, 48.0], 'FR2': [53.0, 46.0]\n        ... }, index=pd.date_range('2024-01-01', periods=2, freq='h'))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple average\n        &gt;&gt;&gt; area_prices = calc.calculate(prices)\n        &gt;&gt;&gt; print(area_prices)\n                   bidding_zone  DE_LU FR\n            datetime\n            2024-01-01 00:00:00  51.0  54.0\n            2024-01-01 01:00:00  46.0  47.0\n    \"\"\"\n\n    def calculate(\n        self,\n        node_price_df: pd.DataFrame,\n        weighting_factor_df: pd.DataFrame = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Calculate area prices with different weighting options.\n\n        Aggregates node-level prices to area-level using simple averaging (when no weights\n        provided) or weighted averaging (when weights provided). The method handles missing\n        nodes gracefully and ensures proper handling of zero weights and NaN values.\n\n        In case you want to exclude certain nodes from the aggregation (e.g. because they\n        are virtual or synthetic nodes), you can simply remove them from the node_price_df\n        before passing it to this method.\n\n        Args:\n            node_price_df: Node-level price time series with datetime index and node columns.\n                Values represent electricity prices in \u20ac/MWh or similar units.\n            weighting_factor_df: Optional weighting factor DataFrame with same structure as\n                node_price_df. Common weighting factors include:\n                - node_demand_df: Demand-weighted prices\n                - node_supply_df: Supply-weighted prices  \n                - node_capacity_df: Capacity-weighted prices\n                If None, simple arithmetic average is used.\n\n        Returns:\n            DataFrame with area-level prices. Index matches input time series, columns\n            represent areas with prices in same units as input.\n\n        Raises:\n            ValueError: If node_price_df structure is invalid\n            KeyError: If required nodes are missing from weighting_factor_df\n\n        Example:\n\n            &gt;&gt;&gt; # Simple average\n            &gt;&gt;&gt; area_prices = calc.calculate(node_prices)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Demand-weighted average  \n            &gt;&gt;&gt; weighted_prices = calc.calculate(node_prices, node_demand)\n        \"\"\"\n        self._validate_node_data(node_price_df, 'node_price_df')\n\n        area_prices = {}\n\n        for area in self.areas:\n            area_nodes = self.get_area_nodes(area)\n            area_nodes = [n for n in area_nodes if n in node_price_df.columns]\n\n            if not area_nodes:\n                continue\n\n            prices = node_price_df[area_nodes]\n\n            if weighting_factor_df is None:\n                area_prices[area] = self._calculate_simple_average(prices)\n            else:\n                self._validate_node_data(weighting_factor_df, 'weighting_factor_df')\n                area_prices[area] = self._calculate_weighted_average(\n                    prices, weighting_factor_df[area_nodes]\n                )\n\n        result = pd.DataFrame(area_prices)\n        result.columns.name = self.area_column\n        return result\n\n    def _calculate_simple_average(self, prices: pd.DataFrame) -&gt; pd.Series:\n        return prices.mean(axis=1)\n\n    def _calculate_weighted_average(\n        self, \n        prices: pd.DataFrame, \n        weights: pd.DataFrame\n    ) -&gt; pd.Series:\n        \"\"\"Calculate weighted average of prices using provided weights.\n\n        Computes volume-weighted or otherwise weighted prices while handling edge cases\n        appropriately. When weights sum to zero, the method defaults to weight of 1 to\n        avoid division errors. When all prices are NaN for a time period, the result\n        is also NaN.\n\n        Args:\n            prices: DataFrame with price time series for nodes in an area\n            weights: DataFrame with weighting factors (e.g., demand, supply) with same\n                structure as prices. Must have non-negative values.\n\n        Returns:\n            Series with weighted average prices over time\n\n        Note:\n            This method assumes weights are extensive quantities (like energy volumes)\n            while prices are intensive quantities (like \u20ac/MWh).\n        \"\"\"\n        weighted_sum = (prices * weights).sum(axis=1)\n        weight_sum = weights.sum(axis=1).replace(0, 1)\n        weighted_price = weighted_sum / weight_sum\n        weighted_price[prices.isna().all(axis=1)] = np.nan\n        return weighted_price\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mescal.energy_data_handling.area_accounting.area_variable_price_calculator.AreaPriceCalculator.calculate","title":"calculate","text":"<pre><code>calculate(node_price_df: DataFrame, weighting_factor_df: DataFrame = None) -&gt; DataFrame\n</code></pre> <p>Calculate area prices with different weighting options.</p> <p>Aggregates node-level prices to area-level using simple averaging (when no weights provided) or weighted averaging (when weights provided). The method handles missing nodes gracefully and ensures proper handling of zero weights and NaN values.</p> <p>In case you want to exclude certain nodes from the aggregation (e.g. because they are virtual or synthetic nodes), you can simply remove them from the node_price_df before passing it to this method.</p> <p>Parameters:</p> Name Type Description Default <code>node_price_df</code> <code>DataFrame</code> <p>Node-level price time series with datetime index and node columns. Values represent electricity prices in \u20ac/MWh or similar units.</p> required <code>weighting_factor_df</code> <code>DataFrame</code> <p>Optional weighting factor DataFrame with same structure as node_price_df. Common weighting factors include: - node_demand_df: Demand-weighted prices - node_supply_df: Supply-weighted prices - node_capacity_df: Capacity-weighted prices If None, simple arithmetic average is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with area-level prices. Index matches input time series, columns</p> <code>DataFrame</code> <p>represent areas with prices in same units as input.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If node_price_df structure is invalid</p> <code>KeyError</code> <p>If required nodes are missing from weighting_factor_df</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Simple average\n&gt;&gt;&gt; area_prices = calc.calculate(node_prices)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Demand-weighted average  \n&gt;&gt;&gt; weighted_prices = calc.calculate(node_prices, node_demand)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_variable_price_calculator.py</code> <pre><code>def calculate(\n    self,\n    node_price_df: pd.DataFrame,\n    weighting_factor_df: pd.DataFrame = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate area prices with different weighting options.\n\n    Aggregates node-level prices to area-level using simple averaging (when no weights\n    provided) or weighted averaging (when weights provided). The method handles missing\n    nodes gracefully and ensures proper handling of zero weights and NaN values.\n\n    In case you want to exclude certain nodes from the aggregation (e.g. because they\n    are virtual or synthetic nodes), you can simply remove them from the node_price_df\n    before passing it to this method.\n\n    Args:\n        node_price_df: Node-level price time series with datetime index and node columns.\n            Values represent electricity prices in \u20ac/MWh or similar units.\n        weighting_factor_df: Optional weighting factor DataFrame with same structure as\n            node_price_df. Common weighting factors include:\n            - node_demand_df: Demand-weighted prices\n            - node_supply_df: Supply-weighted prices  \n            - node_capacity_df: Capacity-weighted prices\n            If None, simple arithmetic average is used.\n\n    Returns:\n        DataFrame with area-level prices. Index matches input time series, columns\n        represent areas with prices in same units as input.\n\n    Raises:\n        ValueError: If node_price_df structure is invalid\n        KeyError: If required nodes are missing from weighting_factor_df\n\n    Example:\n\n        &gt;&gt;&gt; # Simple average\n        &gt;&gt;&gt; area_prices = calc.calculate(node_prices)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Demand-weighted average  \n        &gt;&gt;&gt; weighted_prices = calc.calculate(node_prices, node_demand)\n    \"\"\"\n    self._validate_node_data(node_price_df, 'node_price_df')\n\n    area_prices = {}\n\n    for area in self.areas:\n        area_nodes = self.get_area_nodes(area)\n        area_nodes = [n for n in area_nodes if n in node_price_df.columns]\n\n        if not area_nodes:\n            continue\n\n        prices = node_price_df[area_nodes]\n\n        if weighting_factor_df is None:\n            area_prices[area] = self._calculate_simple_average(prices)\n        else:\n            self._validate_node_data(weighting_factor_df, 'weighting_factor_df')\n            area_prices[area] = self._calculate_weighted_average(\n                prices, weighting_factor_df[area_nodes]\n            )\n\n    result = pd.DataFrame(area_prices)\n    result.columns.name = self.area_column\n    return result\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mescal.energy_data_handling.area_accounting.area_variable_sum_calculator.AreaSumCalculator","title":"AreaSumCalculator","text":"<p>               Bases: <code>AreaVariableCalculatorBase</code></p> <p>General calculator for summing node-level extensive quantities to area level.</p> <p>This calculator aggregates extensive quantities (values that scale with system size) from node-level to area-level using summation. Typical use cases include power generation, demand, energy volumes, reserves, and other additive quantities in energy systems analysis.</p> <p>Unlike intensive quantities (like prices), extensive quantities should be summed when aggregating to higher geographic levels, making this calculator appropriate for many physical quantities in energy modeling.</p> <p>Inherits from AreaVariableCalculatorBase and provides the MESCAL framework's standard approach for area-level aggregation of extensive variables.</p> <p>Parameters:</p> Name Type Description Default <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame mapping nodes to areas</p> required <code>area_column</code> <code>str</code> <p>Column name containing area identifiers</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Node model\n&gt;&gt;&gt; node_model = pd.DataFrame({\n...     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR']\n... }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Sum calculator\n&gt;&gt;&gt; calc = AreaSumCalculator(node_model, 'bidding_zone')\n&gt;&gt;&gt; # Node generation data\n&gt;&gt;&gt; generation = pd.DataFrame({\n...     'DE1': [800, 850], 'DE2': [750, 780],\n...     'FR1': [900, 920], 'FR2': [850, 870]\n... }, index=pd.date_range('2024-01-01', periods=2, freq='h'))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Sum to areas\n&gt;&gt;&gt; area_generation = calc.calculate(generation)\n&gt;&gt;&gt; print(area_generation)\n    bidding_zone  DE_LU   FR\n    2024-01-01 00:00:00  1550  1750\n    2024-01-01 01:00:00  1630  1790\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_variable_sum_calculator.py</code> <pre><code>class AreaSumCalculator(AreaVariableCalculatorBase):\n    \"\"\"General calculator for summing node-level extensive quantities to area level.\n\n    This calculator aggregates extensive quantities (values that scale with system size)\n    from node-level to area-level using summation. Typical use cases include power\n    generation, demand, energy volumes, reserves, and other additive quantities in\n    energy systems analysis.\n\n    Unlike intensive quantities (like prices), extensive quantities should be summed\n    when aggregating to higher geographic levels, making this calculator appropriate\n    for many physical quantities in energy modeling.\n\n    Inherits from AreaVariableCalculatorBase and provides the MESCAL framework's\n    standard approach for area-level aggregation of extensive variables.\n\n    Args:\n        node_model_df: DataFrame mapping nodes to areas\n        area_column: Column name containing area identifiers\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Node model\n        &gt;&gt;&gt; node_model = pd.DataFrame({\n        ...     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR']\n        ... }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Sum calculator\n        &gt;&gt;&gt; calc = AreaSumCalculator(node_model, 'bidding_zone')\n        &gt;&gt;&gt; # Node generation data\n        &gt;&gt;&gt; generation = pd.DataFrame({\n        ...     'DE1': [800, 850], 'DE2': [750, 780],\n        ...     'FR1': [900, 920], 'FR2': [850, 870]\n        ... }, index=pd.date_range('2024-01-01', periods=2, freq='h'))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Sum to areas\n        &gt;&gt;&gt; area_generation = calc.calculate(generation)\n        &gt;&gt;&gt; print(area_generation)\n            bidding_zone  DE_LU   FR\n            2024-01-01 00:00:00  1550  1750\n            2024-01-01 01:00:00  1630  1790\n    \"\"\"\n\n    def calculate(self, node_data_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Calculate area sums from node-level extensive quantity data.\n\n        Sums node-level values within each area to create area-level aggregates.\n        This method is designed for extensive quantities where summation is the\n        appropriate aggregation method (e.g., generation, demand, volumes).\n\n        Missing nodes are handled gracefully - if a node exists in the node model\n        but not in the data, it's simply ignored. Areas with no available nodes\n        are omitted from the output.\n\n        In case you want to exclude certain nodes from the aggregation (e.g. because\n        they are virtual or synthetic nodes), you can simply remove them from the\n        node_data_df before passing it to this method.\n\n        Args:\n            node_data_df: DataFrame with node-level time series data. Index should\n                be datetime, columns should be node identifiers. Values represent\n                extensive quantities (MW, MWh, etc.) that should be summed.\n\n        Returns:\n            DataFrame with area-level aggregated data. Index matches input time series,\n            columns represent areas. Units are preserved from input data.\n\n        Raises:\n            ValueError: If node_data_df structure is invalid\n\n        Example:\n\n            &gt;&gt;&gt; # Sum generation across nodes\n            &gt;&gt;&gt; area_generation = calc.calculate(node_generation_df)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Sum demand across nodes  \n            &gt;&gt;&gt; area_demand = calc.calculate(node_demand_df)\n        \"\"\"\n\n        self._validate_node_data(node_data_df, 'node_data_df')\n\n        area_sums = {}\n        for area in self.areas:\n            area_nodes = self.get_area_nodes(area)\n            area_nodes = [n for n in area_nodes if n in node_data_df.columns]\n            if area_nodes:\n                area_sums[area] = node_data_df[area_nodes].sum(axis=1)\n\n        result = pd.DataFrame(area_sums)\n        result.columns.name = self.area_column\n        return result\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mescal.energy_data_handling.area_accounting.area_variable_sum_calculator.AreaSumCalculator.calculate","title":"calculate","text":"<pre><code>calculate(node_data_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Calculate area sums from node-level extensive quantity data.</p> <p>Sums node-level values within each area to create area-level aggregates. This method is designed for extensive quantities where summation is the appropriate aggregation method (e.g., generation, demand, volumes).</p> <p>Missing nodes are handled gracefully - if a node exists in the node model but not in the data, it's simply ignored. Areas with no available nodes are omitted from the output.</p> <p>In case you want to exclude certain nodes from the aggregation (e.g. because they are virtual or synthetic nodes), you can simply remove them from the node_data_df before passing it to this method.</p> <p>Parameters:</p> Name Type Description Default <code>node_data_df</code> <code>DataFrame</code> <p>DataFrame with node-level time series data. Index should be datetime, columns should be node identifiers. Values represent extensive quantities (MW, MWh, etc.) that should be summed.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with area-level aggregated data. Index matches input time series,</p> <code>DataFrame</code> <p>columns represent areas. Units are preserved from input data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If node_data_df structure is invalid</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Sum generation across nodes\n&gt;&gt;&gt; area_generation = calc.calculate(node_generation_df)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Sum demand across nodes  \n&gt;&gt;&gt; area_demand = calc.calculate(node_demand_df)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_variable_sum_calculator.py</code> <pre><code>def calculate(self, node_data_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate area sums from node-level extensive quantity data.\n\n    Sums node-level values within each area to create area-level aggregates.\n    This method is designed for extensive quantities where summation is the\n    appropriate aggregation method (e.g., generation, demand, volumes).\n\n    Missing nodes are handled gracefully - if a node exists in the node model\n    but not in the data, it's simply ignored. Areas with no available nodes\n    are omitted from the output.\n\n    In case you want to exclude certain nodes from the aggregation (e.g. because\n    they are virtual or synthetic nodes), you can simply remove them from the\n    node_data_df before passing it to this method.\n\n    Args:\n        node_data_df: DataFrame with node-level time series data. Index should\n            be datetime, columns should be node identifiers. Values represent\n            extensive quantities (MW, MWh, etc.) that should be summed.\n\n    Returns:\n        DataFrame with area-level aggregated data. Index matches input time series,\n        columns represent areas. Units are preserved from input data.\n\n    Raises:\n        ValueError: If node_data_df structure is invalid\n\n    Example:\n\n        &gt;&gt;&gt; # Sum generation across nodes\n        &gt;&gt;&gt; area_generation = calc.calculate(node_generation_df)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Sum demand across nodes  \n        &gt;&gt;&gt; area_demand = calc.calculate(node_demand_df)\n    \"\"\"\n\n    self._validate_node_data(node_data_df, 'node_data_df')\n\n    area_sums = {}\n    for area in self.areas:\n        area_nodes = self.get_area_nodes(area)\n        area_nodes = [n for n in area_nodes if n in node_data_df.columns]\n        if area_nodes:\n            area_sums[area] = node_data_df[area_nodes].sum(axis=1)\n\n    result = pd.DataFrame(area_sums)\n    result.columns.name = self.area_column\n    return result\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mescal.energy_data_handling.area_accounting.area_variable_base.AreaVariableCalculatorBase","title":"AreaVariableCalculatorBase","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for calculating energy variables aggregated at area level.</p> <p>This base class provides common functionality for aggregating node-level energy data (such as generation, demand, prices) to higher-level areas (countries, bidding zones, market areas). It handles the mapping between nodes and areas and provides validation and utility methods for area-based calculations.</p> <p>The class is designed to be subclassed for specific variable types, with each subclass implementing its own calculation logic while leveraging the common area mapping and validation functionality provided here.</p> <p>Energy market context: In electricity markets, many variables are naturally defined at the nodal level  (generators, loads, prices) but need to be aggregated to market or geographical areas for analysis, reporting, and trading. This aggregation must handle missing data, different node counts per area, and preserve energy-specific semantics.</p> <p>Parameters:</p> Name Type Description Default <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame containing node information with area assignments. Index should be node identifiers, must contain the specified area_column.</p> required <code>area_column</code> <code>str</code> <p>Name of the column in node_model_df that contains area assignments. Each node should be assigned to exactly one area (NaN values are allowed).</p> required <p>Attributes:</p> Name Type Description <code>node_model_df</code> <p>The input node model DataFrame</p> <code>area_column</code> <p>Name of the area assignment column</p> <code>node_to_area_map</code> <p>Dictionary mapping node IDs to area names</p> <code>areas</code> <p>Sorted list of unique area names (excluding NaN)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If area_column is not found in node_model_df</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Node model with area assignments\n&gt;&gt;&gt; node_model = pd.DataFrame({\n...     'country': ['DE', 'DE', 'FR', 'FR', 'BE'],\n...     'voltage': [380, 220, 380, 220, 380]\n... }, index=['DE1', 'DE2', 'FR1', 'FR2', 'BE1'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Subclass implementation\n&gt;&gt;&gt; class MyAreaCalculator(AreaVariableCalculatorBase):\n...     def calculate(self, **kwargs):\n...         return pd.DataFrame()  # Implementation here\n&gt;&gt;&gt; \n&gt;&gt;&gt; calculator = MyAreaCalculator(node_model, 'country')\n&gt;&gt;&gt; print(calculator.areas)  # ['BE', 'DE', 'FR']\n&gt;&gt;&gt; print(calculator.get_area_nodes('DE'))  # ['DE1', 'DE2']\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_variable_base.py</code> <pre><code>class AreaVariableCalculatorBase(ABC):\n    \"\"\"Abstract base class for calculating energy variables aggregated at area level.\n\n    This base class provides common functionality for aggregating node-level energy data\n    (such as generation, demand, prices) to higher-level areas (countries, bidding zones,\n    market areas). It handles the mapping between nodes and areas and provides validation\n    and utility methods for area-based calculations.\n\n    The class is designed to be subclassed for specific variable types, with each subclass\n    implementing its own calculation logic while leveraging the common area mapping and\n    validation functionality provided here.\n\n    Energy market context:\n    In electricity markets, many variables are naturally defined at the nodal level \n    (generators, loads, prices) but need to be aggregated to market or geographical\n    areas for analysis, reporting, and trading. This aggregation must handle missing\n    data, different node counts per area, and preserve energy-specific semantics.\n\n    Args:\n        node_model_df: DataFrame containing node information with area assignments.\n            Index should be node identifiers, must contain the specified area_column.\n        area_column: Name of the column in node_model_df that contains area assignments.\n            Each node should be assigned to exactly one area (NaN values are allowed).\n\n    Attributes:\n        node_model_df: The input node model DataFrame\n        area_column: Name of the area assignment column\n        node_to_area_map: Dictionary mapping node IDs to area names\n        areas: Sorted list of unique area names (excluding NaN)\n\n    Raises:\n        ValueError: If area_column is not found in node_model_df\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; # Node model with area assignments\n        &gt;&gt;&gt; node_model = pd.DataFrame({\n        ...     'country': ['DE', 'DE', 'FR', 'FR', 'BE'],\n        ...     'voltage': [380, 220, 380, 220, 380]\n        ... }, index=['DE1', 'DE2', 'FR1', 'FR2', 'BE1'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Subclass implementation\n        &gt;&gt;&gt; class MyAreaCalculator(AreaVariableCalculatorBase):\n        ...     def calculate(self, **kwargs):\n        ...         return pd.DataFrame()  # Implementation here\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; calculator = MyAreaCalculator(node_model, 'country')\n        &gt;&gt;&gt; print(calculator.areas)  # ['BE', 'DE', 'FR']\n        &gt;&gt;&gt; print(calculator.get_area_nodes('DE'))  # ['DE1', 'DE2']\n    \"\"\"\n\n    def __init__(self, node_model_df: pd.DataFrame, area_column: str):\n        \"\"\"Initialize the area variable calculator.\n\n        Args:\n            node_model_df: DataFrame with node-to-area mapping\n            area_column: Column name containing area assignments\n\n        Raises:\n            ValueError: If area_column not found in node_model_df\n        \"\"\"\n        self.node_model_df = node_model_df\n        self.area_column = area_column\n        self.node_to_area_map = self._create_node_to_area_map()\n        self.areas = sorted(self.node_model_df[area_column].dropna().unique())\n        self._validate_inputs()\n\n    def _validate_inputs(self):\n        \"\"\"Validate input parameters during initialization.\n\n        Raises:\n            ValueError: If area_column is not found in node_model_df\n        \"\"\"\n        if self.area_column not in self.node_model_df.columns:\n            raise ValueError(f\"Column '{self.area_column}' not found in node_model_df\")\n\n    def _create_node_to_area_map(self) -&gt; dict[str, str]:\n        return self.node_model_df[self.area_column].to_dict()\n\n    def get_area_nodes(self, area: str) -&gt; list[str]:\n        \"\"\"Get all nodes belonging to a specific area.\n\n        Args:\n            area: Area name to get nodes for\n\n        Returns:\n            List of node IDs that belong to the specified area\n\n        Example:\n\n            &gt;&gt;&gt; calculator = MyAreaCalculator(node_model, 'country')\n            &gt;&gt;&gt; german_nodes = calculator.get_area_nodes('DE')\n            &gt;&gt;&gt; print(german_nodes)  # ['DE1', 'DE2']\n        \"\"\"\n        return self.node_model_df[self.node_model_df[self.area_column] == area].index.tolist()\n\n    @abstractmethod\n    def calculate(self, **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Calculate the area variable. Must be implemented by subclasses.\n\n        This method should contain the specific logic for aggregating node-level\n        data to area level for the particular variable type. The implementation\n        will vary depending on whether the variable is extensive (additive like\n        energy volumes) or intensive (averaged like prices).\n\n        Args:\n            **kwargs: Variable-specific parameters for the calculation\n\n        Returns:\n            DataFrame with area-level aggregated data. Index should be datetime\n            for time series data, columns should be area identifiers.\n\n        Raises:\n            NotImplementedError: This is an abstract method\n        \"\"\"\n        pass\n\n    def _validate_node_data(self, node_df: pd.DataFrame, data_name: str):\n        \"\"\"Validate that required nodes are present in node_model_df.\n\n        Logs warnings for any nodes found in the data that are not in the node model.\n        This is important for detecting data inconsistencies or model updates.\n\n        Args:\n            node_df: DataFrame containing node-level data to validate\n            data_name: Descriptive name of the data being validated (for logging)\n\n        Example:\n\n            &gt;&gt;&gt; # Log warning if generation_data has nodes not in node_model_df\n            &gt;&gt;&gt; calculator._validate_node_data(generation_data, \"generation\")\n        \"\"\"\n        missing_nodes = set(node_df.columns) - set(self.node_model_df.index)\n        if missing_nodes:\n            logger.warning(f\"{len(missing_nodes)} nodes missing in node_model_df from {data_name}\")\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mescal.energy_data_handling.area_accounting.area_variable_base.AreaVariableCalculatorBase.__init__","title":"__init__","text":"<pre><code>__init__(node_model_df: DataFrame, area_column: str)\n</code></pre> <p>Initialize the area variable calculator.</p> <p>Parameters:</p> Name Type Description Default <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame with node-to-area mapping</p> required <code>area_column</code> <code>str</code> <p>Column name containing area assignments</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If area_column not found in node_model_df</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_variable_base.py</code> <pre><code>def __init__(self, node_model_df: pd.DataFrame, area_column: str):\n    \"\"\"Initialize the area variable calculator.\n\n    Args:\n        node_model_df: DataFrame with node-to-area mapping\n        area_column: Column name containing area assignments\n\n    Raises:\n        ValueError: If area_column not found in node_model_df\n    \"\"\"\n    self.node_model_df = node_model_df\n    self.area_column = area_column\n    self.node_to_area_map = self._create_node_to_area_map()\n    self.areas = sorted(self.node_model_df[area_column].dropna().unique())\n    self._validate_inputs()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mescal.energy_data_handling.area_accounting.area_variable_base.AreaVariableCalculatorBase.get_area_nodes","title":"get_area_nodes","text":"<pre><code>get_area_nodes(area: str) -&gt; list[str]\n</code></pre> <p>Get all nodes belonging to a specific area.</p> <p>Parameters:</p> Name Type Description Default <code>area</code> <code>str</code> <p>Area name to get nodes for</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of node IDs that belong to the specified area</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; calculator = MyAreaCalculator(node_model, 'country')\n&gt;&gt;&gt; german_nodes = calculator.get_area_nodes('DE')\n&gt;&gt;&gt; print(german_nodes)  # ['DE1', 'DE2']\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_variable_base.py</code> <pre><code>def get_area_nodes(self, area: str) -&gt; list[str]:\n    \"\"\"Get all nodes belonging to a specific area.\n\n    Args:\n        area: Area name to get nodes for\n\n    Returns:\n        List of node IDs that belong to the specified area\n\n    Example:\n\n        &gt;&gt;&gt; calculator = MyAreaCalculator(node_model, 'country')\n        &gt;&gt;&gt; german_nodes = calculator.get_area_nodes('DE')\n        &gt;&gt;&gt; print(german_nodes)  # ['DE1', 'DE2']\n    \"\"\"\n    return self.node_model_df[self.node_model_df[self.area_column] == area].index.tolist()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mescal.energy_data_handling.area_accounting.area_variable_base.AreaVariableCalculatorBase.calculate","title":"calculate  <code>abstractmethod</code>","text":"<pre><code>calculate(**kwargs) -&gt; DataFrame\n</code></pre> <p>Calculate the area variable. Must be implemented by subclasses.</p> <p>This method should contain the specific logic for aggregating node-level data to area level for the particular variable type. The implementation will vary depending on whether the variable is extensive (additive like energy volumes) or intensive (averaged like prices).</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Variable-specific parameters for the calculation</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with area-level aggregated data. Index should be datetime</p> <code>DataFrame</code> <p>for time series data, columns should be area identifiers.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This is an abstract method</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_variable_base.py</code> <pre><code>@abstractmethod\ndef calculate(self, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Calculate the area variable. Must be implemented by subclasses.\n\n    This method should contain the specific logic for aggregating node-level\n    data to area level for the particular variable type. The implementation\n    will vary depending on whether the variable is extensive (additive like\n    energy volumes) or intensive (averaged like prices).\n\n    Args:\n        **kwargs: Variable-specific parameters for the calculation\n\n    Returns:\n        DataFrame with area-level aggregated data. Index should be datetime\n        for time series data, columns should be area identifiers.\n\n    Raises:\n        NotImplementedError: This is an abstract method\n    \"\"\"\n    pass\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mescal.energy_data_handling.area_accounting.area_variable_base.ExampleSumCalculator","title":"ExampleSumCalculator","text":"<p>               Bases: <code>AreaVariableCalculatorBase</code></p> <p>Example implementation that sums node-level data to area level.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_variable_base.py</code> <pre><code>class ExampleSumCalculator(AreaVariableCalculatorBase):\n    \"\"\"Example implementation that sums node-level data to area level.\"\"\"\n\n    def calculate(self, node_data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Sum node data for each area (extensive variable aggregation).\"\"\"\n        self._validate_node_data(node_data, \"example_data\")\n\n        result_dict = {}\n        for area in self.areas:\n            area_nodes = self.get_area_nodes(area)\n            # Filter to nodes that exist in both model and data\n            available_nodes = [n for n in area_nodes if n in node_data.columns]\n            if available_nodes:\n                result_dict[area] = node_data[available_nodes].sum(axis=1)\n            else:\n                # Create empty series with same index if no data available\n                result_dict[area] = pd.Series(index=node_data.index, dtype=float)\n\n        result_df = pd.DataFrame(result_dict)\n        result_df.columns.name = self.area_column\n        return result_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mescal.energy_data_handling.area_accounting.area_variable_base.ExampleSumCalculator.calculate","title":"calculate","text":"<pre><code>calculate(node_data: DataFrame) -&gt; DataFrame\n</code></pre> <p>Sum node data for each area (extensive variable aggregation).</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_variable_base.py</code> <pre><code>def calculate(self, node_data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Sum node data for each area (extensive variable aggregation).\"\"\"\n    self._validate_node_data(node_data, \"example_data\")\n\n    result_dict = {}\n    for area in self.areas:\n        area_nodes = self.get_area_nodes(area)\n        # Filter to nodes that exist in both model and data\n        available_nodes = [n for n in area_nodes if n in node_data.columns]\n        if available_nodes:\n            result_dict[area] = node_data[available_nodes].sum(axis=1)\n        else:\n            # Create empty series with same index if no data available\n            result_dict[area] = pd.Series(index=node_data.index, dtype=float)\n\n    result_df = pd.DataFrame(result_dict)\n    result_df.columns.name = self.area_column\n    return result_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/","title":"AreaBorder Variable Accounting","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mescal.energy_data_handling.area_accounting.border_variable_base.AreaBorderVariableCalculatorBase","title":"AreaBorderVariableCalculatorBase","text":"<p>               Bases: <code>ABC</code>, <code>AreaBorderNamingConventions</code></p> <p>Abstract base class for calculating energy variables at area border level.</p> <p>This base class provides functionality for aggregating line-level energy data  (flows, capacities, price spreads) to area border level. An area border represents the interface between two areas (countries, bidding zones, etc.).</p> <p>The class handles the complex mapping from transmission lines to area borders, including proper handling of line directionality. Lines are classified as either \"up\" or \"down\" relative to the border direction based on their node endpoints.</p> <p>Border directionality: - \"Up\" direction: From area_from to area_to (as defined in border naming) - \"Down\" direction: From area_to to area_from - Line direction is determined by comparing line endpoints to border areas</p> <p>Parameters:</p> Name Type Description Default <code>area_border_model_df</code> <code>DataFrame</code> <p>DataFrame containing area border definitions. Index should be border identifiers (e.g., 'DE-FR', 'FR-BE').</p> required <code>line_model_df</code> <code>DataFrame</code> <p>DataFrame containing transmission line information. Must include node_from_col and node_to_col columns.</p> required <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame containing node information with area assignments. Must include area_column for mapping nodes to areas.</p> required <code>area_column</code> <code>str</code> <p>Column name in node_model_df containing area assignments.</p> required <code>node_from_col</code> <code>str</code> <p>Column name in line_model_df for line starting node.</p> <code>'node_from'</code> <code>node_to_col</code> <code>str</code> <p>Column name in line_model_df for line ending node.</p> <code>'node_to'</code> <p>Attributes:</p> Name Type Description <code>area_border_model_df</code> <p>Border model DataFrame</p> <code>line_model_df</code> <p>Line model DataFrame  </p> <code>node_model_df</code> <p>Node model DataFrame</p> <code>area_column</code> <p>Name of area assignment column</p> <code>node_from_col</code> <p>Name of line from-node column</p> <code>node_to_col</code> <p>Name of line to-node column</p> <code>node_to_area_map</code> <p>Dictionary mapping node IDs to area names</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from input DataFrames</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Define borders between areas\n&gt;&gt;&gt; border_model = pd.DataFrame(index=['DE-FR', 'FR-BE'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Define transmission lines  \n&gt;&gt;&gt; line_model = pd.DataFrame({\n...     'node_from': ['DE1', 'FR1'],\n...     'node_to': ['FR1', 'BE1'],\n...     'capacity': [1000, 800]\n... }, index=['Line1', 'Line2'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Node-to-area mapping\n&gt;&gt;&gt; node_model = pd.DataFrame({\n...     'country': ['DE', 'FR', 'BE']\n... }, index=['DE1', 'FR1', 'BE1'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Subclass for specific calculation\n&gt;&gt;&gt; class MyBorderCalculator(AreaBorderVariableCalculatorBase):\n...     @property\n...     def variable_name(self):\n...         return \"my_variable\"\n...     def calculate(self, **kwargs):\n...         return pd.DataFrame()\n&gt;&gt;&gt; \n&gt;&gt;&gt; calculator = MyBorderCalculator(\n...     border_model, line_model, node_model, 'country'\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_variable_base.py</code> <pre><code>class AreaBorderVariableCalculatorBase(ABC, AreaBorderNamingConventions):\n    \"\"\"Abstract base class for calculating energy variables at area border level.\n\n    This base class provides functionality for aggregating line-level energy data \n    (flows, capacities, price spreads) to area border level. An area border represents\n    the interface between two areas (countries, bidding zones, etc.).\n\n    The class handles the complex mapping from transmission lines to area borders,\n    including proper handling of line directionality. Lines are classified as either\n    \"up\" or \"down\" relative to the border direction based on their node endpoints.\n\n    Border directionality:\n    - \"Up\" direction: From area_from to area_to (as defined in border naming)\n    - \"Down\" direction: From area_to to area_from\n    - Line direction is determined by comparing line endpoints to border areas\n\n    Args:\n        area_border_model_df: DataFrame containing area border definitions.\n            Index should be border identifiers (e.g., 'DE-FR', 'FR-BE').\n        line_model_df: DataFrame containing transmission line information.\n            Must include node_from_col and node_to_col columns.\n        node_model_df: DataFrame containing node information with area assignments.\n            Must include area_column for mapping nodes to areas.\n        area_column: Column name in node_model_df containing area assignments.\n        node_from_col: Column name in line_model_df for line starting node.\n        node_to_col: Column name in line_model_df for line ending node.\n\n    Attributes:\n        area_border_model_df: Border model DataFrame\n        line_model_df: Line model DataFrame  \n        node_model_df: Node model DataFrame\n        area_column: Name of area assignment column\n        node_from_col: Name of line from-node column\n        node_to_col: Name of line to-node column\n        node_to_area_map: Dictionary mapping node IDs to area names\n\n    Raises:\n        ValueError: If required columns are missing from input DataFrames\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; # Define borders between areas\n        &gt;&gt;&gt; border_model = pd.DataFrame(index=['DE-FR', 'FR-BE'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Define transmission lines  \n        &gt;&gt;&gt; line_model = pd.DataFrame({\n        ...     'node_from': ['DE1', 'FR1'],\n        ...     'node_to': ['FR1', 'BE1'],\n        ...     'capacity': [1000, 800]\n        ... }, index=['Line1', 'Line2'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Node-to-area mapping\n        &gt;&gt;&gt; node_model = pd.DataFrame({\n        ...     'country': ['DE', 'FR', 'BE']\n        ... }, index=['DE1', 'FR1', 'BE1'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Subclass for specific calculation\n        &gt;&gt;&gt; class MyBorderCalculator(AreaBorderVariableCalculatorBase):\n        ...     @property\n        ...     def variable_name(self):\n        ...         return \"my_variable\"\n        ...     def calculate(self, **kwargs):\n        ...         return pd.DataFrame()\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; calculator = MyBorderCalculator(\n        ...     border_model, line_model, node_model, 'country'\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        area_border_model_df: pd.DataFrame,\n        line_model_df: pd.DataFrame,\n        node_model_df: pd.DataFrame,\n        area_column: str,\n        node_from_col: str = 'node_from',\n        node_to_col: str = 'node_to'\n    ):\n        \"\"\"Initialize the area border variable calculator.\n\n        Args:\n            area_border_model_df: DataFrame with border definitions\n            line_model_df: DataFrame with line information including endpoints\n            node_model_df: DataFrame with node-to-area mapping\n            area_column: Column name for area assignments in node_model_df\n            node_from_col: Column name for line starting node in line_model_df\n            node_to_col: Column name for line ending node in line_model_df\n\n        Raises:\n            ValueError: If required columns are missing from DataFrames\n        \"\"\"\n        super().__init__(area_column)\n        self.area_border_model_df = area_border_model_df\n        self.line_model_df = line_model_df\n        self.node_model_df = node_model_df\n        self.area_column = area_column\n        self.node_from_col = node_from_col\n        self.node_to_col = node_to_col\n        self.node_to_area_map = self._create_node_to_area_map()\n        self._validate_inputs()\n\n    def _validate_inputs(self):\n        \"\"\"Validate input parameters during initialization.\n\n        Raises:\n            ValueError: If required columns are missing from DataFrames\n        \"\"\"\n        if self.area_column not in self.node_model_df.columns:\n            raise ValueError(f\"Column '{self.area_column}' not found in node_model_df\")\n        if self.node_from_col not in self.line_model_df.columns:\n            raise ValueError(f\"Column '{self.node_from_col}' not found in line_model_df\")\n        if self.node_to_col not in self.line_model_df.columns:\n            raise ValueError(f\"Column '{self.node_to_col}' not found in line_model_df\")\n\n    def _create_node_to_area_map(self) -&gt; dict[Hashable, str]:\n        \"\"\"Create a mapping dictionary from node IDs to area names.\n\n        Returns:\n            Dictionary with node IDs as keys and area names as values.\n            Nodes with NaN area assignments will have NaN values.\n        \"\"\"\n        return self.node_model_df[self.area_column].to_dict()\n\n    def get_border_lines_in_topological_up_and_down_direction(self, border_id: str) -&gt; tuple[list[Hashable], list[Hashable]]:\n        \"\"\"Get transmission lines for a border classified by topological direction.\n\n        This method identifies which transmission lines connect the two areas of a border\n        and classifies them based on their topological direction relative to the border.\n\n        Border directionality logic:\n        - \"Up\" direction: Lines where node_from is in area_from and node_to is in area_to\n        - \"Down\" direction: Lines where node_from is in area_to and node_to is in area_from\n\n        This classification is essential for correctly aggregating directional quantities\n        like power flows, where the sign and direction matter for market analysis.\n\n        Args:\n            border_id: Border identifier (e.g., 'DE-FR') that will be decomposed\n                into area_from and area_to using the naming convention.\n\n        Returns:\n            Tuple containing two lists:\n            - lines_up: Line IDs for lines in the \"up\" direction\n            - lines_down: Line IDs for lines in the \"down\" direction\n\n        Example:\n\n            &gt;&gt;&gt; # For border 'DE-FR'\n            &gt;&gt;&gt; lines_up, lines_down = calculator.get_border_lines_in_topological_up_and_down_direction('DE-FR')\n            &gt;&gt;&gt; # lines_up: Lines from German nodes to French nodes  \n            &gt;&gt;&gt; # lines_down: Lines from French nodes to German nodes\n        \"\"\"\n        area_from, area_to = self.decompose_area_border_name_to_areas(border_id)\n        nodes_in_area_from = self.node_model_df.loc[self.node_model_df[self.area_column] == area_from].index.to_list()\n        nodes_in_area_to = self.node_model_df.loc[self.node_model_df[self.area_column] == area_to].index.to_list()\n        lines_up = self.line_model_df.loc[\n                self.line_model_df[self.node_from_col].isin(nodes_in_area_from)\n                &amp; self.line_model_df[self.node_to_col].isin(nodes_in_area_to)\n            ].index.to_list()\n        lines_down = self.line_model_df.loc[\n                self.line_model_df[self.node_from_col].isin(nodes_in_area_to)\n                &amp; self.line_model_df[self.node_to_col].isin(nodes_in_area_from)\n            ].index.to_list()\n        return lines_up, lines_down\n\n    @abstractmethod\n    def calculate(self, **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Calculate the border variable. Must be implemented by subclasses.\n\n        This method should contain the specific logic for aggregating line-level\n        data to border level for the particular variable type. The implementation\n        will vary based on the variable (flows, capacities, prices, etc.) and\n        should handle directional aggregation appropriately.\n\n        Args:\n            **kwargs: Variable-specific parameters for the calculation\n\n        Returns:\n            DataFrame with border-level aggregated data. Index should be datetime\n            for time series data, columns should be border identifiers.\n\n        Raises:\n            NotImplementedError: This is an abstract method\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def variable_name(self) -&gt; str:\n        \"\"\"Name of the variable being calculated.\n\n        This property should return a descriptive name for the variable being\n        calculated by this calculator. Used for naming output columns and logging.\n\n        Returns:\n            String name of the variable (e.g., 'border_flow', 'border_capacity')\n        \"\"\"\n        pass\n\n    def _validate_time_series_data(self, df: pd.DataFrame, data_name: str):\n        \"\"\"Validate that time series data has appropriate datetime index.\n\n        Logs warnings if the data doesn't have a DatetimeIndex, which may indicate\n        data formatting issues or non-time-series data being used inappropriately.\n\n        Args:\n            df: DataFrame to validate\n            data_name: Descriptive name of the data for logging purposes\n        \"\"\"\n        if not isinstance(df.index, pd.DatetimeIndex):\n            logger.warning(f\"{data_name} does not have DatetimeIndex\")\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mescal.energy_data_handling.area_accounting.border_variable_base.AreaBorderVariableCalculatorBase.variable_name","title":"variable_name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>variable_name: str\n</code></pre> <p>Name of the variable being calculated.</p> <p>This property should return a descriptive name for the variable being calculated by this calculator. Used for naming output columns and logging.</p> <p>Returns:</p> Type Description <code>str</code> <p>String name of the variable (e.g., 'border_flow', 'border_capacity')</p>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mescal.energy_data_handling.area_accounting.border_variable_base.AreaBorderVariableCalculatorBase.__init__","title":"__init__","text":"<pre><code>__init__(area_border_model_df: DataFrame, line_model_df: DataFrame, node_model_df: DataFrame, area_column: str, node_from_col: str = 'node_from', node_to_col: str = 'node_to')\n</code></pre> <p>Initialize the area border variable calculator.</p> <p>Parameters:</p> Name Type Description Default <code>area_border_model_df</code> <code>DataFrame</code> <p>DataFrame with border definitions</p> required <code>line_model_df</code> <code>DataFrame</code> <p>DataFrame with line information including endpoints</p> required <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame with node-to-area mapping</p> required <code>area_column</code> <code>str</code> <p>Column name for area assignments in node_model_df</p> required <code>node_from_col</code> <code>str</code> <p>Column name for line starting node in line_model_df</p> <code>'node_from'</code> <code>node_to_col</code> <code>str</code> <p>Column name for line ending node in line_model_df</p> <code>'node_to'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from DataFrames</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_variable_base.py</code> <pre><code>def __init__(\n    self,\n    area_border_model_df: pd.DataFrame,\n    line_model_df: pd.DataFrame,\n    node_model_df: pd.DataFrame,\n    area_column: str,\n    node_from_col: str = 'node_from',\n    node_to_col: str = 'node_to'\n):\n    \"\"\"Initialize the area border variable calculator.\n\n    Args:\n        area_border_model_df: DataFrame with border definitions\n        line_model_df: DataFrame with line information including endpoints\n        node_model_df: DataFrame with node-to-area mapping\n        area_column: Column name for area assignments in node_model_df\n        node_from_col: Column name for line starting node in line_model_df\n        node_to_col: Column name for line ending node in line_model_df\n\n    Raises:\n        ValueError: If required columns are missing from DataFrames\n    \"\"\"\n    super().__init__(area_column)\n    self.area_border_model_df = area_border_model_df\n    self.line_model_df = line_model_df\n    self.node_model_df = node_model_df\n    self.area_column = area_column\n    self.node_from_col = node_from_col\n    self.node_to_col = node_to_col\n    self.node_to_area_map = self._create_node_to_area_map()\n    self._validate_inputs()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mescal.energy_data_handling.area_accounting.border_variable_base.AreaBorderVariableCalculatorBase.get_border_lines_in_topological_up_and_down_direction","title":"get_border_lines_in_topological_up_and_down_direction","text":"<pre><code>get_border_lines_in_topological_up_and_down_direction(border_id: str) -&gt; tuple[list[Hashable], list[Hashable]]\n</code></pre> <p>Get transmission lines for a border classified by topological direction.</p> <p>This method identifies which transmission lines connect the two areas of a border and classifies them based on their topological direction relative to the border.</p> <p>Border directionality logic: - \"Up\" direction: Lines where node_from is in area_from and node_to is in area_to - \"Down\" direction: Lines where node_from is in area_to and node_to is in area_from</p> <p>This classification is essential for correctly aggregating directional quantities like power flows, where the sign and direction matter for market analysis.</p> <p>Parameters:</p> Name Type Description Default <code>border_id</code> <code>str</code> <p>Border identifier (e.g., 'DE-FR') that will be decomposed into area_from and area_to using the naming convention.</p> required <p>Returns:</p> Type Description <code>list[Hashable]</code> <p>Tuple containing two lists:</p> <code>list[Hashable]</code> <ul> <li>lines_up: Line IDs for lines in the \"up\" direction</li> </ul> <code>tuple[list[Hashable], list[Hashable]]</code> <ul> <li>lines_down: Line IDs for lines in the \"down\" direction</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; # For border 'DE-FR'\n&gt;&gt;&gt; lines_up, lines_down = calculator.get_border_lines_in_topological_up_and_down_direction('DE-FR')\n&gt;&gt;&gt; # lines_up: Lines from German nodes to French nodes  \n&gt;&gt;&gt; # lines_down: Lines from French nodes to German nodes\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_variable_base.py</code> <pre><code>def get_border_lines_in_topological_up_and_down_direction(self, border_id: str) -&gt; tuple[list[Hashable], list[Hashable]]:\n    \"\"\"Get transmission lines for a border classified by topological direction.\n\n    This method identifies which transmission lines connect the two areas of a border\n    and classifies them based on their topological direction relative to the border.\n\n    Border directionality logic:\n    - \"Up\" direction: Lines where node_from is in area_from and node_to is in area_to\n    - \"Down\" direction: Lines where node_from is in area_to and node_to is in area_from\n\n    This classification is essential for correctly aggregating directional quantities\n    like power flows, where the sign and direction matter for market analysis.\n\n    Args:\n        border_id: Border identifier (e.g., 'DE-FR') that will be decomposed\n            into area_from and area_to using the naming convention.\n\n    Returns:\n        Tuple containing two lists:\n        - lines_up: Line IDs for lines in the \"up\" direction\n        - lines_down: Line IDs for lines in the \"down\" direction\n\n    Example:\n\n        &gt;&gt;&gt; # For border 'DE-FR'\n        &gt;&gt;&gt; lines_up, lines_down = calculator.get_border_lines_in_topological_up_and_down_direction('DE-FR')\n        &gt;&gt;&gt; # lines_up: Lines from German nodes to French nodes  \n        &gt;&gt;&gt; # lines_down: Lines from French nodes to German nodes\n    \"\"\"\n    area_from, area_to = self.decompose_area_border_name_to_areas(border_id)\n    nodes_in_area_from = self.node_model_df.loc[self.node_model_df[self.area_column] == area_from].index.to_list()\n    nodes_in_area_to = self.node_model_df.loc[self.node_model_df[self.area_column] == area_to].index.to_list()\n    lines_up = self.line_model_df.loc[\n            self.line_model_df[self.node_from_col].isin(nodes_in_area_from)\n            &amp; self.line_model_df[self.node_to_col].isin(nodes_in_area_to)\n        ].index.to_list()\n    lines_down = self.line_model_df.loc[\n            self.line_model_df[self.node_from_col].isin(nodes_in_area_to)\n            &amp; self.line_model_df[self.node_to_col].isin(nodes_in_area_from)\n        ].index.to_list()\n    return lines_up, lines_down\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mescal.energy_data_handling.area_accounting.border_variable_base.AreaBorderVariableCalculatorBase.calculate","title":"calculate  <code>abstractmethod</code>","text":"<pre><code>calculate(**kwargs) -&gt; DataFrame\n</code></pre> <p>Calculate the border variable. Must be implemented by subclasses.</p> <p>This method should contain the specific logic for aggregating line-level data to border level for the particular variable type. The implementation will vary based on the variable (flows, capacities, prices, etc.) and should handle directional aggregation appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Variable-specific parameters for the calculation</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with border-level aggregated data. Index should be datetime</p> <code>DataFrame</code> <p>for time series data, columns should be border identifiers.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This is an abstract method</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_variable_base.py</code> <pre><code>@abstractmethod\ndef calculate(self, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Calculate the border variable. Must be implemented by subclasses.\n\n    This method should contain the specific logic for aggregating line-level\n    data to border level for the particular variable type. The implementation\n    will vary based on the variable (flows, capacities, prices, etc.) and\n    should handle directional aggregation appropriately.\n\n    Args:\n        **kwargs: Variable-specific parameters for the calculation\n\n    Returns:\n        DataFrame with border-level aggregated data. Index should be datetime\n        for time series data, columns should be border identifiers.\n\n    Raises:\n        NotImplementedError: This is an abstract method\n    \"\"\"\n    pass\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mescal.energy_data_handling.area_accounting.border_variable_capacity_calculator.BorderCapacityCalculator","title":"BorderCapacityCalculator","text":"<p>               Bases: <code>AreaBorderVariableCalculatorBase</code></p> <p>Calculates aggregated transmission capacities for area borders.</p> <p>This calculator aggregates line-level transmission capacities to border level, handling bidirectional capacity data with proper directional aggregation.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from mescal.energy_data_handling.network_lines_data import NetworkLineCapacitiesData\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Create capacity data\n&gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n&gt;&gt;&gt; capacities = NetworkLineCapacitiesData(\n...     capacities_up=pd.DataFrame({...}),\n...     capacities_down=pd.DataFrame({...})\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; calculator = BorderCapacityCalculator(\n...     area_border_model_df, line_model_df, node_model_df, 'country'\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Calculate capacities for up direction (area_from \u2192 area_to)\n&gt;&gt;&gt; up_capacities = calculator.calculate(capacities, direction='up')\n&gt;&gt;&gt; print(up_capacities)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_variable_capacity_calculator.py</code> <pre><code>class BorderCapacityCalculator(AreaBorderVariableCalculatorBase):\n    \"\"\"Calculates aggregated transmission capacities for area borders.\n\n    This calculator aggregates line-level transmission capacities to border level,\n    handling bidirectional capacity data with proper directional aggregation.\n\n    Example:\n\n        &gt;&gt;&gt; from mescal.energy_data_handling.network_lines_data import NetworkLineCapacitiesData\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Create capacity data\n        &gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n        &gt;&gt;&gt; capacities = NetworkLineCapacitiesData(\n        ...     capacities_up=pd.DataFrame({...}),\n        ...     capacities_down=pd.DataFrame({...})\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; calculator = BorderCapacityCalculator(\n        ...     area_border_model_df, line_model_df, node_model_df, 'country'\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Calculate capacities for up direction (area_from \u2192 area_to)\n        &gt;&gt;&gt; up_capacities = calculator.calculate(capacities, direction='up')\n        &gt;&gt;&gt; print(up_capacities)\n    \"\"\"\n\n    @property\n    def variable_name(self) -&gt; str:\n        return \"border_capacity\"\n\n    def calculate(\n            self,\n            line_capacity_data: NetworkLineCapacitiesData,\n            direction: Literal['up', 'down'] = 'up'\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregate line-level transmission capacities to border level.\n\n        Sums transmission capacities of all lines belonging to each border,\n        respecting the specified direction and handling bidirectional capacity data.\n        Lines are aggregated based on their topological relationship to the border.\n\n        Direction logic:\n        - 'up': Capacities for flows from area_from to area_to\n        - 'down': Capacities for flows from area_to to area_from\n\n        For each border, the method:\n        1. Identifies lines in 'up' and 'down' topological directions\n        2. Selects appropriate capacity data based on requested direction\n        3. Sums capacities across all border lines\n        4. Handles missing data by excluding unavailable lines\n\n        Args:\n            line_capacity_data: NetworkLineCapacitiesData containing bidirectional\n                capacity time series. Must include capacities_up and capacities_down\n                DataFrames with line IDs as columns and timestamps as index.\n            direction: Direction for capacity aggregation:\n                - 'up': Sum capacities for area_from \u2192 area_to flows\n                - 'down': Sum capacities for area_to \u2192 area_from flows\n\n        Returns:\n            DataFrame with border-level capacity aggregations. Index matches the \n            input capacity data, columns are border identifiers. Values represent\n            total transmission capacity in MW for each border and timestamp.\n\n        Raises:\n            ValueError: If direction is not 'up' or 'down'\n\n        Example:\n\n            &gt;&gt;&gt; # Calculate up-direction capacities (exports from area_from)\n            &gt;&gt;&gt; up_caps = calculator.calculate(capacity_data, direction='up')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Calculate down-direction capacities (imports to area_from)  \n            &gt;&gt;&gt; down_caps = calculator.calculate(capacity_data, direction='down')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; print(f\"DE\u2192FR capacity: {up_caps.loc['2024-01-01 12:00', 'DE-FR']:.0f} MW\")\n        \"\"\"\n        self._validate_time_series_data(line_capacity_data.capacities_up, \"capacities_up\")\n        self._validate_time_series_data(line_capacity_data.capacities_down, \"capacities_down\")\n\n        border_capacities = {}\n\n        for border_id, border in self.area_border_model_df.iterrows():\n            lines_up, lines_down = self.get_border_lines_in_topological_up_and_down_direction(border_id)\n\n            if not lines_up and not lines_down:\n                # No lines found for this border - create empty series\n                index = line_capacity_data.capacities_up.index\n                border_capacities[border_id] = pd.Series(index=index, dtype=float)\n                continue\n\n            if direction == 'up':\n                # For 'up' direction: use up capacities of lines_up + down capacities of lines_down\n                capacity_parts = []\n                if lines_up:\n                    available_lines_up = [line for line in lines_up if line in line_capacity_data.capacities_up.columns]\n                    if available_lines_up:\n                        capacity_parts.append(line_capacity_data.capacities_up[available_lines_up])\n\n                if lines_down:\n                    available_lines_down = [line for line in lines_down if line in line_capacity_data.capacities_down.columns]\n                    if available_lines_down:\n                        capacity_parts.append(line_capacity_data.capacities_down[available_lines_down])\n\n            elif direction == 'down':\n                # For 'down' direction: use down capacities of lines_up + up capacities of lines_down\n                capacity_parts = []\n                if lines_up:\n                    available_lines_up = [line for line in lines_up if line in line_capacity_data.capacities_down.columns]\n                    if available_lines_up:\n                        capacity_parts.append(line_capacity_data.capacities_down[available_lines_up])\n\n                if lines_down:\n                    available_lines_down = [line for line in lines_down if line in line_capacity_data.capacities_up.columns]\n                    if available_lines_down:\n                        capacity_parts.append(line_capacity_data.capacities_up[available_lines_down])\n            else:\n                raise ValueError(f\"Unknown capacity direction: {direction}. Must be 'up' or 'down'\")\n\n            # Combine and sum capacities\n            if capacity_parts:\n                all_capacities = pd.concat(capacity_parts, axis=1)\n                border_capacities[border_id] = all_capacities.sum(axis=1)\n            else:\n                # No capacity data available for any lines\n                index = line_capacity_data.capacities_up.index\n                border_capacities[border_id] = pd.Series(index=index, dtype=float)\n\n        result = pd.DataFrame(border_capacities)\n        result.columns.name = self.border_identifier\n        return result\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mescal.energy_data_handling.area_accounting.border_variable_capacity_calculator.BorderCapacityCalculator.calculate","title":"calculate","text":"<pre><code>calculate(line_capacity_data: NetworkLineCapacitiesData, direction: Literal['up', 'down'] = 'up') -&gt; DataFrame\n</code></pre> <p>Aggregate line-level transmission capacities to border level.</p> <p>Sums transmission capacities of all lines belonging to each border, respecting the specified direction and handling bidirectional capacity data. Lines are aggregated based on their topological relationship to the border.</p> <p>Direction logic: - 'up': Capacities for flows from area_from to area_to - 'down': Capacities for flows from area_to to area_from</p> <p>For each border, the method: 1. Identifies lines in 'up' and 'down' topological directions 2. Selects appropriate capacity data based on requested direction 3. Sums capacities across all border lines 4. Handles missing data by excluding unavailable lines</p> <p>Parameters:</p> Name Type Description Default <code>line_capacity_data</code> <code>NetworkLineCapacitiesData</code> <p>NetworkLineCapacitiesData containing bidirectional capacity time series. Must include capacities_up and capacities_down DataFrames with line IDs as columns and timestamps as index.</p> required <code>direction</code> <code>Literal['up', 'down']</code> <p>Direction for capacity aggregation: - 'up': Sum capacities for area_from \u2192 area_to flows - 'down': Sum capacities for area_to \u2192 area_from flows</p> <code>'up'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with border-level capacity aggregations. Index matches the </p> <code>DataFrame</code> <p>input capacity data, columns are border identifiers. Values represent</p> <code>DataFrame</code> <p>total transmission capacity in MW for each border and timestamp.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If direction is not 'up' or 'down'</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Calculate up-direction capacities (exports from area_from)\n&gt;&gt;&gt; up_caps = calculator.calculate(capacity_data, direction='up')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Calculate down-direction capacities (imports to area_from)  \n&gt;&gt;&gt; down_caps = calculator.calculate(capacity_data, direction='down')\n&gt;&gt;&gt; \n&gt;&gt;&gt; print(f\"DE\u2192FR capacity: {up_caps.loc['2024-01-01 12:00', 'DE-FR']:.0f} MW\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_variable_capacity_calculator.py</code> <pre><code>def calculate(\n        self,\n        line_capacity_data: NetworkLineCapacitiesData,\n        direction: Literal['up', 'down'] = 'up'\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregate line-level transmission capacities to border level.\n\n    Sums transmission capacities of all lines belonging to each border,\n    respecting the specified direction and handling bidirectional capacity data.\n    Lines are aggregated based on their topological relationship to the border.\n\n    Direction logic:\n    - 'up': Capacities for flows from area_from to area_to\n    - 'down': Capacities for flows from area_to to area_from\n\n    For each border, the method:\n    1. Identifies lines in 'up' and 'down' topological directions\n    2. Selects appropriate capacity data based on requested direction\n    3. Sums capacities across all border lines\n    4. Handles missing data by excluding unavailable lines\n\n    Args:\n        line_capacity_data: NetworkLineCapacitiesData containing bidirectional\n            capacity time series. Must include capacities_up and capacities_down\n            DataFrames with line IDs as columns and timestamps as index.\n        direction: Direction for capacity aggregation:\n            - 'up': Sum capacities for area_from \u2192 area_to flows\n            - 'down': Sum capacities for area_to \u2192 area_from flows\n\n    Returns:\n        DataFrame with border-level capacity aggregations. Index matches the \n        input capacity data, columns are border identifiers. Values represent\n        total transmission capacity in MW for each border and timestamp.\n\n    Raises:\n        ValueError: If direction is not 'up' or 'down'\n\n    Example:\n\n        &gt;&gt;&gt; # Calculate up-direction capacities (exports from area_from)\n        &gt;&gt;&gt; up_caps = calculator.calculate(capacity_data, direction='up')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Calculate down-direction capacities (imports to area_from)  \n        &gt;&gt;&gt; down_caps = calculator.calculate(capacity_data, direction='down')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; print(f\"DE\u2192FR capacity: {up_caps.loc['2024-01-01 12:00', 'DE-FR']:.0f} MW\")\n    \"\"\"\n    self._validate_time_series_data(line_capacity_data.capacities_up, \"capacities_up\")\n    self._validate_time_series_data(line_capacity_data.capacities_down, \"capacities_down\")\n\n    border_capacities = {}\n\n    for border_id, border in self.area_border_model_df.iterrows():\n        lines_up, lines_down = self.get_border_lines_in_topological_up_and_down_direction(border_id)\n\n        if not lines_up and not lines_down:\n            # No lines found for this border - create empty series\n            index = line_capacity_data.capacities_up.index\n            border_capacities[border_id] = pd.Series(index=index, dtype=float)\n            continue\n\n        if direction == 'up':\n            # For 'up' direction: use up capacities of lines_up + down capacities of lines_down\n            capacity_parts = []\n            if lines_up:\n                available_lines_up = [line for line in lines_up if line in line_capacity_data.capacities_up.columns]\n                if available_lines_up:\n                    capacity_parts.append(line_capacity_data.capacities_up[available_lines_up])\n\n            if lines_down:\n                available_lines_down = [line for line in lines_down if line in line_capacity_data.capacities_down.columns]\n                if available_lines_down:\n                    capacity_parts.append(line_capacity_data.capacities_down[available_lines_down])\n\n        elif direction == 'down':\n            # For 'down' direction: use down capacities of lines_up + up capacities of lines_down\n            capacity_parts = []\n            if lines_up:\n                available_lines_up = [line for line in lines_up if line in line_capacity_data.capacities_down.columns]\n                if available_lines_up:\n                    capacity_parts.append(line_capacity_data.capacities_down[available_lines_up])\n\n            if lines_down:\n                available_lines_down = [line for line in lines_down if line in line_capacity_data.capacities_up.columns]\n                if available_lines_down:\n                    capacity_parts.append(line_capacity_data.capacities_up[available_lines_down])\n        else:\n            raise ValueError(f\"Unknown capacity direction: {direction}. Must be 'up' or 'down'\")\n\n        # Combine and sum capacities\n        if capacity_parts:\n            all_capacities = pd.concat(capacity_parts, axis=1)\n            border_capacities[border_id] = all_capacities.sum(axis=1)\n        else:\n            # No capacity data available for any lines\n            index = line_capacity_data.capacities_up.index\n            border_capacities[border_id] = pd.Series(index=index, dtype=float)\n\n    result = pd.DataFrame(border_capacities)\n    result.columns.name = self.border_identifier\n    return result\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mescal.energy_data_handling.area_accounting.border_variable_flow_calculator.BorderFlowCalculator","title":"BorderFlowCalculator","text":"<p>               Bases: <code>AreaBorderVariableCalculatorBase</code></p> <p>Calculates aggregated power flows for area borders.</p> <p>This calculator aggregates line-level power flows to border level, handling bidirectional flow data and transmission losses. The calculator can aggregate both sent and received flows, accounting for transmission losses that occur between sending and receiving ends. It supports multiple output formats including directional flows and net flows.</p> <p>Flow aggregation logic: - Lines and flows are classified as \"up\" or \"down\" based on topological direction - Flows are aggregated respecting directionality and loss conventions - Net flows represent the algebraic sum (up_flow - down_flow)</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from mescal.energy_data_handling.network_lines_data import NetworkLineFlowsData\n&gt;&gt;&gt; calculator = BorderFlowCalculator(\n...     area_border_model_df, line_model_df, node_model_df, 'country'\n... )\n&gt;&gt;&gt; # Calculate net sent flows (before losses)\n&gt;&gt;&gt; net_flows = calculator.calculate(flow_data, flow_type='sent', direction='net')\n&gt;&gt;&gt; print(net_flows)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_variable_flow_calculator.py</code> <pre><code>class BorderFlowCalculator(AreaBorderVariableCalculatorBase):\n    \"\"\"Calculates aggregated power flows for area borders.\n\n    This calculator aggregates line-level power flows to border level, handling\n    bidirectional flow data and transmission losses.\n    The calculator can aggregate both sent and received flows, accounting for\n    transmission losses that occur between sending and receiving ends. It supports\n    multiple output formats including directional flows and net flows.\n\n    Flow aggregation logic:\n    - Lines and flows are classified as \"up\" or \"down\" based on topological direction\n    - Flows are aggregated respecting directionality and loss conventions\n    - Net flows represent the algebraic sum (up_flow - down_flow)\n\n    Example:\n\n        &gt;&gt;&gt; from mescal.energy_data_handling.network_lines_data import NetworkLineFlowsData\n        &gt;&gt;&gt; calculator = BorderFlowCalculator(\n        ...     area_border_model_df, line_model_df, node_model_df, 'country'\n        ... )\n        &gt;&gt;&gt; # Calculate net sent flows (before losses)\n        &gt;&gt;&gt; net_flows = calculator.calculate(flow_data, flow_type='sent', direction='net')\n        &gt;&gt;&gt; print(net_flows)\n    \"\"\"\n\n    @property\n    def variable_name(self) -&gt; str:\n        return \"border_flow\"\n\n    def calculate(\n        self,\n        line_flow_data: NetworkLineFlowsData,\n        flow_type: Literal['sent', 'received'] = 'sent',\n        direction: Literal['up', 'down', 'net'] = 'net'\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregate line-level power flows to border level.\n\n        Sums power flows of all lines belonging to each border, respecting flow\n        directionality and transmission loss conventions. The aggregation handles\n        both pre-loss (sent) and post-loss (received) flows.\n\n        Flow type selection:\n        - 'sent': Flows before transmission losses (injected into lines)  \n        - 'received': Flows after transmission losses (withdrawn from lines)\n\n        Direction options:\n        - 'up': Flows from area_from to area_to only\n        - 'down': Flows from area_to to area_from only\n        - 'net': Net flows (up - down), positive means net export from area_from\n\n        The method handles missing data by preserving NaN values when all \n        constituent flows are missing for a given timestamp.\n\n        Args:\n            line_flow_data: NetworkLineFlowsData containing bidirectional flow\n                time series. Must include sent_up, received_up, sent_down, and\n                received_down DataFrames with line IDs as columns.\n            flow_type: Type of flows to aggregate:\n                - 'sent': Pre-loss flows (power injected into transmission)\n                - 'received': Post-loss flows (power withdrawn after losses)\n            direction: Flow direction to calculate:\n                - 'up': Flows from area_from \u2192 area_to\n                - 'down': Flows from area_to \u2192 area_from  \n                - 'net': Net flows (up - down)\n\n        Returns:\n            DataFrame with border-level flow aggregations. Index matches input\n            flow data, columns are border identifiers. Values represent power\n            flows in MW. For net flows, positive values indicate net export\n            from area_from to area_to.\n\n        Raises:\n            ValueError: If flow_type not in ['sent', 'received'] or direction\n                not in ['up', 'down', 'net']\n\n        Example:\n\n            &gt;&gt;&gt; # Calculate net sent flows (most common use case)\n            &gt;&gt;&gt; net_sent = calculator.calculate(flows, 'sent', 'net')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Calculate received flows in up direction only\n            &gt;&gt;&gt; up_received = calculator.calculate(flows, 'received', 'up')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; print(f\"DE\u2192FR net flow: {net_sent.loc['2024-01-01 12:00', 'DE-FR']:.0f} MW\")\n        \"\"\"\n        # Validate inputs\n        if flow_type not in ['sent', 'received']:\n            raise ValueError(f\"Unknown flow_type: {flow_type}. Must be 'sent' or 'received'\")\n        if direction not in ['up', 'down', 'net']:\n            raise ValueError(f\"Unknown flow direction: {direction}. Must be 'up', 'down', or 'net'\")\n\n        self._validate_time_series_data(line_flow_data.sent_up, \"sent_up\")\n        self._validate_time_series_data(line_flow_data.received_up, \"received_up\")\n\n        border_flows = {}\n\n        for border_id, border in self.area_border_model_df.iterrows():\n            lines_up, lines_down = self.get_border_lines_in_topological_up_and_down_direction(border_id)\n\n            if not lines_up and not lines_down:\n                # No lines for this border - create empty series\n                index = line_flow_data.sent_up.index\n                border_flows[border_id] = pd.Series(index=index, dtype=float)\n                continue\n\n            # Select appropriate flow data based on flow_type\n            if flow_type == 'sent':\n                flow_data_up = line_flow_data.sent_up\n                flow_data_down = line_flow_data.sent_down\n            else:  # flow_type == 'received'\n                flow_data_up = line_flow_data.received_up  \n                flow_data_down = line_flow_data.received_down\n\n            # Aggregate flows by direction relative to border\n            flow_parts_up = []\n            flow_parts_down = []\n\n            if lines_up:\n                # Lines in topological \"up\" direction\n                available_lines_up = [line for line in lines_up if line in flow_data_up.columns]\n                if available_lines_up:\n                    flow_parts_up.append(flow_data_up[available_lines_up])\n\n            if lines_down:  \n                # Lines in topological \"down\" direction contribute to opposite border flow\n                available_lines_down = [line for line in lines_down if line in flow_data_down.columns]\n                if available_lines_down:\n                    flow_parts_up.append(flow_data_down[available_lines_down])\n\n            if lines_down:\n                # Lines in topological \"down\" direction  \n                available_lines_down = [line for line in lines_down if line in flow_data_up.columns]\n                if available_lines_down:\n                    flow_parts_down.append(flow_data_up[available_lines_down])\n\n            if lines_up:\n                # Lines in topological \"up\" direction contribute to opposite border flow\n                available_lines_up = [line for line in lines_up if line in flow_data_down.columns]\n                if available_lines_up:\n                    flow_parts_down.append(flow_data_down[available_lines_up])\n\n            # Sum flows for each direction\n            if flow_parts_up:\n                flows_up_combined = pd.concat(flow_parts_up, axis=1)\n                flow_up = flows_up_combined.sum(axis=1)\n                flow_up[flows_up_combined.isna().all(axis=1)] = np.nan\n            else:\n                flow_up = pd.Series(index=line_flow_data.sent_up.index, dtype=float)\n\n            if flow_parts_down:\n                flows_down_combined = pd.concat(flow_parts_down, axis=1)  \n                flow_down = flows_down_combined.sum(axis=1)\n                flow_down[flows_down_combined.isna().all(axis=1)] = np.nan\n            else:\n                flow_down = pd.Series(index=line_flow_data.sent_up.index, dtype=float)\n\n            # Select final output based on direction parameter\n            if direction == 'up':\n                border_flows[border_id] = flow_up\n            elif direction == 'down':\n                border_flows[border_id] = flow_down\n            else:  # direction == 'net'\n                flow_net = flow_up.subtract(flow_down, fill_value=0)\n                # Preserve NaN when both directions are NaN\n                flow_net[flow_up.isna() &amp; flow_down.isna()] = np.nan\n                border_flows[border_id] = flow_net\n\n        result = pd.DataFrame(border_flows)\n        result.columns.name = self.border_identifier\n        return result\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mescal.energy_data_handling.area_accounting.border_variable_flow_calculator.BorderFlowCalculator.calculate","title":"calculate","text":"<pre><code>calculate(line_flow_data: NetworkLineFlowsData, flow_type: Literal['sent', 'received'] = 'sent', direction: Literal['up', 'down', 'net'] = 'net') -&gt; DataFrame\n</code></pre> <p>Aggregate line-level power flows to border level.</p> <p>Sums power flows of all lines belonging to each border, respecting flow directionality and transmission loss conventions. The aggregation handles both pre-loss (sent) and post-loss (received) flows.</p> <p>Flow type selection: - 'sent': Flows before transmission losses (injected into lines) - 'received': Flows after transmission losses (withdrawn from lines)</p> <p>Direction options: - 'up': Flows from area_from to area_to only - 'down': Flows from area_to to area_from only - 'net': Net flows (up - down), positive means net export from area_from</p> <p>The method handles missing data by preserving NaN values when all  constituent flows are missing for a given timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>line_flow_data</code> <code>NetworkLineFlowsData</code> <p>NetworkLineFlowsData containing bidirectional flow time series. Must include sent_up, received_up, sent_down, and received_down DataFrames with line IDs as columns.</p> required <code>flow_type</code> <code>Literal['sent', 'received']</code> <p>Type of flows to aggregate: - 'sent': Pre-loss flows (power injected into transmission) - 'received': Post-loss flows (power withdrawn after losses)</p> <code>'sent'</code> <code>direction</code> <code>Literal['up', 'down', 'net']</code> <p>Flow direction to calculate: - 'up': Flows from area_from \u2192 area_to - 'down': Flows from area_to \u2192 area_from - 'net': Net flows (up - down)</p> <code>'net'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with border-level flow aggregations. Index matches input</p> <code>DataFrame</code> <p>flow data, columns are border identifiers. Values represent power</p> <code>DataFrame</code> <p>flows in MW. For net flows, positive values indicate net export</p> <code>DataFrame</code> <p>from area_from to area_to.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If flow_type not in ['sent', 'received'] or direction not in ['up', 'down', 'net']</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Calculate net sent flows (most common use case)\n&gt;&gt;&gt; net_sent = calculator.calculate(flows, 'sent', 'net')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Calculate received flows in up direction only\n&gt;&gt;&gt; up_received = calculator.calculate(flows, 'received', 'up')\n&gt;&gt;&gt; \n&gt;&gt;&gt; print(f\"DE\u2192FR net flow: {net_sent.loc['2024-01-01 12:00', 'DE-FR']:.0f} MW\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_variable_flow_calculator.py</code> <pre><code>def calculate(\n    self,\n    line_flow_data: NetworkLineFlowsData,\n    flow_type: Literal['sent', 'received'] = 'sent',\n    direction: Literal['up', 'down', 'net'] = 'net'\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregate line-level power flows to border level.\n\n    Sums power flows of all lines belonging to each border, respecting flow\n    directionality and transmission loss conventions. The aggregation handles\n    both pre-loss (sent) and post-loss (received) flows.\n\n    Flow type selection:\n    - 'sent': Flows before transmission losses (injected into lines)  \n    - 'received': Flows after transmission losses (withdrawn from lines)\n\n    Direction options:\n    - 'up': Flows from area_from to area_to only\n    - 'down': Flows from area_to to area_from only\n    - 'net': Net flows (up - down), positive means net export from area_from\n\n    The method handles missing data by preserving NaN values when all \n    constituent flows are missing for a given timestamp.\n\n    Args:\n        line_flow_data: NetworkLineFlowsData containing bidirectional flow\n            time series. Must include sent_up, received_up, sent_down, and\n            received_down DataFrames with line IDs as columns.\n        flow_type: Type of flows to aggregate:\n            - 'sent': Pre-loss flows (power injected into transmission)\n            - 'received': Post-loss flows (power withdrawn after losses)\n        direction: Flow direction to calculate:\n            - 'up': Flows from area_from \u2192 area_to\n            - 'down': Flows from area_to \u2192 area_from  \n            - 'net': Net flows (up - down)\n\n    Returns:\n        DataFrame with border-level flow aggregations. Index matches input\n        flow data, columns are border identifiers. Values represent power\n        flows in MW. For net flows, positive values indicate net export\n        from area_from to area_to.\n\n    Raises:\n        ValueError: If flow_type not in ['sent', 'received'] or direction\n            not in ['up', 'down', 'net']\n\n    Example:\n\n        &gt;&gt;&gt; # Calculate net sent flows (most common use case)\n        &gt;&gt;&gt; net_sent = calculator.calculate(flows, 'sent', 'net')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Calculate received flows in up direction only\n        &gt;&gt;&gt; up_received = calculator.calculate(flows, 'received', 'up')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; print(f\"DE\u2192FR net flow: {net_sent.loc['2024-01-01 12:00', 'DE-FR']:.0f} MW\")\n    \"\"\"\n    # Validate inputs\n    if flow_type not in ['sent', 'received']:\n        raise ValueError(f\"Unknown flow_type: {flow_type}. Must be 'sent' or 'received'\")\n    if direction not in ['up', 'down', 'net']:\n        raise ValueError(f\"Unknown flow direction: {direction}. Must be 'up', 'down', or 'net'\")\n\n    self._validate_time_series_data(line_flow_data.sent_up, \"sent_up\")\n    self._validate_time_series_data(line_flow_data.received_up, \"received_up\")\n\n    border_flows = {}\n\n    for border_id, border in self.area_border_model_df.iterrows():\n        lines_up, lines_down = self.get_border_lines_in_topological_up_and_down_direction(border_id)\n\n        if not lines_up and not lines_down:\n            # No lines for this border - create empty series\n            index = line_flow_data.sent_up.index\n            border_flows[border_id] = pd.Series(index=index, dtype=float)\n            continue\n\n        # Select appropriate flow data based on flow_type\n        if flow_type == 'sent':\n            flow_data_up = line_flow_data.sent_up\n            flow_data_down = line_flow_data.sent_down\n        else:  # flow_type == 'received'\n            flow_data_up = line_flow_data.received_up  \n            flow_data_down = line_flow_data.received_down\n\n        # Aggregate flows by direction relative to border\n        flow_parts_up = []\n        flow_parts_down = []\n\n        if lines_up:\n            # Lines in topological \"up\" direction\n            available_lines_up = [line for line in lines_up if line in flow_data_up.columns]\n            if available_lines_up:\n                flow_parts_up.append(flow_data_up[available_lines_up])\n\n        if lines_down:  \n            # Lines in topological \"down\" direction contribute to opposite border flow\n            available_lines_down = [line for line in lines_down if line in flow_data_down.columns]\n            if available_lines_down:\n                flow_parts_up.append(flow_data_down[available_lines_down])\n\n        if lines_down:\n            # Lines in topological \"down\" direction  \n            available_lines_down = [line for line in lines_down if line in flow_data_up.columns]\n            if available_lines_down:\n                flow_parts_down.append(flow_data_up[available_lines_down])\n\n        if lines_up:\n            # Lines in topological \"up\" direction contribute to opposite border flow\n            available_lines_up = [line for line in lines_up if line in flow_data_down.columns]\n            if available_lines_up:\n                flow_parts_down.append(flow_data_down[available_lines_up])\n\n        # Sum flows for each direction\n        if flow_parts_up:\n            flows_up_combined = pd.concat(flow_parts_up, axis=1)\n            flow_up = flows_up_combined.sum(axis=1)\n            flow_up[flows_up_combined.isna().all(axis=1)] = np.nan\n        else:\n            flow_up = pd.Series(index=line_flow_data.sent_up.index, dtype=float)\n\n        if flow_parts_down:\n            flows_down_combined = pd.concat(flow_parts_down, axis=1)  \n            flow_down = flows_down_combined.sum(axis=1)\n            flow_down[flows_down_combined.isna().all(axis=1)] = np.nan\n        else:\n            flow_down = pd.Series(index=line_flow_data.sent_up.index, dtype=float)\n\n        # Select final output based on direction parameter\n        if direction == 'up':\n            border_flows[border_id] = flow_up\n        elif direction == 'down':\n            border_flows[border_id] = flow_down\n        else:  # direction == 'net'\n            flow_net = flow_up.subtract(flow_down, fill_value=0)\n            # Preserve NaN when both directions are NaN\n            flow_net[flow_up.isna() &amp; flow_down.isna()] = np.nan\n            border_flows[border_id] = flow_net\n\n    result = pd.DataFrame(border_flows)\n    result.columns.name = self.border_identifier\n    return result\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mescal.energy_data_handling.area_accounting.border_variable_price_spread_calculator.BorderPriceSpreadCalculator","title":"BorderPriceSpreadCalculator","text":"<p>               Bases: <code>AreaBorderVariableCalculatorBase</code></p> <p>Calculates electricity price spreads between areas for each border.</p> <p>This calculator computes price differences between connected areas. Price spreads are fundamental indicators in electricity markets for: - Market integration analysis (zero spreads indicate perfect coupling) - Congestion identification (non-zero spreads suggest transmission constraints) - Arbitrage opportunity assessment (price differences drive trading incentives) - Market efficiency evaluation (persistent spreads may indicate inefficiencies) - Cross-border flow direction prediction (flows typically follow price gradients)</p> <p>The calculator supports multiple spread calculation methods (Spread Types): - 'raw': price_to - price_from (preserves direction and sign) - 'absolute': |price_to - price_from| (magnitude only) - 'directional_up': max(price_to - price_from, 0) (only positive spreads) - 'directional_down': max(price_from - price_to, 0) (only negative spreads as positive)</p> <p>Attributes:</p> Name Type Description <code>variable_name</code> <code>str</code> <p>Returns 'price_spread' for identification</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample area price data\n&gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n&gt;&gt;&gt; area_prices = pd.DataFrame({\n...     'DE': np.random.uniform(40, 80, 24),  # German prices\n...     'FR': np.random.uniform(35, 75, 24),  # French prices\n...     'BE': np.random.uniform(45, 85, 24)   # Belgian prices\n... }, index=time_index)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Set up border model and calculator (see base class docs for setup)\n&gt;&gt;&gt; calculator = BorderPriceSpreadCalculator(\n...     border_model_df, line_model_df, node_model_df, 'country'\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate raw price spreads\n&gt;&gt;&gt; raw_spreads = calculator.calculate(area_prices, spread_type='raw')\n&gt;&gt;&gt; print(f\"Average spread DE-FR: {raw_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate all spread types at once\n&gt;&gt;&gt; all_spreads = calculator.calculate_all_spread_types(area_prices)\n&gt;&gt;&gt; print(all_spreads.head())\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_variable_price_spread_calculator.py</code> <pre><code>class BorderPriceSpreadCalculator(AreaBorderVariableCalculatorBase):\n    \"\"\"Calculates electricity price spreads between areas for each border.\n\n    This calculator computes price differences between connected areas.\n    Price spreads are fundamental indicators in electricity markets for:\n    - Market integration analysis (zero spreads indicate perfect coupling)\n    - Congestion identification (non-zero spreads suggest transmission constraints)\n    - Arbitrage opportunity assessment (price differences drive trading incentives)\n    - Market efficiency evaluation (persistent spreads may indicate inefficiencies)\n    - Cross-border flow direction prediction (flows typically follow price gradients)\n\n    The calculator supports multiple spread calculation methods (Spread Types):\n    - 'raw': price_to - price_from (preserves direction and sign)\n    - 'absolute': |price_to - price_from| (magnitude only)\n    - 'directional_up': max(price_to - price_from, 0) (only positive spreads)\n    - 'directional_down': max(price_from - price_to, 0) (only negative spreads as positive)\n\n    Attributes:\n        variable_name (str): Returns 'price_spread' for identification\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample area price data\n        &gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n        &gt;&gt;&gt; area_prices = pd.DataFrame({\n        ...     'DE': np.random.uniform(40, 80, 24),  # German prices\n        ...     'FR': np.random.uniform(35, 75, 24),  # French prices\n        ...     'BE': np.random.uniform(45, 85, 24)   # Belgian prices\n        ... }, index=time_index)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Set up border model and calculator (see base class docs for setup)\n        &gt;&gt;&gt; calculator = BorderPriceSpreadCalculator(\n        ...     border_model_df, line_model_df, node_model_df, 'country'\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate raw price spreads\n        &gt;&gt;&gt; raw_spreads = calculator.calculate(area_prices, spread_type='raw')\n        &gt;&gt;&gt; print(f\"Average spread DE-FR: {raw_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate all spread types at once\n        &gt;&gt;&gt; all_spreads = calculator.calculate_all_spread_types(area_prices)\n        &gt;&gt;&gt; print(all_spreads.head())\n    \"\"\"\n\n    @property\n    def variable_name(self) -&gt; str:\n        return \"price_spread\"\n\n    def calculate(\n        self,\n        area_price_df: pd.DataFrame,\n        spread_type: Literal['raw', 'absolute', 'directional_up', 'directional_down'] = 'raw'\n    ) -&gt; pd.DataFrame:\n        \"\"\"Calculate electricity price spreads between connected market areas.\n\n        Computes price differences across transmission borders using the specified\n        calculation method. Price spreads are calculated as directional differences\n        based on the border naming convention (area_from \u2192 area_to).\n\n        The calculation handles missing area data gracefully by excluding borders\n        where either area lacks price data. This is common when analyzing subsets\n        of larger energy systems or when dealing with data availability issues.\n\n        Args:\n            area_price_df (pd.DataFrame): Time series of area-level electricity prices.\n                - Index: DateTime index for time series analysis\n                - Columns: Area identifiers matching border area names\n                - Values: Prices in consistent units (e.g., EUR/MWh, USD/MWh)\n                - Example shape: (8760 hours, N areas) for annual analysis\n\n            spread_type (Literal): Method for calculating price spreads.\n                - 'raw': Directional price differences (default, preserves sign)\n                - 'absolute': Magnitude of price differences (always non-negative)\n                - 'directional_up': Only spreads where price_to &gt; price_from\n                - 'directional_down': Only spreads where price_from &gt; price_to\n\n        Returns:\n            pd.DataFrame: Border-level price spreads with temporal dimension.\n                - Index: Same as input area_price_df (typically DatetimeIndex)\n                - Columns: Border identifiers (e.g., 'DE-FR', 'FR-BE')\n                - Column name: Set to self.border_identifier for consistency\n                - Values: Price spreads in same units as input prices\n                - Missing data: NaN where area price data is unavailable\n\n        Raises:\n            ValueError: If spread_type is not one of the supported options\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Create hourly price data for German and French markets\n            &gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n            &gt;&gt;&gt; prices = pd.DataFrame({\n            ...     'DE': [45.2, 43.1, 41.8, 39.5, 38.2, 42.1, 52.3, 65.4,\n            ...            72.1, 68.9, 64.2, 58.7, 55.1, 53.8, 56.2, 61.4,\n            ...            67.8, 74.2, 69.1, 64.3, 58.9, 52.1, 48.7, 46.3],\n            ...     'FR': [42.8, 41.2, 39.1, 37.8, 36.4, 40.3, 49.8, 62.1,\n            ...            68.9, 65.2, 61.4, 56.8, 53.2, 51.9, 54.1, 58.7,\n            ...            64.3, 70.8, 66.2, 61.1, 56.3, 49.8, 46.1, 43.9]\n            ... }, index=time_index)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Calculate raw spreads (FR - DE for DE-FR border)\n            &gt;&gt;&gt; raw_spreads = calculator.calculate(prices, 'raw')\n            &gt;&gt;&gt; print(f\"Average DE-FR spread: {raw_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n            &gt;&gt;&gt; # Output: Average DE-FR spread: -2.15 EUR/MWh (German prices higher)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Calculate absolute spreads for congestion analysis\n            &gt;&gt;&gt; abs_spreads = calculator.calculate(prices, 'absolute')\n            &gt;&gt;&gt; print(f\"Average absolute spread: {abs_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n            &gt;&gt;&gt; # Output: Average absolute spread: 2.15 EUR/MWh\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Analyze directional spreads for flow prediction\n            &gt;&gt;&gt; up_spreads = calculator.calculate(prices, 'directional_up')\n            &gt;&gt;&gt; down_spreads = calculator.calculate(prices, 'directional_down')\n            &gt;&gt;&gt; print(f\"Hours with FR &gt; DE prices: {(up_spreads['DE-FR'] &gt; 0).sum()}\")\n            &gt;&gt;&gt; print(f\"Hours with DE &gt; FR prices: {(down_spreads['DE-FR'] &gt; 0).sum()}\")\n        \"\"\"\n        self._validate_time_series_data(area_price_df, 'area_price_df')\n\n        spreads = {}\n\n        for border_id, border in self.area_border_model_df.iterrows():\n            area_from = border[self.source_area_identifier]\n            area_to = border[self.target_area_identifier]\n\n            if area_from in area_price_df.columns and area_to in area_price_df.columns:\n                price_from = area_price_df[area_from]\n                price_to = area_price_df[area_to]\n\n                raw_spread = price_to - price_from\n\n                if spread_type == 'raw':\n                    spreads[border_id] = raw_spread\n                elif spread_type == 'absolute':\n                    spreads[border_id] = raw_spread.abs()\n                elif spread_type == 'directional_up':\n                    spreads[border_id] = raw_spread.clip(lower=0)\n                elif spread_type == 'directional_down':\n                    spreads[border_id] = (-1 * raw_spread).clip(lower=0)\n                else:\n                    raise ValueError(f\"Unknown spread_type: {spread_type}\")\n\n        result = pd.DataFrame(spreads)\n        result.columns.name = self.border_identifier\n        return result\n\n    def calculate_all_spread_types(self, area_price_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Calculate all price spread types simultaneously for comprehensive analysis.\n\n        Returns a MultiIndex DataFrame with all four spread calculation methods\n        (raw, absolute, directional_up, directional_down) in a single DataFrame,\n        providing a complete view of price relationships across all borders.\n\n        Args:\n            area_price_df (pd.DataFrame): Time series of area-level electricity prices.\n                Same format as required by the calculate() method:\n                - Index: DateTime index for temporal analysis\n                - Columns: Area identifiers matching border definitions\n                - Values: Prices in consistent units (e.g., EUR/MWh, USD/MWh)\n\n        Returns:\n            pd.DataFrame: MultiIndex DataFrame with comprehensive spread analysis.\n                - Index: Same temporal index as input area_price_df\n                - Columns: MultiIndex with two levels:\n                    - Level 0 ('spread_type'): ['raw', 'absolute', 'directional_up', 'directional_down']\n                    - Level 1 (border_identifier): Border names (e.g., 'DE-FR', 'FR-BE')\n                - Values: Price spreads in same units as input prices\n                - Structure: (time_periods, spread_types \u00d7 borders)\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Create sample price data\n            &gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n            &gt;&gt;&gt; prices = pd.DataFrame({\n            ...     'DE': np.random.uniform(40, 80, 24),\n            ...     'FR': np.random.uniform(35, 75, 24),\n            ...     'BE': np.random.uniform(45, 85, 24)\n            ... }, index=time_index)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Calculate all spread types\n            &gt;&gt;&gt; all_spreads = calculator.calculate_all_spread_types(prices)\n            &gt;&gt;&gt; print(all_spreads.columns.names)\n            &gt;&gt;&gt; # Output: ['spread_type', 'country_border']\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Access specific spread types\n            &gt;&gt;&gt; raw_spreads = all_spreads['raw']\n            &gt;&gt;&gt; absolute_spreads = all_spreads['absolute']\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Analyze spread statistics by type\n            &gt;&gt;&gt; spread_stats = all_spreads.groupby(level='spread_type', axis=1).mean()\n            &gt;&gt;&gt; print(spread_stats)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Compare directional flows\n            &gt;&gt;&gt; up_flows = all_spreads['directional_up'].sum(axis=1)\n            &gt;&gt;&gt; down_flows = all_spreads['directional_down'].sum(axis=1)\n            &gt;&gt;&gt; net_spread_pressure = up_flows - down_flows\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Identify hours with high price volatility\n            &gt;&gt;&gt; high_volatility_hours = absolute_spreads.mean(axis=1) &gt; 10  # EUR/MWh threshold\n            &gt;&gt;&gt; print(f\"Hours with high spread volatility: {high_volatility_hours.sum()}\")\n        \"\"\"\n        results = {}\n        for spread_type in ['raw', 'absolute', 'directional_up', 'directional_down']:\n            results[spread_type] = self.calculate(area_price_df, spread_type)\n\n        return pd.concat(results, axis=1, names=['spread_type'])\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mescal.energy_data_handling.area_accounting.border_variable_price_spread_calculator.BorderPriceSpreadCalculator.calculate","title":"calculate","text":"<pre><code>calculate(area_price_df: DataFrame, spread_type: Literal['raw', 'absolute', 'directional_up', 'directional_down'] = 'raw') -&gt; DataFrame\n</code></pre> <p>Calculate electricity price spreads between connected market areas.</p> <p>Computes price differences across transmission borders using the specified calculation method. Price spreads are calculated as directional differences based on the border naming convention (area_from \u2192 area_to).</p> <p>The calculation handles missing area data gracefully by excluding borders where either area lacks price data. This is common when analyzing subsets of larger energy systems or when dealing with data availability issues.</p> <p>Parameters:</p> Name Type Description Default <code>area_price_df</code> <code>DataFrame</code> <p>Time series of area-level electricity prices. - Index: DateTime index for time series analysis - Columns: Area identifiers matching border area names - Values: Prices in consistent units (e.g., EUR/MWh, USD/MWh) - Example shape: (8760 hours, N areas) for annual analysis</p> required <code>spread_type</code> <code>Literal</code> <p>Method for calculating price spreads. - 'raw': Directional price differences (default, preserves sign) - 'absolute': Magnitude of price differences (always non-negative) - 'directional_up': Only spreads where price_to &gt; price_from - 'directional_down': Only spreads where price_from &gt; price_to</p> <code>'raw'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Border-level price spreads with temporal dimension. - Index: Same as input area_price_df (typically DatetimeIndex) - Columns: Border identifiers (e.g., 'DE-FR', 'FR-BE') - Column name: Set to self.border_identifier for consistency - Values: Price spreads in same units as input prices - Missing data: NaN where area price data is unavailable</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If spread_type is not one of the supported options</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create hourly price data for German and French markets\n&gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n&gt;&gt;&gt; prices = pd.DataFrame({\n...     'DE': [45.2, 43.1, 41.8, 39.5, 38.2, 42.1, 52.3, 65.4,\n...            72.1, 68.9, 64.2, 58.7, 55.1, 53.8, 56.2, 61.4,\n...            67.8, 74.2, 69.1, 64.3, 58.9, 52.1, 48.7, 46.3],\n...     'FR': [42.8, 41.2, 39.1, 37.8, 36.4, 40.3, 49.8, 62.1,\n...            68.9, 65.2, 61.4, 56.8, 53.2, 51.9, 54.1, 58.7,\n...            64.3, 70.8, 66.2, 61.1, 56.3, 49.8, 46.1, 43.9]\n... }, index=time_index)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate raw spreads (FR - DE for DE-FR border)\n&gt;&gt;&gt; raw_spreads = calculator.calculate(prices, 'raw')\n&gt;&gt;&gt; print(f\"Average DE-FR spread: {raw_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n&gt;&gt;&gt; # Output: Average DE-FR spread: -2.15 EUR/MWh (German prices higher)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate absolute spreads for congestion analysis\n&gt;&gt;&gt; abs_spreads = calculator.calculate(prices, 'absolute')\n&gt;&gt;&gt; print(f\"Average absolute spread: {abs_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n&gt;&gt;&gt; # Output: Average absolute spread: 2.15 EUR/MWh\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Analyze directional spreads for flow prediction\n&gt;&gt;&gt; up_spreads = calculator.calculate(prices, 'directional_up')\n&gt;&gt;&gt; down_spreads = calculator.calculate(prices, 'directional_down')\n&gt;&gt;&gt; print(f\"Hours with FR &gt; DE prices: {(up_spreads['DE-FR'] &gt; 0).sum()}\")\n&gt;&gt;&gt; print(f\"Hours with DE &gt; FR prices: {(down_spreads['DE-FR'] &gt; 0).sum()}\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_variable_price_spread_calculator.py</code> <pre><code>def calculate(\n    self,\n    area_price_df: pd.DataFrame,\n    spread_type: Literal['raw', 'absolute', 'directional_up', 'directional_down'] = 'raw'\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate electricity price spreads between connected market areas.\n\n    Computes price differences across transmission borders using the specified\n    calculation method. Price spreads are calculated as directional differences\n    based on the border naming convention (area_from \u2192 area_to).\n\n    The calculation handles missing area data gracefully by excluding borders\n    where either area lacks price data. This is common when analyzing subsets\n    of larger energy systems or when dealing with data availability issues.\n\n    Args:\n        area_price_df (pd.DataFrame): Time series of area-level electricity prices.\n            - Index: DateTime index for time series analysis\n            - Columns: Area identifiers matching border area names\n            - Values: Prices in consistent units (e.g., EUR/MWh, USD/MWh)\n            - Example shape: (8760 hours, N areas) for annual analysis\n\n        spread_type (Literal): Method for calculating price spreads.\n            - 'raw': Directional price differences (default, preserves sign)\n            - 'absolute': Magnitude of price differences (always non-negative)\n            - 'directional_up': Only spreads where price_to &gt; price_from\n            - 'directional_down': Only spreads where price_from &gt; price_to\n\n    Returns:\n        pd.DataFrame: Border-level price spreads with temporal dimension.\n            - Index: Same as input area_price_df (typically DatetimeIndex)\n            - Columns: Border identifiers (e.g., 'DE-FR', 'FR-BE')\n            - Column name: Set to self.border_identifier for consistency\n            - Values: Price spreads in same units as input prices\n            - Missing data: NaN where area price data is unavailable\n\n    Raises:\n        ValueError: If spread_type is not one of the supported options\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create hourly price data for German and French markets\n        &gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n        &gt;&gt;&gt; prices = pd.DataFrame({\n        ...     'DE': [45.2, 43.1, 41.8, 39.5, 38.2, 42.1, 52.3, 65.4,\n        ...            72.1, 68.9, 64.2, 58.7, 55.1, 53.8, 56.2, 61.4,\n        ...            67.8, 74.2, 69.1, 64.3, 58.9, 52.1, 48.7, 46.3],\n        ...     'FR': [42.8, 41.2, 39.1, 37.8, 36.4, 40.3, 49.8, 62.1,\n        ...            68.9, 65.2, 61.4, 56.8, 53.2, 51.9, 54.1, 58.7,\n        ...            64.3, 70.8, 66.2, 61.1, 56.3, 49.8, 46.1, 43.9]\n        ... }, index=time_index)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate raw spreads (FR - DE for DE-FR border)\n        &gt;&gt;&gt; raw_spreads = calculator.calculate(prices, 'raw')\n        &gt;&gt;&gt; print(f\"Average DE-FR spread: {raw_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n        &gt;&gt;&gt; # Output: Average DE-FR spread: -2.15 EUR/MWh (German prices higher)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate absolute spreads for congestion analysis\n        &gt;&gt;&gt; abs_spreads = calculator.calculate(prices, 'absolute')\n        &gt;&gt;&gt; print(f\"Average absolute spread: {abs_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n        &gt;&gt;&gt; # Output: Average absolute spread: 2.15 EUR/MWh\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Analyze directional spreads for flow prediction\n        &gt;&gt;&gt; up_spreads = calculator.calculate(prices, 'directional_up')\n        &gt;&gt;&gt; down_spreads = calculator.calculate(prices, 'directional_down')\n        &gt;&gt;&gt; print(f\"Hours with FR &gt; DE prices: {(up_spreads['DE-FR'] &gt; 0).sum()}\")\n        &gt;&gt;&gt; print(f\"Hours with DE &gt; FR prices: {(down_spreads['DE-FR'] &gt; 0).sum()}\")\n    \"\"\"\n    self._validate_time_series_data(area_price_df, 'area_price_df')\n\n    spreads = {}\n\n    for border_id, border in self.area_border_model_df.iterrows():\n        area_from = border[self.source_area_identifier]\n        area_to = border[self.target_area_identifier]\n\n        if area_from in area_price_df.columns and area_to in area_price_df.columns:\n            price_from = area_price_df[area_from]\n            price_to = area_price_df[area_to]\n\n            raw_spread = price_to - price_from\n\n            if spread_type == 'raw':\n                spreads[border_id] = raw_spread\n            elif spread_type == 'absolute':\n                spreads[border_id] = raw_spread.abs()\n            elif spread_type == 'directional_up':\n                spreads[border_id] = raw_spread.clip(lower=0)\n            elif spread_type == 'directional_down':\n                spreads[border_id] = (-1 * raw_spread).clip(lower=0)\n            else:\n                raise ValueError(f\"Unknown spread_type: {spread_type}\")\n\n    result = pd.DataFrame(spreads)\n    result.columns.name = self.border_identifier\n    return result\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mescal.energy_data_handling.area_accounting.border_variable_price_spread_calculator.BorderPriceSpreadCalculator.calculate_all_spread_types","title":"calculate_all_spread_types","text":"<pre><code>calculate_all_spread_types(area_price_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Calculate all price spread types simultaneously for comprehensive analysis.</p> <p>Returns a MultiIndex DataFrame with all four spread calculation methods (raw, absolute, directional_up, directional_down) in a single DataFrame, providing a complete view of price relationships across all borders.</p> <p>Parameters:</p> Name Type Description Default <code>area_price_df</code> <code>DataFrame</code> <p>Time series of area-level electricity prices. Same format as required by the calculate() method: - Index: DateTime index for temporal analysis - Columns: Area identifiers matching border definitions - Values: Prices in consistent units (e.g., EUR/MWh, USD/MWh)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: MultiIndex DataFrame with comprehensive spread analysis. - Index: Same temporal index as input area_price_df - Columns: MultiIndex with two levels:     - Level 0 ('spread_type'): ['raw', 'absolute', 'directional_up', 'directional_down']     - Level 1 (border_identifier): Border names (e.g., 'DE-FR', 'FR-BE') - Values: Price spreads in same units as input prices - Structure: (time_periods, spread_types \u00d7 borders)</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample price data\n&gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n&gt;&gt;&gt; prices = pd.DataFrame({\n...     'DE': np.random.uniform(40, 80, 24),\n...     'FR': np.random.uniform(35, 75, 24),\n...     'BE': np.random.uniform(45, 85, 24)\n... }, index=time_index)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate all spread types\n&gt;&gt;&gt; all_spreads = calculator.calculate_all_spread_types(prices)\n&gt;&gt;&gt; print(all_spreads.columns.names)\n&gt;&gt;&gt; # Output: ['spread_type', 'country_border']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Access specific spread types\n&gt;&gt;&gt; raw_spreads = all_spreads['raw']\n&gt;&gt;&gt; absolute_spreads = all_spreads['absolute']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Analyze spread statistics by type\n&gt;&gt;&gt; spread_stats = all_spreads.groupby(level='spread_type', axis=1).mean()\n&gt;&gt;&gt; print(spread_stats)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compare directional flows\n&gt;&gt;&gt; up_flows = all_spreads['directional_up'].sum(axis=1)\n&gt;&gt;&gt; down_flows = all_spreads['directional_down'].sum(axis=1)\n&gt;&gt;&gt; net_spread_pressure = up_flows - down_flows\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Identify hours with high price volatility\n&gt;&gt;&gt; high_volatility_hours = absolute_spreads.mean(axis=1) &gt; 10  # EUR/MWh threshold\n&gt;&gt;&gt; print(f\"Hours with high spread volatility: {high_volatility_hours.sum()}\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_variable_price_spread_calculator.py</code> <pre><code>def calculate_all_spread_types(self, area_price_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate all price spread types simultaneously for comprehensive analysis.\n\n    Returns a MultiIndex DataFrame with all four spread calculation methods\n    (raw, absolute, directional_up, directional_down) in a single DataFrame,\n    providing a complete view of price relationships across all borders.\n\n    Args:\n        area_price_df (pd.DataFrame): Time series of area-level electricity prices.\n            Same format as required by the calculate() method:\n            - Index: DateTime index for temporal analysis\n            - Columns: Area identifiers matching border definitions\n            - Values: Prices in consistent units (e.g., EUR/MWh, USD/MWh)\n\n    Returns:\n        pd.DataFrame: MultiIndex DataFrame with comprehensive spread analysis.\n            - Index: Same temporal index as input area_price_df\n            - Columns: MultiIndex with two levels:\n                - Level 0 ('spread_type'): ['raw', 'absolute', 'directional_up', 'directional_down']\n                - Level 1 (border_identifier): Border names (e.g., 'DE-FR', 'FR-BE')\n            - Values: Price spreads in same units as input prices\n            - Structure: (time_periods, spread_types \u00d7 borders)\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample price data\n        &gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n        &gt;&gt;&gt; prices = pd.DataFrame({\n        ...     'DE': np.random.uniform(40, 80, 24),\n        ...     'FR': np.random.uniform(35, 75, 24),\n        ...     'BE': np.random.uniform(45, 85, 24)\n        ... }, index=time_index)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate all spread types\n        &gt;&gt;&gt; all_spreads = calculator.calculate_all_spread_types(prices)\n        &gt;&gt;&gt; print(all_spreads.columns.names)\n        &gt;&gt;&gt; # Output: ['spread_type', 'country_border']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Access specific spread types\n        &gt;&gt;&gt; raw_spreads = all_spreads['raw']\n        &gt;&gt;&gt; absolute_spreads = all_spreads['absolute']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Analyze spread statistics by type\n        &gt;&gt;&gt; spread_stats = all_spreads.groupby(level='spread_type', axis=1).mean()\n        &gt;&gt;&gt; print(spread_stats)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compare directional flows\n        &gt;&gt;&gt; up_flows = all_spreads['directional_up'].sum(axis=1)\n        &gt;&gt;&gt; down_flows = all_spreads['directional_down'].sum(axis=1)\n        &gt;&gt;&gt; net_spread_pressure = up_flows - down_flows\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Identify hours with high price volatility\n        &gt;&gt;&gt; high_volatility_hours = absolute_spreads.mean(axis=1) &gt; 10  # EUR/MWh threshold\n        &gt;&gt;&gt; print(f\"Hours with high spread volatility: {high_volatility_hours.sum()}\")\n    \"\"\"\n    results = {}\n    for spread_type in ['raw', 'absolute', 'directional_up', 'directional_down']:\n        results[spread_type] = self.calculate(area_price_df, spread_type)\n\n    return pd.concat(results, axis=1, names=['spread_type'])\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/","title":"Area and AreaBorder Model-DF Generators","text":"<p>Border model generation for energy system area connectivity analysis.</p> <p>This module provides functionality for identifying and modeling borders between energy system areas based on line topologies. It supports the creation of comprehensive border_model_dfs that capture directional relationships, naming conventions, and geometric properties essential for energy systems analysis.</p> Key Capabilities <ul> <li>Automatic border identification from line topology</li> <li>Standardized border naming conventions with directional awareness</li> <li>Integration with geometric border calculators</li> <li>Network graph generation for area connectivity analysis</li> <li>Support for both physical and logical borders (geographically touching borders vs geographically separated borders)</li> </ul> Typical Energy Use Cases <ul> <li>Modeling interconnections between countries, control areas, or market zones</li> <li>Cross-border capacity and flow analysis</li> <li>Network visualization and analysis</li> </ul> MESCAL Integration <p>This module integrates with MESCAL's area accounting system to provide border_model_df building capabilities that support spatial energy system analysis and cross-border flow calculations.</p>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator","title":"AreaModelGenerator","text":"<p>               Bases: <code>GeoModelGeneratorBase</code></p> <p>Generates comprehensive area model DataFrames from node-to-area mappings.</p> <p>This class creates detailed area model DataFrames that aggregate node-level data into area-level representations for energy system analysis. It supports automatic area discovery, node counting, and geographic representative point calculation for visualization and spatial analysis.</p> <p>The generator processes node model data with area assignments to create comprehensive area models suitable for energy system aggregation, market analysis, and spatial visualization workflows.</p> Key Features <ul> <li>Automatic area discovery from node-to-area mappings</li> <li>Representative geographic point calculation for visualization</li> <li>Integration with geometric area data (polygons, boundaries)</li> <li>Support for different area granularities (countries, bidding zones, regions)</li> <li>Robust handling of missing or incomplete area assignments</li> </ul> MESCAL Integration <p>Designed to work with MESCAL's area accounting system, providing area model building capabilities that support spatial energy system analysis, capacity aggregation, and visualization workflows.</p> <p>Attributes:</p> Name Type Description <code>node_model_df</code> <code>DataFrame</code> <p>Node-level data with area assignments</p> <code>area_column</code> <code>str</code> <p>Column name containing area identifiers</p> <code>geo_location_column</code> <code>str</code> <p>Column name containing geographic Point objects</p> <p>Examples:</p> <pre><code>Basic area model generation:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from shapely.geometry import Point\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create node model with area assignments\n&gt;&gt;&gt; node_data = pd.DataFrame({\n&gt;&gt;&gt;     'voltage': [380, 380, 220, 380, 220],\n&gt;&gt;&gt;     'country': ['DE', 'DE', 'FR', 'FR', 'BE'],\n&gt;&gt;&gt;     'capacity_mw': [2000, 1500, 800, 1200, 600],\n&gt;&gt;&gt;     'location': [Point(10, 52), Point(11, 53), Point(2, 48),\n&gt;&gt;&gt;                  Point(3, 49), Point(4, 50)]\n&gt;&gt;&gt; }, index=['DE1', 'DE2', 'FR1', 'FR2', 'BE1'])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate area model\n&gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n&gt;&gt;&gt; area_model = generator.generate_area_model()\n&gt;&gt;&gt; print(area_model)\n              node_count projection_point\n    country\n    DE               2    POINT (10.5 52.5)\n    FR               2    POINT (2.5 48.5)\n    BE               1    POINT (4 50)\n\nEnhanced area model with geometry:\n&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; from shapely.geometry import Polygon\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create area geometries\n&gt;&gt;&gt; area_polygons = gpd.GeoDataFrame({\n&gt;&gt;&gt;     'geometry': [\n&gt;&gt;&gt;         Polygon([(9, 51), (12, 51), (12, 54), (9, 54)]),  # DE\n&gt;&gt;&gt;         Polygon([(1, 47), (4, 47), (4, 50), (1, 50)]),   # FR\n&gt;&gt;&gt;         Polygon([(3, 49), (5, 49), (5, 51), (3, 51)])    # BE\n&gt;&gt;&gt;     ]\n&gt;&gt;&gt; }, index=['DE', 'FR', 'BE'])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Enhance with geometry\n&gt;&gt;&gt; area_model_geo = generator.enhance_with_geometry(area_model, area_polygons)\n&gt;&gt;&gt; print(f\"Enhanced model has geometry: {'geometry' in area_model_geo.columns}\")\n\nCustom enhancement workflow:\n&gt;&gt;&gt; # Step-by-step area model building\n&gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n&gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(base_model)\n&gt;&gt;&gt; final_model = generator.enhance_area_model_df_by_adding_representative_geo_point(enhanced_model)\n&gt;&gt;&gt; print(f\"Created area model with {len(final_model)} areas\")\n</code></pre> Energy Domain Context <ul> <li>Area models are fundamental for energy system analysis, enabling:<ul> <li>Projection of node-level data to area-level data (e.g. nodal prices -&gt; area prices)</li> <li>Market zone aggregation and analysis</li> <li>Regional energy balance studies</li> <li>...</li> </ul> </li> </ul> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>class AreaModelGenerator(GeoModelGeneratorBase):\n    \"\"\"Generates comprehensive area model DataFrames from node-to-area mappings.\n\n    This class creates detailed area model DataFrames that aggregate node-level data\n    into area-level representations for energy system analysis. It supports\n    automatic area discovery, node counting, and geographic representative point\n    calculation for visualization and spatial analysis.\n\n    The generator processes node model data with area assignments to create\n    comprehensive area models suitable for energy system aggregation, market\n    analysis, and spatial visualization workflows.\n\n    Key Features:\n        - Automatic area discovery from node-to-area mappings\n        - Representative geographic point calculation for visualization\n        - Integration with geometric area data (polygons, boundaries)\n        - Support for different area granularities (countries, bidding zones, regions)\n        - Robust handling of missing or incomplete area assignments\n\n    MESCAL Integration:\n        Designed to work with MESCAL's area accounting system, providing\n        area model building capabilities that support spatial energy system analysis,\n        capacity aggregation, and visualization workflows.\n\n    Attributes:\n        node_model_df (pd.DataFrame): Node-level data with area assignments\n        area_column (str): Column name containing area identifiers\n        geo_location_column (str): Column name containing geographic Point objects\n\n    Examples:\n\n        Basic area model generation:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from shapely.geometry import Point\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create node model with area assignments\n        &gt;&gt;&gt; node_data = pd.DataFrame({\n        &gt;&gt;&gt;     'voltage': [380, 380, 220, 380, 220],\n        &gt;&gt;&gt;     'country': ['DE', 'DE', 'FR', 'FR', 'BE'],\n        &gt;&gt;&gt;     'capacity_mw': [2000, 1500, 800, 1200, 600],\n        &gt;&gt;&gt;     'location': [Point(10, 52), Point(11, 53), Point(2, 48),\n        &gt;&gt;&gt;                  Point(3, 49), Point(4, 50)]\n        &gt;&gt;&gt; }, index=['DE1', 'DE2', 'FR1', 'FR2', 'BE1'])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Generate area model\n        &gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n        &gt;&gt;&gt; area_model = generator.generate_area_model()\n        &gt;&gt;&gt; print(area_model)\n                      node_count projection_point\n            country\n            DE               2    POINT (10.5 52.5)\n            FR               2    POINT (2.5 48.5)\n            BE               1    POINT (4 50)\n\n        Enhanced area model with geometry:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; from shapely.geometry import Polygon\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create area geometries\n        &gt;&gt;&gt; area_polygons = gpd.GeoDataFrame({\n        &gt;&gt;&gt;     'geometry': [\n        &gt;&gt;&gt;         Polygon([(9, 51), (12, 51), (12, 54), (9, 54)]),  # DE\n        &gt;&gt;&gt;         Polygon([(1, 47), (4, 47), (4, 50), (1, 50)]),   # FR\n        &gt;&gt;&gt;         Polygon([(3, 49), (5, 49), (5, 51), (3, 51)])    # BE\n        &gt;&gt;&gt;     ]\n        &gt;&gt;&gt; }, index=['DE', 'FR', 'BE'])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Enhance with geometry\n        &gt;&gt;&gt; area_model_geo = generator.enhance_with_geometry(area_model, area_polygons)\n        &gt;&gt;&gt; print(f\"Enhanced model has geometry: {'geometry' in area_model_geo.columns}\")\n\n        Custom enhancement workflow:\n        &gt;&gt;&gt; # Step-by-step area model building\n        &gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n        &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(base_model)\n        &gt;&gt;&gt; final_model = generator.enhance_area_model_df_by_adding_representative_geo_point(enhanced_model)\n        &gt;&gt;&gt; print(f\"Created area model with {len(final_model)} areas\")\n\n    Energy Domain Context:\n        - Area models are fundamental for energy system analysis, enabling:\n            - Projection of node-level data to area-level data (e.g. nodal prices -&gt; area prices)\n            - Market zone aggregation and analysis\n            - Regional energy balance studies\n            - ...\n    \"\"\"\n\n    def __init__(\n            self,\n            node_model_df: pd.DataFrame,\n            area_column: str,\n            geo_location_column: str = None,\n    ):\n        \"\"\"Initialize the area model generator.\n\n        Args:\n            node_model_df: DataFrame containing node-level data with area assignments.\n                Must contain area_column with area identifiers for each node.\n                May contain geographic Point objects for spatial analysis.\n            area_column: Column name in node_model_df containing area assignments\n                (e.g., 'country', 'bidding_zone', 'market_region', 'control_area').\n            geo_location_column: Column name containing geographic Point objects\n                for representative point calculation. If None, automatically\n                detects column containing Point geometries.\n\n        Raises:\n            ValueError: If area_column is not found in node_model_df columns.\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from shapely.geometry import Point\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Node data with area assignments\n            &gt;&gt;&gt; nodes = pd.DataFrame({\n            &gt;&gt;&gt;     'voltage_kv': [380, 220, 380, 150],\n            &gt;&gt;&gt;     'country': ['DE', 'DE', 'FR', 'FR'], \n            &gt;&gt;&gt;     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR'],\n            &gt;&gt;&gt;     'coordinates': [Point(10, 52), Point(11, 53), Point(2, 48), Point(3, 49)]\n            &gt;&gt;&gt; }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Initialize for country-level analysis\n            &gt;&gt;&gt; generator = AreaModelGenerator(nodes, 'country', 'coordinates')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Or let it auto-detect geographic column\n            &gt;&gt;&gt; generator = AreaModelGenerator(nodes, 'bidding_zone')\n        \"\"\"\n        self.node_model_df = node_model_df\n        self.area_column = area_column\n        self.geo_location_column = geo_location_column or self._identify_geo_location_column()\n        self._validate_inputs()\n\n    def _validate_inputs(self):\n        if self.area_column not in self.node_model_df.columns:\n            raise ValueError(\n                f\"Area column '{self.area_column}' not found in node_model_df. \"\n                f\"Available columns: {list(self.node_model_df.columns)}\"\n            )\n\n    def _identify_geo_location_column(self) -&gt; str | None:\n        for c in self.node_model_df.columns:\n            if all(isinstance(i, Point) or i is None for i in self.node_model_df[c].values):\n                return c\n        return None\n\n    def generate_base_area_model_from_area_names_in_node_model_df(self) -&gt; pd.DataFrame:\n        \"\"\"Generate base area model DataFrame from unique area names in node data.\n\n        Creates a minimal area model DataFrame containing only the unique area\n        identifiers found in the node model data. This forms the foundation\n        for building comprehensive area models.\n\n        Returns:\n            pd.DataFrame: Base area model with area identifiers as index.\n                Contains no additional columns - serves as starting point\n                for enhancement with node counts, geographic data, etc.\n\n        Example:\n\n            &gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n            &gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n            &gt;&gt;&gt; print(base_model)\n                Empty DataFrame\n                Columns: []\n                Index: ['DE', 'FR', 'BE']\n\n        Note:\n            Areas with None or NaN values in the area_column are excluded\n            from the generated model.\n        \"\"\"\n        unique_areas = self.node_model_df[self.area_column].dropna().unique()\n        area_model_df = pd.DataFrame(index=unique_areas)\n        area_model_df.index.name = self.area_column\n        return area_model_df\n\n    def ensure_completeness_of_area_model_df(self, area_model_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Ensure area model contains all areas present in node data.\n\n        Validates and extends an existing area model DataFrame to include\n        any areas found in the node data that might be missing from the\n        provided area model. This is useful when working with predefined\n        area models that may not cover all areas in the dataset.\n\n        Args:\n            area_model_df: Existing area model DataFrame to validate and extend.\n\n        Returns:\n            pd.DataFrame: Complete area model containing all areas from node data.\n                Existing data is preserved, new areas are added with NaN values\n                for existing columns.\n\n        Example:\n\n            &gt;&gt;&gt; # Predefined area model missing some areas\n            &gt;&gt;&gt; partial_model = pd.DataFrame({\n            &gt;&gt;&gt;     'max_price': [5000, 3000]\n            &gt;&gt;&gt; }, index=['DE', 'FR'])\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Ensure completeness (adds 'BE' if present in node data)\n            &gt;&gt;&gt; complete_model = generator.ensure_completeness_of_area_model_df(partial_model)\n            &gt;&gt;&gt; print(complete_model)\n                          max_price\n                country\n                DE             5000\n                FR             3000\n                BE              NaN\n\n        Use Case:\n            Essential for maintaining data consistency when combining\n            predefined area models with dynamic node-based area discovery.\n        \"\"\"\n        complete_set = area_model_df.index.to_list()\n        for a in self.node_model_df[self.area_column].unique():\n            if a is not None and a not in complete_set:\n                complete_set.append(a)\n        return area_model_df.reindex(complete_set)\n\n    def enhance_area_model_df_by_adding_node_count_per_area(\n            self,\n            area_model_df: pd.DataFrame,\n            node_count_column_name: str = 'node_count'\n    ) -&gt; pd.DataFrame:\n        \"\"\"Enhance area model by adding node count statistics per area.\n\n        Aggregates the number of nodes assigned to each area and adds this\n        information to the area model DataFrame. Node counts are essential\n        for understanding infrastructure density and capacity distribution.\n\n        Args:\n            area_model_df: Base area model DataFrame to enhance.\n            node_count_column_name: Name for the new node count column.\n                Defaults to 'node_count'.\n\n        Returns:\n            pd.DataFrame: Enhanced area model with node count column added.\n                Existing data is preserved, node counts are added for all areas.\n                Areas not present in node data will have NaN node counts.\n\n        Example:\n\n            &gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n            &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(base_model)\n            &gt;&gt;&gt; print(enhanced_model)\n                          node_count\n                country\n                DE               2\n                FR               2\n                BE               1\n\n            &gt;&gt;&gt; # Custom column name\n            &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(\n            &gt;&gt;&gt;     base_model, 'infrastructure_count'\n            &gt;&gt;&gt; )\n        \"\"\"\n        enhanced_df = area_model_df.copy()\n        node_counts = self.node_model_df[self.area_column].value_counts().to_dict()\n        for node, count in node_counts.items():\n            if node in enhanced_df.index:\n                enhanced_df.loc[node, node_count_column_name] = count\n        return enhanced_df\n\n    def enhance_area_model_df_by_adding_representative_geo_point(\n            self,\n            area_model_df: pd.DataFrame | gpd.GeoDataFrame,\n            target_column_name: str = 'projection_point',\n            round_point_decimals: int = 4,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Enhance area model by adding representative geographic points for\n        labeling and KPI printing in map visualizations.\n\n        Calculates representative geographic points for each area based on\n        either area geometries (if available) or node locations within each area.\n        These points are useful for visualization, labeling, and spatial analysis.\n\n        The method supports two calculation modes:\n        1. Geometry-based: Uses area polygon centroids or representative points\n        2. Node-based: Calculates centroid from node locations within each area\n\n        Args:\n            area_model_df: Area model DataFrame to enhance. Can be regular DataFrame\n                or GeoDataFrame with 'geometry' column.\n            target_column_name: Name for the new representative point column.\n                Defaults to 'projection_point'.\n            round_point_decimals: Number of decimal places for coordinate rounding.\n                Set to None to disable rounding. Defaults to 4.\n\n        Returns:\n            pd.DataFrame: Enhanced area model with representative points added.\n                Points are added as Shapely Point objects suitable for mapping\n                and spatial analysis.\n\n        Raises:\n            TypeError: If geo_location_column contains non-Point objects.\n\n        Example:\n\n            &gt;&gt;&gt; # Node-based representative points\n            &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_representative_geo_point(base_model)\n            &gt;&gt;&gt; print(enhanced_model)\n                          projection_point\n                country\n                DE        POINT (10.5 52.5)\n                FR        POINT (2.5 48.5)\n                BE        POINT (4 50)\n\n            &gt;&gt;&gt; # With custom column name and precision\n            &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_representative_geo_point(\n            &gt;&gt;&gt;     base_model, 'center_point', round_point_decimals=2\n            &gt;&gt;&gt; )\n\n            &gt;&gt;&gt; # Access coordinates for mapping\n            &gt;&gt;&gt; center = enhanced_model.loc['DE', 'projection_point']\n            &gt;&gt;&gt; print(f\"DE center: {center.x:.2f}, {center.y:.2f}\")\n        \"\"\"\n        enhanced_df = area_model_df.copy()\n\n        def round_point(point: Point | None) -&gt; Point | None:\n            if point is None or round_point_decimals is None:\n                return point\n            return type(point)(round(point.x, round_point_decimals), round(point.y, round_point_decimals))\n\n        if target_column_name not in enhanced_df:\n            enhanced_df[target_column_name] = None\n\n        for area in enhanced_df.index:\n            if pd.notna(enhanced_df.loc[area, target_column_name]):\n                continue\n            if 'geometry' in enhanced_df.columns:\n                geo = enhanced_df.loc[area, 'geometry']\n                if pd.notna(geo) and isinstance(geo, (Polygon, MultiPolygon)):\n                    enhanced_df.loc[area, target_column_name] = round_point(self.get_representative_area_point(geo))\n            elif self.geo_location_column:\n                nodes = self.node_model_df.loc[self.node_model_df[self.area_column] == area, self.geo_location_column]\n                nodes = nodes.dropna()\n                if not nodes.empty:\n                    locations = [n for n in nodes.values if n is not None]\n                    if not all(isinstance(i, Point) for i in locations):\n                        raise TypeError(\n                            f'Geographic location column \"{self.geo_location_column}\" must contain only '\n                            f'Point objects. Found: {[type(i).__name__ for i in locations if not isinstance(i, Point)]}'\n                        )\n                    representative_point = self._compute_representative_point_from_cloud_of_2d_points(locations)\n                    enhanced_df.loc[area, target_column_name] = round_point(representative_point)\n        return enhanced_df\n\n    def generate_area_model(self) -&gt; pd.DataFrame:\n        \"\"\"Generate complete area model with node counts and representative points.\n\n        Creates a comprehensive area model DataFrame by combining base area\n        discovery, node count aggregation, and representative geographic point\n        calculation. This is the main method for generating complete area models.\n\n        The generated model includes:\n            - All unique areas from node data\n            - Node count per area for capacity/infrastructure analysis\n            - Representative geographic points for visualization\n\n        Returns:\n            pd.DataFrame: Complete area model with node counts and geographic data.\n                Index contains area identifiers, columns include 'node_count'\n                and 'projection_point' (if geographic data available).\n\n        Example:\n\n            &gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n            &gt;&gt;&gt; area_model = generator.generate_area_model()\n            &gt;&gt;&gt; print(area_model)\n                          node_count projection_point\n                country\n                DE               2    POINT (10.5 52.5)\n                FR               2    POINT (2.5 48.5)\n                BE               1    POINT (4 50)\n        \"\"\"\n        area_model_df = self.generate_base_area_model_from_area_names_in_node_model_df()\n        area_model_df = self.enhance_area_model_df_by_adding_node_count_per_area(area_model_df)\n        area_model_df = self.enhance_area_model_df_by_adding_representative_geo_point(area_model_df)\n        return area_model_df\n\n    def enhance_with_geometry(\n        self,\n        area_model_df: pd.DataFrame,\n        area_gdf: gpd.GeoDataFrame\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Enhance area model with geometric polygon data for spatial analysis.\n\n        Integrates area polygon geometries from a GeoDataFrame into the area model,\n        enabling advanced spatial analysis, visualization, and border calculations.\n        The method matches areas by index and creates a proper GeoDataFrame output.\n\n        Args:\n            area_model_df: Area model DataFrame to enhance with geometry.\n            area_gdf: GeoDataFrame containing area polygon geometries.\n                Must have 'geometry' column with Polygon or MultiPolygon objects.\n                Areas are matched by index values.\n\n        Returns:\n            gpd.GeoDataFrame: Enhanced area model as GeoDataFrame with geometry column.\n                All original data is preserved, geometry column is added for areas\n                that exist in both DataFrames. Missing geometries are set to None.\n\n        Example:\n\n            &gt;&gt;&gt; import geopandas as gpd\n            &gt;&gt;&gt; from shapely.geometry import Polygon\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Create area geometries\n            &gt;&gt;&gt; area_polygons = gpd.GeoDataFrame({\n            &gt;&gt;&gt;     'area_name': ['Germany', 'France', 'Belgium'],\n            &gt;&gt;&gt;     'geometry': [\n            &gt;&gt;&gt;         Polygon([(9, 51), (12, 51), (12, 54), (9, 54)]),\n            &gt;&gt;&gt;         Polygon([(1, 47), (4, 47), (4, 50), (1, 50)]),\n            &gt;&gt;&gt;         Polygon([(3, 49), (5, 49), (5, 51), (3, 51)])\n            &gt;&gt;&gt;     ]\n            &gt;&gt;&gt; }, index=['DE', 'FR', 'BE'])\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Enhance area model with geometry\n            &gt;&gt;&gt; geo_model = generator.enhance_with_geometry(area_model, area_polygons)\n            &gt;&gt;&gt; print(f\"Model has geometry: {isinstance(geo_model, gpd.GeoDataFrame)}\")\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Use for spatial operations\n            &gt;&gt;&gt; total_area = geo_model['geometry'].area.sum()\n            &gt;&gt;&gt; print(f\"Total area: {total_area:.0f} square units\")\n        \"\"\"\n        enhanced_df = area_model_df.copy()\n        if 'geometry' not in enhanced_df.columns:\n            enhanced_df['geometry'] = None\n        for area in area_model_df.index:\n            if area in area_gdf.index:\n                enhanced_df.loc[area, 'geometry'] = area_gdf.loc[area, 'geometry']\n        if not isinstance(enhanced_df, gpd.GeoDataFrame):\n            enhanced_df = gpd.GeoDataFrame(enhanced_df, geometry='geometry')\n        return enhanced_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.__init__","title":"__init__","text":"<pre><code>__init__(node_model_df: DataFrame, area_column: str, geo_location_column: str = None)\n</code></pre> <p>Initialize the area model generator.</p> <p>Parameters:</p> Name Type Description Default <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame containing node-level data with area assignments. Must contain area_column with area identifiers for each node. May contain geographic Point objects for spatial analysis.</p> required <code>area_column</code> <code>str</code> <p>Column name in node_model_df containing area assignments (e.g., 'country', 'bidding_zone', 'market_region', 'control_area').</p> required <code>geo_location_column</code> <code>str</code> <p>Column name containing geographic Point objects for representative point calculation. If None, automatically detects column containing Point geometries.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If area_column is not found in node_model_df columns.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from shapely.geometry import Point\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Node data with area assignments\n&gt;&gt;&gt; nodes = pd.DataFrame({\n&gt;&gt;&gt;     'voltage_kv': [380, 220, 380, 150],\n&gt;&gt;&gt;     'country': ['DE', 'DE', 'FR', 'FR'], \n&gt;&gt;&gt;     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR'],\n&gt;&gt;&gt;     'coordinates': [Point(10, 52), Point(11, 53), Point(2, 48), Point(3, 49)]\n&gt;&gt;&gt; }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Initialize for country-level analysis\n&gt;&gt;&gt; generator = AreaModelGenerator(nodes, 'country', 'coordinates')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Or let it auto-detect geographic column\n&gt;&gt;&gt; generator = AreaModelGenerator(nodes, 'bidding_zone')\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def __init__(\n        self,\n        node_model_df: pd.DataFrame,\n        area_column: str,\n        geo_location_column: str = None,\n):\n    \"\"\"Initialize the area model generator.\n\n    Args:\n        node_model_df: DataFrame containing node-level data with area assignments.\n            Must contain area_column with area identifiers for each node.\n            May contain geographic Point objects for spatial analysis.\n        area_column: Column name in node_model_df containing area assignments\n            (e.g., 'country', 'bidding_zone', 'market_region', 'control_area').\n        geo_location_column: Column name containing geographic Point objects\n            for representative point calculation. If None, automatically\n            detects column containing Point geometries.\n\n    Raises:\n        ValueError: If area_column is not found in node_model_df columns.\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from shapely.geometry import Point\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Node data with area assignments\n        &gt;&gt;&gt; nodes = pd.DataFrame({\n        &gt;&gt;&gt;     'voltage_kv': [380, 220, 380, 150],\n        &gt;&gt;&gt;     'country': ['DE', 'DE', 'FR', 'FR'], \n        &gt;&gt;&gt;     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR'],\n        &gt;&gt;&gt;     'coordinates': [Point(10, 52), Point(11, 53), Point(2, 48), Point(3, 49)]\n        &gt;&gt;&gt; }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Initialize for country-level analysis\n        &gt;&gt;&gt; generator = AreaModelGenerator(nodes, 'country', 'coordinates')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Or let it auto-detect geographic column\n        &gt;&gt;&gt; generator = AreaModelGenerator(nodes, 'bidding_zone')\n    \"\"\"\n    self.node_model_df = node_model_df\n    self.area_column = area_column\n    self.geo_location_column = geo_location_column or self._identify_geo_location_column()\n    self._validate_inputs()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.generate_base_area_model_from_area_names_in_node_model_df","title":"generate_base_area_model_from_area_names_in_node_model_df","text":"<pre><code>generate_base_area_model_from_area_names_in_node_model_df() -&gt; DataFrame\n</code></pre> <p>Generate base area model DataFrame from unique area names in node data.</p> <p>Creates a minimal area model DataFrame containing only the unique area identifiers found in the node model data. This forms the foundation for building comprehensive area models.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Base area model with area identifiers as index. Contains no additional columns - serves as starting point for enhancement with node counts, geographic data, etc.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n&gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n&gt;&gt;&gt; print(base_model)\n    Empty DataFrame\n    Columns: []\n    Index: ['DE', 'FR', 'BE']\n</code></pre> Note <p>Areas with None or NaN values in the area_column are excluded from the generated model.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def generate_base_area_model_from_area_names_in_node_model_df(self) -&gt; pd.DataFrame:\n    \"\"\"Generate base area model DataFrame from unique area names in node data.\n\n    Creates a minimal area model DataFrame containing only the unique area\n    identifiers found in the node model data. This forms the foundation\n    for building comprehensive area models.\n\n    Returns:\n        pd.DataFrame: Base area model with area identifiers as index.\n            Contains no additional columns - serves as starting point\n            for enhancement with node counts, geographic data, etc.\n\n    Example:\n\n        &gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n        &gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n        &gt;&gt;&gt; print(base_model)\n            Empty DataFrame\n            Columns: []\n            Index: ['DE', 'FR', 'BE']\n\n    Note:\n        Areas with None or NaN values in the area_column are excluded\n        from the generated model.\n    \"\"\"\n    unique_areas = self.node_model_df[self.area_column].dropna().unique()\n    area_model_df = pd.DataFrame(index=unique_areas)\n    area_model_df.index.name = self.area_column\n    return area_model_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.ensure_completeness_of_area_model_df","title":"ensure_completeness_of_area_model_df","text":"<pre><code>ensure_completeness_of_area_model_df(area_model_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Ensure area model contains all areas present in node data.</p> <p>Validates and extends an existing area model DataFrame to include any areas found in the node data that might be missing from the provided area model. This is useful when working with predefined area models that may not cover all areas in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>area_model_df</code> <code>DataFrame</code> <p>Existing area model DataFrame to validate and extend.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Complete area model containing all areas from node data. Existing data is preserved, new areas are added with NaN values for existing columns.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Predefined area model missing some areas\n&gt;&gt;&gt; partial_model = pd.DataFrame({\n&gt;&gt;&gt;     'max_price': [5000, 3000]\n&gt;&gt;&gt; }, index=['DE', 'FR'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Ensure completeness (adds 'BE' if present in node data)\n&gt;&gt;&gt; complete_model = generator.ensure_completeness_of_area_model_df(partial_model)\n&gt;&gt;&gt; print(complete_model)\n              max_price\n    country\n    DE             5000\n    FR             3000\n    BE              NaN\n</code></pre> Use Case <p>Essential for maintaining data consistency when combining predefined area models with dynamic node-based area discovery.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def ensure_completeness_of_area_model_df(self, area_model_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Ensure area model contains all areas present in node data.\n\n    Validates and extends an existing area model DataFrame to include\n    any areas found in the node data that might be missing from the\n    provided area model. This is useful when working with predefined\n    area models that may not cover all areas in the dataset.\n\n    Args:\n        area_model_df: Existing area model DataFrame to validate and extend.\n\n    Returns:\n        pd.DataFrame: Complete area model containing all areas from node data.\n            Existing data is preserved, new areas are added with NaN values\n            for existing columns.\n\n    Example:\n\n        &gt;&gt;&gt; # Predefined area model missing some areas\n        &gt;&gt;&gt; partial_model = pd.DataFrame({\n        &gt;&gt;&gt;     'max_price': [5000, 3000]\n        &gt;&gt;&gt; }, index=['DE', 'FR'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Ensure completeness (adds 'BE' if present in node data)\n        &gt;&gt;&gt; complete_model = generator.ensure_completeness_of_area_model_df(partial_model)\n        &gt;&gt;&gt; print(complete_model)\n                      max_price\n            country\n            DE             5000\n            FR             3000\n            BE              NaN\n\n    Use Case:\n        Essential for maintaining data consistency when combining\n        predefined area models with dynamic node-based area discovery.\n    \"\"\"\n    complete_set = area_model_df.index.to_list()\n    for a in self.node_model_df[self.area_column].unique():\n        if a is not None and a not in complete_set:\n            complete_set.append(a)\n    return area_model_df.reindex(complete_set)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.enhance_area_model_df_by_adding_node_count_per_area","title":"enhance_area_model_df_by_adding_node_count_per_area","text":"<pre><code>enhance_area_model_df_by_adding_node_count_per_area(area_model_df: DataFrame, node_count_column_name: str = 'node_count') -&gt; DataFrame\n</code></pre> <p>Enhance area model by adding node count statistics per area.</p> <p>Aggregates the number of nodes assigned to each area and adds this information to the area model DataFrame. Node counts are essential for understanding infrastructure density and capacity distribution.</p> <p>Parameters:</p> Name Type Description Default <code>area_model_df</code> <code>DataFrame</code> <p>Base area model DataFrame to enhance.</p> required <code>node_count_column_name</code> <code>str</code> <p>Name for the new node count column. Defaults to 'node_count'.</p> <code>'node_count'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Enhanced area model with node count column added. Existing data is preserved, node counts are added for all areas. Areas not present in node data will have NaN node counts.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n&gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(base_model)\n&gt;&gt;&gt; print(enhanced_model)\n              node_count\n    country\n    DE               2\n    FR               2\n    BE               1\n\n&gt;&gt;&gt; # Custom column name\n&gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(\n&gt;&gt;&gt;     base_model, 'infrastructure_count'\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def enhance_area_model_df_by_adding_node_count_per_area(\n        self,\n        area_model_df: pd.DataFrame,\n        node_count_column_name: str = 'node_count'\n) -&gt; pd.DataFrame:\n    \"\"\"Enhance area model by adding node count statistics per area.\n\n    Aggregates the number of nodes assigned to each area and adds this\n    information to the area model DataFrame. Node counts are essential\n    for understanding infrastructure density and capacity distribution.\n\n    Args:\n        area_model_df: Base area model DataFrame to enhance.\n        node_count_column_name: Name for the new node count column.\n            Defaults to 'node_count'.\n\n    Returns:\n        pd.DataFrame: Enhanced area model with node count column added.\n            Existing data is preserved, node counts are added for all areas.\n            Areas not present in node data will have NaN node counts.\n\n    Example:\n\n        &gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n        &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(base_model)\n        &gt;&gt;&gt; print(enhanced_model)\n                      node_count\n            country\n            DE               2\n            FR               2\n            BE               1\n\n        &gt;&gt;&gt; # Custom column name\n        &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(\n        &gt;&gt;&gt;     base_model, 'infrastructure_count'\n        &gt;&gt;&gt; )\n    \"\"\"\n    enhanced_df = area_model_df.copy()\n    node_counts = self.node_model_df[self.area_column].value_counts().to_dict()\n    for node, count in node_counts.items():\n        if node in enhanced_df.index:\n            enhanced_df.loc[node, node_count_column_name] = count\n    return enhanced_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.enhance_area_model_df_by_adding_representative_geo_point","title":"enhance_area_model_df_by_adding_representative_geo_point","text":"<pre><code>enhance_area_model_df_by_adding_representative_geo_point(area_model_df: DataFrame | GeoDataFrame, target_column_name: str = 'projection_point', round_point_decimals: int = 4) -&gt; DataFrame\n</code></pre> <p>Enhance area model by adding representative geographic points for labeling and KPI printing in map visualizations.</p> <p>Calculates representative geographic points for each area based on either area geometries (if available) or node locations within each area. These points are useful for visualization, labeling, and spatial analysis.</p> <p>The method supports two calculation modes: 1. Geometry-based: Uses area polygon centroids or representative points 2. Node-based: Calculates centroid from node locations within each area</p> <p>Parameters:</p> Name Type Description Default <code>area_model_df</code> <code>DataFrame | GeoDataFrame</code> <p>Area model DataFrame to enhance. Can be regular DataFrame or GeoDataFrame with 'geometry' column.</p> required <code>target_column_name</code> <code>str</code> <p>Name for the new representative point column. Defaults to 'projection_point'.</p> <code>'projection_point'</code> <code>round_point_decimals</code> <code>int</code> <p>Number of decimal places for coordinate rounding. Set to None to disable rounding. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Enhanced area model with representative points added. Points are added as Shapely Point objects suitable for mapping and spatial analysis.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If geo_location_column contains non-Point objects.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Node-based representative points\n&gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_representative_geo_point(base_model)\n&gt;&gt;&gt; print(enhanced_model)\n              projection_point\n    country\n    DE        POINT (10.5 52.5)\n    FR        POINT (2.5 48.5)\n    BE        POINT (4 50)\n\n&gt;&gt;&gt; # With custom column name and precision\n&gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_representative_geo_point(\n&gt;&gt;&gt;     base_model, 'center_point', round_point_decimals=2\n&gt;&gt;&gt; )\n\n&gt;&gt;&gt; # Access coordinates for mapping\n&gt;&gt;&gt; center = enhanced_model.loc['DE', 'projection_point']\n&gt;&gt;&gt; print(f\"DE center: {center.x:.2f}, {center.y:.2f}\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def enhance_area_model_df_by_adding_representative_geo_point(\n        self,\n        area_model_df: pd.DataFrame | gpd.GeoDataFrame,\n        target_column_name: str = 'projection_point',\n        round_point_decimals: int = 4,\n) -&gt; pd.DataFrame:\n    \"\"\"Enhance area model by adding representative geographic points for\n    labeling and KPI printing in map visualizations.\n\n    Calculates representative geographic points for each area based on\n    either area geometries (if available) or node locations within each area.\n    These points are useful for visualization, labeling, and spatial analysis.\n\n    The method supports two calculation modes:\n    1. Geometry-based: Uses area polygon centroids or representative points\n    2. Node-based: Calculates centroid from node locations within each area\n\n    Args:\n        area_model_df: Area model DataFrame to enhance. Can be regular DataFrame\n            or GeoDataFrame with 'geometry' column.\n        target_column_name: Name for the new representative point column.\n            Defaults to 'projection_point'.\n        round_point_decimals: Number of decimal places for coordinate rounding.\n            Set to None to disable rounding. Defaults to 4.\n\n    Returns:\n        pd.DataFrame: Enhanced area model with representative points added.\n            Points are added as Shapely Point objects suitable for mapping\n            and spatial analysis.\n\n    Raises:\n        TypeError: If geo_location_column contains non-Point objects.\n\n    Example:\n\n        &gt;&gt;&gt; # Node-based representative points\n        &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_representative_geo_point(base_model)\n        &gt;&gt;&gt; print(enhanced_model)\n                      projection_point\n            country\n            DE        POINT (10.5 52.5)\n            FR        POINT (2.5 48.5)\n            BE        POINT (4 50)\n\n        &gt;&gt;&gt; # With custom column name and precision\n        &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_representative_geo_point(\n        &gt;&gt;&gt;     base_model, 'center_point', round_point_decimals=2\n        &gt;&gt;&gt; )\n\n        &gt;&gt;&gt; # Access coordinates for mapping\n        &gt;&gt;&gt; center = enhanced_model.loc['DE', 'projection_point']\n        &gt;&gt;&gt; print(f\"DE center: {center.x:.2f}, {center.y:.2f}\")\n    \"\"\"\n    enhanced_df = area_model_df.copy()\n\n    def round_point(point: Point | None) -&gt; Point | None:\n        if point is None or round_point_decimals is None:\n            return point\n        return type(point)(round(point.x, round_point_decimals), round(point.y, round_point_decimals))\n\n    if target_column_name not in enhanced_df:\n        enhanced_df[target_column_name] = None\n\n    for area in enhanced_df.index:\n        if pd.notna(enhanced_df.loc[area, target_column_name]):\n            continue\n        if 'geometry' in enhanced_df.columns:\n            geo = enhanced_df.loc[area, 'geometry']\n            if pd.notna(geo) and isinstance(geo, (Polygon, MultiPolygon)):\n                enhanced_df.loc[area, target_column_name] = round_point(self.get_representative_area_point(geo))\n        elif self.geo_location_column:\n            nodes = self.node_model_df.loc[self.node_model_df[self.area_column] == area, self.geo_location_column]\n            nodes = nodes.dropna()\n            if not nodes.empty:\n                locations = [n for n in nodes.values if n is not None]\n                if not all(isinstance(i, Point) for i in locations):\n                    raise TypeError(\n                        f'Geographic location column \"{self.geo_location_column}\" must contain only '\n                        f'Point objects. Found: {[type(i).__name__ for i in locations if not isinstance(i, Point)]}'\n                    )\n                representative_point = self._compute_representative_point_from_cloud_of_2d_points(locations)\n                enhanced_df.loc[area, target_column_name] = round_point(representative_point)\n    return enhanced_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.generate_area_model","title":"generate_area_model","text":"<pre><code>generate_area_model() -&gt; DataFrame\n</code></pre> <p>Generate complete area model with node counts and representative points.</p> <p>Creates a comprehensive area model DataFrame by combining base area discovery, node count aggregation, and representative geographic point calculation. This is the main method for generating complete area models.</p> The generated model includes <ul> <li>All unique areas from node data</li> <li>Node count per area for capacity/infrastructure analysis</li> <li>Representative geographic points for visualization</li> </ul> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Complete area model with node counts and geographic data. Index contains area identifiers, columns include 'node_count' and 'projection_point' (if geographic data available).</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n&gt;&gt;&gt; area_model = generator.generate_area_model()\n&gt;&gt;&gt; print(area_model)\n              node_count projection_point\n    country\n    DE               2    POINT (10.5 52.5)\n    FR               2    POINT (2.5 48.5)\n    BE               1    POINT (4 50)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def generate_area_model(self) -&gt; pd.DataFrame:\n    \"\"\"Generate complete area model with node counts and representative points.\n\n    Creates a comprehensive area model DataFrame by combining base area\n    discovery, node count aggregation, and representative geographic point\n    calculation. This is the main method for generating complete area models.\n\n    The generated model includes:\n        - All unique areas from node data\n        - Node count per area for capacity/infrastructure analysis\n        - Representative geographic points for visualization\n\n    Returns:\n        pd.DataFrame: Complete area model with node counts and geographic data.\n            Index contains area identifiers, columns include 'node_count'\n            and 'projection_point' (if geographic data available).\n\n    Example:\n\n        &gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n        &gt;&gt;&gt; area_model = generator.generate_area_model()\n        &gt;&gt;&gt; print(area_model)\n                      node_count projection_point\n            country\n            DE               2    POINT (10.5 52.5)\n            FR               2    POINT (2.5 48.5)\n            BE               1    POINT (4 50)\n    \"\"\"\n    area_model_df = self.generate_base_area_model_from_area_names_in_node_model_df()\n    area_model_df = self.enhance_area_model_df_by_adding_node_count_per_area(area_model_df)\n    area_model_df = self.enhance_area_model_df_by_adding_representative_geo_point(area_model_df)\n    return area_model_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.enhance_with_geometry","title":"enhance_with_geometry","text":"<pre><code>enhance_with_geometry(area_model_df: DataFrame, area_gdf: GeoDataFrame) -&gt; GeoDataFrame\n</code></pre> <p>Enhance area model with geometric polygon data for spatial analysis.</p> <p>Integrates area polygon geometries from a GeoDataFrame into the area model, enabling advanced spatial analysis, visualization, and border calculations. The method matches areas by index and creates a proper GeoDataFrame output.</p> <p>Parameters:</p> Name Type Description Default <code>area_model_df</code> <code>DataFrame</code> <p>Area model DataFrame to enhance with geometry.</p> required <code>area_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing area polygon geometries. Must have 'geometry' column with Polygon or MultiPolygon objects. Areas are matched by index values.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Enhanced area model as GeoDataFrame with geometry column. All original data is preserved, geometry column is added for areas that exist in both DataFrames. Missing geometries are set to None.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; from shapely.geometry import Polygon\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Create area geometries\n&gt;&gt;&gt; area_polygons = gpd.GeoDataFrame({\n&gt;&gt;&gt;     'area_name': ['Germany', 'France', 'Belgium'],\n&gt;&gt;&gt;     'geometry': [\n&gt;&gt;&gt;         Polygon([(9, 51), (12, 51), (12, 54), (9, 54)]),\n&gt;&gt;&gt;         Polygon([(1, 47), (4, 47), (4, 50), (1, 50)]),\n&gt;&gt;&gt;         Polygon([(3, 49), (5, 49), (5, 51), (3, 51)])\n&gt;&gt;&gt;     ]\n&gt;&gt;&gt; }, index=['DE', 'FR', 'BE'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Enhance area model with geometry\n&gt;&gt;&gt; geo_model = generator.enhance_with_geometry(area_model, area_polygons)\n&gt;&gt;&gt; print(f\"Model has geometry: {isinstance(geo_model, gpd.GeoDataFrame)}\")\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Use for spatial operations\n&gt;&gt;&gt; total_area = geo_model['geometry'].area.sum()\n&gt;&gt;&gt; print(f\"Total area: {total_area:.0f} square units\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def enhance_with_geometry(\n    self,\n    area_model_df: pd.DataFrame,\n    area_gdf: gpd.GeoDataFrame\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Enhance area model with geometric polygon data for spatial analysis.\n\n    Integrates area polygon geometries from a GeoDataFrame into the area model,\n    enabling advanced spatial analysis, visualization, and border calculations.\n    The method matches areas by index and creates a proper GeoDataFrame output.\n\n    Args:\n        area_model_df: Area model DataFrame to enhance with geometry.\n        area_gdf: GeoDataFrame containing area polygon geometries.\n            Must have 'geometry' column with Polygon or MultiPolygon objects.\n            Areas are matched by index values.\n\n    Returns:\n        gpd.GeoDataFrame: Enhanced area model as GeoDataFrame with geometry column.\n            All original data is preserved, geometry column is added for areas\n            that exist in both DataFrames. Missing geometries are set to None.\n\n    Example:\n\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; from shapely.geometry import Polygon\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Create area geometries\n        &gt;&gt;&gt; area_polygons = gpd.GeoDataFrame({\n        &gt;&gt;&gt;     'area_name': ['Germany', 'France', 'Belgium'],\n        &gt;&gt;&gt;     'geometry': [\n        &gt;&gt;&gt;         Polygon([(9, 51), (12, 51), (12, 54), (9, 54)]),\n        &gt;&gt;&gt;         Polygon([(1, 47), (4, 47), (4, 50), (1, 50)]),\n        &gt;&gt;&gt;         Polygon([(3, 49), (5, 49), (5, 51), (3, 51)])\n        &gt;&gt;&gt;     ]\n        &gt;&gt;&gt; }, index=['DE', 'FR', 'BE'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Enhance area model with geometry\n        &gt;&gt;&gt; geo_model = generator.enhance_with_geometry(area_model, area_polygons)\n        &gt;&gt;&gt; print(f\"Model has geometry: {isinstance(geo_model, gpd.GeoDataFrame)}\")\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Use for spatial operations\n        &gt;&gt;&gt; total_area = geo_model['geometry'].area.sum()\n        &gt;&gt;&gt; print(f\"Total area: {total_area:.0f} square units\")\n    \"\"\"\n    enhanced_df = area_model_df.copy()\n    if 'geometry' not in enhanced_df.columns:\n        enhanced_df['geometry'] = None\n    for area in area_model_df.index:\n        if area in area_gdf.index:\n            enhanced_df.loc[area, 'geometry'] = area_gdf.loc[area, 'geometry']\n    if not isinstance(enhanced_df, gpd.GeoDataFrame):\n        enhanced_df = gpd.GeoDataFrame(enhanced_df, geometry='geometry')\n    return enhanced_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_generator.AreaBorderNamingConventions","title":"AreaBorderNamingConventions","text":"<p>Standardized naming conventions for energy system area borders.</p> <p>This class provides consistent naming patterns for borders between energy system areas (countries, bidding zones, market regions). It ensures standardized naming across different analysis workflows and supports bidirectional relationship management.</p> The naming system supports <ul> <li>Configurable separators and prefixes/suffixes</li> <li>Bidirectional border identification (A-B and B-A)</li> <li>Alphabetically sorted canonical border names</li> <li>Consistent column naming for source and target areas</li> </ul> Key Features <ul> <li>Configurable naming patterns for different use cases</li> <li>Automatic opposite border name generation</li> <li>Alphabetical sorting for canonical border representation</li> <li>Consistent identifier generation for database/DataFrame columns</li> </ul> <p>Attributes:</p> Name Type Description <code>JOIN_AREA_NAMES_BY</code> <code>str</code> <p>Separator for area names in border identifiers</p> <code>SOURCE_AREA_IDENTIFIER_SUFFIX</code> <code>str</code> <p>Suffix for source area column names</p> <code>TARGET_AREA_IDENTIFIER_SUFFIX</code> <code>str</code> <p>Suffix for target area column names</p> <code>OPPOSITE_BORDER_IDENTIFIER</code> <code>str</code> <p>Column name for opposite border references</p> <code>SORTED_BORDER_IDENTIFIER</code> <code>str</code> <p>Column name for alphabetically sorted borders</p> <code>NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER</code> <code>str</code> <p>Boolean indicator column</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n&gt;&gt;&gt; border_name = conventions.get_area_border_name('DE', 'FR')\n&gt;&gt;&gt; print(border_name)  # 'DE - FR'\n&gt;&gt;&gt; opposite = conventions.get_opposite_area_border_name(border_name)\n&gt;&gt;&gt; print(opposite)  # 'FR - DE'\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>class AreaBorderNamingConventions:\n    \"\"\"Standardized naming conventions for energy system area borders.\n\n    This class provides consistent naming patterns for borders between energy\n    system areas (countries, bidding zones, market regions). It ensures\n    standardized naming across different analysis workflows and supports\n    bidirectional relationship management.\n\n    The naming system supports:\n        - Configurable separators and prefixes/suffixes\n        - Bidirectional border identification (A-B and B-A)\n        - Alphabetically sorted canonical border names\n        - Consistent column naming for source and target areas\n\n    Key Features:\n        - Configurable naming patterns for different use cases\n        - Automatic opposite border name generation\n        - Alphabetical sorting for canonical border representation\n        - Consistent identifier generation for database/DataFrame columns\n\n    Attributes:\n        JOIN_AREA_NAMES_BY (str): Separator for area names in border identifiers\n        SOURCE_AREA_IDENTIFIER_SUFFIX (str): Suffix for source area column names\n        TARGET_AREA_IDENTIFIER_SUFFIX (str): Suffix for target area column names\n        OPPOSITE_BORDER_IDENTIFIER (str): Column name for opposite border references\n        SORTED_BORDER_IDENTIFIER (str): Column name for alphabetically sorted borders\n        NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER (str): Boolean indicator column\n\n    Example:\n\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n        &gt;&gt;&gt; border_name = conventions.get_area_border_name('DE', 'FR')\n        &gt;&gt;&gt; print(border_name)  # 'DE - FR'\n        &gt;&gt;&gt; opposite = conventions.get_opposite_area_border_name(border_name)\n        &gt;&gt;&gt; print(opposite)  # 'FR - DE'\n    \"\"\"\n\n    JOIN_AREA_NAMES_BY = ' - '\n    SOURCE_AREA_IDENTIFIER_PREFIX = ''\n    TARGET_AREA_IDENTIFIER_PREFIX = ''\n    SOURCE_AREA_IDENTIFIER_SUFFIX = '_from'\n    TARGET_AREA_IDENTIFIER_SUFFIX = '_to'\n    OPPOSITE_BORDER_IDENTIFIER = 'opposite_border'\n    SORTED_BORDER_IDENTIFIER = 'sorted_border'\n    NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER = 'name_is_alphabetically_sorted'\n    PROJECTION_POINT_IDENTIFIER = 'projection_point'\n    AZIMUTH_ANGLE_IDENTIFIER = 'azimuth_angle'\n    BORDER_IS_PHYSICAL_IDENTIFIER = 'is_physical'\n    BORDER_LINE_STRING_IDENTIFIER = 'geo_line_string'\n\n    def __init__(\n            self,\n            area_column: str,\n            border_identifier: str = None,\n            source_area_identifier: str = None,\n            target_area_identifier: str = None,\n    ):\n        \"\"\"Initialize border naming conventions.\n\n        Args:\n            area_column: Name of the area column (e.g., 'country', 'bidding_zone')\n            border_identifier: Custom name for border identifier column.\n                Defaults to '{area_column}_border'\n            source_area_identifier: Custom name for source area column.\n                Defaults to '{area_column}_from'\n            target_area_identifier: Custom name for target area column.\n                Defaults to '{area_column}_to'\n\n        Example:\n\n            &gt;&gt;&gt; # Standard naming\n            &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n            &gt;&gt;&gt; print(conventions.border_identifier)  # 'country_border'\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Custom naming\n            &gt;&gt;&gt; conventions = AreaBorderNamingConventions(\n            ...     'bidding_zone',\n            ...     border_identifier='interconnection',\n            ...     source_area_identifier='origin_zone'\n            ... )\n        \"\"\"\n        self.area_column = area_column\n        self.border_identifier = border_identifier or self._default_border_identifier()\n        self.source_area_identifier = source_area_identifier or self._default_source_area_identifier()\n        self.target_area_identifier = target_area_identifier or self._default_target_area_identifier()\n\n    def _default_border_identifier(self) -&gt; str:\n        return f'{self.area_column}_border'\n\n    def _default_source_area_identifier(self) -&gt; str:\n        return f'{self.SOURCE_AREA_IDENTIFIER_PREFIX}{self.area_column}{self.SOURCE_AREA_IDENTIFIER_SUFFIX}'\n\n    def _default_target_area_identifier(self) -&gt; str:\n        return f'{self.TARGET_AREA_IDENTIFIER_PREFIX}{self.area_column}{self.TARGET_AREA_IDENTIFIER_SUFFIX}'\n\n    def get_area_border_name(self, area_from: str, area_to: str) -&gt; str:\n        \"\"\"Generate standardized border name from source and target areas.\n\n        Args:\n            area_from: Source area identifier (e.g., 'DE', 'FR_North')\n            area_to: Target area identifier (e.g., 'FR', 'DE_South')\n\n        Returns:\n            str: Formatted border name using the configured separator (e.g. 'DE - FR', 'FR_North - DE_South')\n\n        Example:\n\n            &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n            &gt;&gt;&gt; border_name = conventions.get_area_border_name('DE', 'FR')\n            &gt;&gt;&gt; print(border_name)  # 'DE - FR'\n        \"\"\"\n        return f'{area_from}{self.JOIN_AREA_NAMES_BY}{area_to}'\n\n    def decompose_area_border_name_to_areas(self, border_name: str) -&gt; Tuple[str, str]:\n        \"\"\"Extract source and target area names from border identifier.\n\n        Args:\n            border_name: Border name in standard format (e.g., 'DE - FR')\n\n        Returns:\n            Tuple[str, str]: Source and target area names\n\n        Raises:\n            ValueError: If border_name doesn't contain the expected separator\n\n        Example:\n\n            &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n            &gt;&gt;&gt; area_from, area_to = conventions.decompose_area_border_name_to_areas('DE - FR')\n            &gt;&gt;&gt; print(f\"From: {area_from}, To: {area_to}\")  # From: DE, To: FR\n        \"\"\"\n        area_from, area_to = border_name.split(self.JOIN_AREA_NAMES_BY)\n        return area_from, area_to\n\n    def get_opposite_area_border_name(self, border_name: str) -&gt; str:\n        \"\"\"Generate the opposite direction border name.\n\n        Args:\n            border_name: Original border name (e.g., 'DE - FR')\n\n        Returns:\n            str: Opposite direction border name (e.g., 'FR - DE')\n\n        Example:\n\n            &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n            &gt;&gt;&gt; opposite = conventions.get_opposite_area_border_name('DE - FR')\n            &gt;&gt;&gt; print(opposite)  # 'FR - DE'\n\n        Energy Domain Context:\n            Energy flows and capacities are often directional, requiring\n            tracking of both A\u2192B and B\u2192A relationships for comprehensive\n            border analysis.\n        \"\"\"\n        area_from, area_to = self.decompose_area_border_name_to_areas(border_name)\n        return self.get_area_border_name(area_to, area_from)\n\n    def get_alphabetically_sorted_border(self, border_name: str) -&gt; str:\n        \"\"\"Generate alphabetically sorted canonical border name.\n\n        Creates a canonical representation where area names are sorted\n        alphabetically, useful for identifying unique borders regardless\n        of direction specification, or for matching borders of opposite direction.\n\n        Args:\n            border_name: Border name in any direction (e.g., 'FR - DE' or 'DE - FR')\n\n        Returns:\n            str: Alphabetically sorted border name (e.g., 'DE - FR')\n\n        Example:\n\n            &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n            &gt;&gt;&gt; sorted_border = conventions.get_alphabetically_sorted_border('FR - DE')\n            &gt;&gt;&gt; print(sorted_border)  # 'DE - FR'\n\n        Use Case:\n            Canonical naming is essential for border deduplication and\n            consistent reference in energy system databases and analysis.\n        \"\"\"\n        area_from, area_to = self.decompose_area_border_name_to_areas(border_name)\n        return self.get_area_border_name(*list(sorted([area_from, area_to])))\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_generator.AreaBorderNamingConventions.__init__","title":"__init__","text":"<pre><code>__init__(area_column: str, border_identifier: str = None, source_area_identifier: str = None, target_area_identifier: str = None)\n</code></pre> <p>Initialize border naming conventions.</p> <p>Parameters:</p> Name Type Description Default <code>area_column</code> <code>str</code> <p>Name of the area column (e.g., 'country', 'bidding_zone')</p> required <code>border_identifier</code> <code>str</code> <p>Custom name for border identifier column. Defaults to '{area_column}_border'</p> <code>None</code> <code>source_area_identifier</code> <code>str</code> <p>Custom name for source area column. Defaults to '{area_column}_from'</p> <code>None</code> <code>target_area_identifier</code> <code>str</code> <p>Custom name for target area column. Defaults to '{area_column}_to'</p> <code>None</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Standard naming\n&gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n&gt;&gt;&gt; print(conventions.border_identifier)  # 'country_border'\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Custom naming\n&gt;&gt;&gt; conventions = AreaBorderNamingConventions(\n...     'bidding_zone',\n...     border_identifier='interconnection',\n...     source_area_identifier='origin_zone'\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def __init__(\n        self,\n        area_column: str,\n        border_identifier: str = None,\n        source_area_identifier: str = None,\n        target_area_identifier: str = None,\n):\n    \"\"\"Initialize border naming conventions.\n\n    Args:\n        area_column: Name of the area column (e.g., 'country', 'bidding_zone')\n        border_identifier: Custom name for border identifier column.\n            Defaults to '{area_column}_border'\n        source_area_identifier: Custom name for source area column.\n            Defaults to '{area_column}_from'\n        target_area_identifier: Custom name for target area column.\n            Defaults to '{area_column}_to'\n\n    Example:\n\n        &gt;&gt;&gt; # Standard naming\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n        &gt;&gt;&gt; print(conventions.border_identifier)  # 'country_border'\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Custom naming\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions(\n        ...     'bidding_zone',\n        ...     border_identifier='interconnection',\n        ...     source_area_identifier='origin_zone'\n        ... )\n    \"\"\"\n    self.area_column = area_column\n    self.border_identifier = border_identifier or self._default_border_identifier()\n    self.source_area_identifier = source_area_identifier or self._default_source_area_identifier()\n    self.target_area_identifier = target_area_identifier or self._default_target_area_identifier()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_generator.AreaBorderNamingConventions.get_area_border_name","title":"get_area_border_name","text":"<pre><code>get_area_border_name(area_from: str, area_to: str) -&gt; str\n</code></pre> <p>Generate standardized border name from source and target areas.</p> <p>Parameters:</p> Name Type Description Default <code>area_from</code> <code>str</code> <p>Source area identifier (e.g., 'DE', 'FR_North')</p> required <code>area_to</code> <code>str</code> <p>Target area identifier (e.g., 'FR', 'DE_South')</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted border name using the configured separator (e.g. 'DE - FR', 'FR_North - DE_South')</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n&gt;&gt;&gt; border_name = conventions.get_area_border_name('DE', 'FR')\n&gt;&gt;&gt; print(border_name)  # 'DE - FR'\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def get_area_border_name(self, area_from: str, area_to: str) -&gt; str:\n    \"\"\"Generate standardized border name from source and target areas.\n\n    Args:\n        area_from: Source area identifier (e.g., 'DE', 'FR_North')\n        area_to: Target area identifier (e.g., 'FR', 'DE_South')\n\n    Returns:\n        str: Formatted border name using the configured separator (e.g. 'DE - FR', 'FR_North - DE_South')\n\n    Example:\n\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n        &gt;&gt;&gt; border_name = conventions.get_area_border_name('DE', 'FR')\n        &gt;&gt;&gt; print(border_name)  # 'DE - FR'\n    \"\"\"\n    return f'{area_from}{self.JOIN_AREA_NAMES_BY}{area_to}'\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_generator.AreaBorderNamingConventions.decompose_area_border_name_to_areas","title":"decompose_area_border_name_to_areas","text":"<pre><code>decompose_area_border_name_to_areas(border_name: str) -&gt; Tuple[str, str]\n</code></pre> <p>Extract source and target area names from border identifier.</p> <p>Parameters:</p> Name Type Description Default <code>border_name</code> <code>str</code> <p>Border name in standard format (e.g., 'DE - FR')</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Tuple[str, str]: Source and target area names</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If border_name doesn't contain the expected separator</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n&gt;&gt;&gt; area_from, area_to = conventions.decompose_area_border_name_to_areas('DE - FR')\n&gt;&gt;&gt; print(f\"From: {area_from}, To: {area_to}\")  # From: DE, To: FR\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def decompose_area_border_name_to_areas(self, border_name: str) -&gt; Tuple[str, str]:\n    \"\"\"Extract source and target area names from border identifier.\n\n    Args:\n        border_name: Border name in standard format (e.g., 'DE - FR')\n\n    Returns:\n        Tuple[str, str]: Source and target area names\n\n    Raises:\n        ValueError: If border_name doesn't contain the expected separator\n\n    Example:\n\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n        &gt;&gt;&gt; area_from, area_to = conventions.decompose_area_border_name_to_areas('DE - FR')\n        &gt;&gt;&gt; print(f\"From: {area_from}, To: {area_to}\")  # From: DE, To: FR\n    \"\"\"\n    area_from, area_to = border_name.split(self.JOIN_AREA_NAMES_BY)\n    return area_from, area_to\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_generator.AreaBorderNamingConventions.get_opposite_area_border_name","title":"get_opposite_area_border_name","text":"<pre><code>get_opposite_area_border_name(border_name: str) -&gt; str\n</code></pre> <p>Generate the opposite direction border name.</p> <p>Parameters:</p> Name Type Description Default <code>border_name</code> <code>str</code> <p>Original border name (e.g., 'DE - FR')</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Opposite direction border name (e.g., 'FR - DE')</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n&gt;&gt;&gt; opposite = conventions.get_opposite_area_border_name('DE - FR')\n&gt;&gt;&gt; print(opposite)  # 'FR - DE'\n</code></pre> Energy Domain Context <p>Energy flows and capacities are often directional, requiring tracking of both A\u2192B and B\u2192A relationships for comprehensive border analysis.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def get_opposite_area_border_name(self, border_name: str) -&gt; str:\n    \"\"\"Generate the opposite direction border name.\n\n    Args:\n        border_name: Original border name (e.g., 'DE - FR')\n\n    Returns:\n        str: Opposite direction border name (e.g., 'FR - DE')\n\n    Example:\n\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n        &gt;&gt;&gt; opposite = conventions.get_opposite_area_border_name('DE - FR')\n        &gt;&gt;&gt; print(opposite)  # 'FR - DE'\n\n    Energy Domain Context:\n        Energy flows and capacities are often directional, requiring\n        tracking of both A\u2192B and B\u2192A relationships for comprehensive\n        border analysis.\n    \"\"\"\n    area_from, area_to = self.decompose_area_border_name_to_areas(border_name)\n    return self.get_area_border_name(area_to, area_from)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_generator.AreaBorderNamingConventions.get_alphabetically_sorted_border","title":"get_alphabetically_sorted_border","text":"<pre><code>get_alphabetically_sorted_border(border_name: str) -&gt; str\n</code></pre> <p>Generate alphabetically sorted canonical border name.</p> <p>Creates a canonical representation where area names are sorted alphabetically, useful for identifying unique borders regardless of direction specification, or for matching borders of opposite direction.</p> <p>Parameters:</p> Name Type Description Default <code>border_name</code> <code>str</code> <p>Border name in any direction (e.g., 'FR - DE' or 'DE - FR')</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Alphabetically sorted border name (e.g., 'DE - FR')</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n&gt;&gt;&gt; sorted_border = conventions.get_alphabetically_sorted_border('FR - DE')\n&gt;&gt;&gt; print(sorted_border)  # 'DE - FR'\n</code></pre> Use Case <p>Canonical naming is essential for border deduplication and consistent reference in energy system databases and analysis.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def get_alphabetically_sorted_border(self, border_name: str) -&gt; str:\n    \"\"\"Generate alphabetically sorted canonical border name.\n\n    Creates a canonical representation where area names are sorted\n    alphabetically, useful for identifying unique borders regardless\n    of direction specification, or for matching borders of opposite direction.\n\n    Args:\n        border_name: Border name in any direction (e.g., 'FR - DE' or 'DE - FR')\n\n    Returns:\n        str: Alphabetically sorted border name (e.g., 'DE - FR')\n\n    Example:\n\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n        &gt;&gt;&gt; sorted_border = conventions.get_alphabetically_sorted_border('FR - DE')\n        &gt;&gt;&gt; print(sorted_border)  # 'DE - FR'\n\n    Use Case:\n        Canonical naming is essential for border deduplication and\n        consistent reference in energy system databases and analysis.\n    \"\"\"\n    area_from, area_to = self.decompose_area_border_name_to_areas(border_name)\n    return self.get_area_border_name(*list(sorted([area_from, area_to])))\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_generator.AreaBorderModelGenerator","title":"AreaBorderModelGenerator","text":"<p>               Bases: <code>AreaBorderNamingConventions</code></p> <p>Generates comprehensive border models from energy system topology.</p> <p>This class analyzes line connectivity and node-to-area mappings to automatically identify borders between energy system areas. It creates a comprehensive border_model_df with standardized naming, directional relationships, and integration points for geometric analysis.</p> <p>The generator processes line topology data to identify cross-area connections. It supports bidirectional relationship tracking and provides network graph representations for connectivity analysis.</p> Key Features <ul> <li>Automatic border discovery from line topology</li> <li>Bidirectional border relationship management</li> <li>Standardized naming conventions with configurable patterns</li> <li>Network graph generation for connectivity analysis</li> <li>Integration with geometric border calculators</li> <li>Support for different area granularities (countries, bidding zones, etc.)</li> </ul> MESCAL Integration <p>Designed to work with MESCAL's area accounting system, providing border modeling capabilities that integrate with flow calculators, capacity analyzers, and visualization tools.</p> <p>Attributes:</p> Name Type Description <code>line_model_df</code> <code>DataFrame</code> <p>Transmission line data with topology information</p> <code>node_model_df</code> <code>DataFrame</code> <p>Node data with area assignments</p> <code>node_from_col</code> <code>str</code> <p>Column name for line source nodes</p> <code>node_to_col</code> <code>str</code> <p>Column name for line target nodes</p> <code>node_to_area_map</code> <code>dict</code> <p>Mapping from nodes to their assigned areas</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Create border model from transmission data\n&gt;&gt;&gt; generator = AreaBorderModelGenerator(\n...     node_df, line_df, 'country', 'node_from', 'node_to'\n... )\n&gt;&gt;&gt; border_model = generator.generate_area_border_model()\n&gt;&gt;&gt; print(f\"Found {len(border_model)} directional borders\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>class AreaBorderModelGenerator(AreaBorderNamingConventions):\n    \"\"\"Generates comprehensive border models from energy system topology.\n\n    This class analyzes line connectivity and node-to-area mappings\n    to automatically identify borders between energy system areas. It creates\n    a comprehensive border_model_df with standardized naming, directional relationships,\n    and integration points for geometric analysis.\n\n    The generator processes line topology data to identify cross-area connections.\n    It supports bidirectional relationship tracking and provides network graph\n    representations for connectivity analysis.\n\n    Key Features:\n        - Automatic border discovery from line topology\n        - Bidirectional border relationship management\n        - Standardized naming conventions with configurable patterns\n        - Network graph generation for connectivity analysis\n        - Integration with geometric border calculators\n        - Support for different area granularities (countries, bidding zones, etc.)\n\n    MESCAL Integration:\n        Designed to work with MESCAL's area accounting system, providing\n        border modeling capabilities that integrate with flow calculators,\n        capacity analyzers, and visualization tools.\n\n    Attributes:\n        line_model_df (pd.DataFrame): Transmission line data with topology information\n        node_model_df (pd.DataFrame): Node data with area assignments\n        node_from_col (str): Column name for line source nodes\n        node_to_col (str): Column name for line target nodes\n        node_to_area_map (dict): Mapping from nodes to their assigned areas\n\n    Example:\n\n        &gt;&gt;&gt; # Create border model from transmission data\n        &gt;&gt;&gt; generator = AreaBorderModelGenerator(\n        ...     node_df, line_df, 'country', 'node_from', 'node_to'\n        ... )\n        &gt;&gt;&gt; border_model = generator.generate_area_border_model()\n        &gt;&gt;&gt; print(f\"Found {len(border_model)} directional borders\")\n    \"\"\"\n\n    def __init__(\n        self, \n        node_model_df: pd.DataFrame,\n        line_model_df: pd.DataFrame,\n        area_column: str,\n        node_from_col: str,\n        node_to_col: str,\n        border_identifier: str = None,\n        source_area_identifier: str = None,\n        target_area_identifier: str = None,\n    ):\n        \"\"\"Initialize the area border model generator.\n\n        Args:\n            node_model_df: DataFrame containing node-level data with area assignments.\n                Must contain area_column with area identifiers for each node.\n            line_model_df: DataFrame containing transmission line topology data.\n                Must contain node_from_col and node_to_col with node identifiers.\n            area_column: Column name in node_model_df containing area assignments\n                (e.g., 'country', 'bidding_zone', 'market_region')\n            node_from_col: Column name in line_model_df for source node identifiers\n            node_to_col: Column name in line_model_df for target node identifiers\n            border_identifier: Custom border column name (optional)\n            source_area_identifier: Custom source area column name (optional)\n            target_area_identifier: Custom target area column name (optional)\n\n        Raises:\n            ValueError: If required columns are not found in input DataFrames\n\n        Example:\n\n            &gt;&gt;&gt; generator = AreaBorderModelGenerator(\n            ...     nodes_df=node_data,\n            ...     lines_df=transmission_data,\n            ...     area_column='bidding_zone',\n            ...     node_from_col='bus_from',\n            ...     node_to_col='bus_to'\n            ... )\n        \"\"\"\n        super().__init__(area_column, border_identifier, source_area_identifier, target_area_identifier)\n        self.line_model_df = line_model_df\n        self.node_model_df = node_model_df\n        self.node_from_col = node_from_col\n        self.node_to_col = node_to_col\n\n        self._validate_inputs()\n        self.node_to_area_map = self._create_node_to_area_map()\n\n    def _validate_inputs(self):\n        if self.area_column not in self.node_model_df.columns:\n            raise ValueError(\n                f\"Area column '{self.area_column}' not found in node_model_df. \"\n                f\"Available columns: {list(self.node_model_df.columns)}\"\n            )\n        if self.node_from_col not in self.line_model_df.columns:\n            raise ValueError(\n                f\"Source node column '{self.node_from_col}' not found in line_model_df. \"\n                f\"Available columns: {list(self.line_model_df.columns)}\"\n            )\n        if self.node_to_col not in self.line_model_df.columns:\n            raise ValueError(\n                f\"Target node column '{self.node_to_col}' not found in line_model_df. \"\n                f\"Available columns: {list(self.line_model_df.columns)}\"\n            )\n\n    def _create_node_to_area_map(self) -&gt; dict:\n        \"\"\"Create mapping from node identifiers to their assigned areas.\n\n        Returns:\n            dict: Mapping from node IDs to area assignments\n\n        Note:\n            Nodes with None or NaN area assignments are included in the mapping\n            but will be filtered out during border identification.\n        \"\"\"\n        return self.node_model_df[self.area_column].to_dict()\n\n    def generate_area_border_model(self) -&gt; pd.DataFrame:\n        \"\"\"Generate comprehensive border model with all relationship data.\n\n        Analyzes transmission line topology to identify borders between areas,\n        creating a comprehensive DataFrame with directional relationships,\n        naming conventions, and reference data for further analysis.\n\n        The generated model includes:\n            - Border identifiers in both directions (A\u2192B and B\u2192A)\n            - Source and target area columns\n            - Opposite border references for bidirectional analysis\n            - Alphabetically sorted canonical border names\n            - Boolean indicators for alphabetical sorting\n\n        Returns:\n            pd.DataFrame: Comprehensive border model indexed by border identifiers.\n                Returns empty DataFrame with proper column structure if no borders found.\n\n        Example:\n\n            &gt;&gt;&gt; border_model = generator.generate_area_border_model()\n            &gt;&gt;&gt; print(border_model.columns)\n            ['country_from', 'country_to', 'opposite_border', 'sorted_border', 'name_is_alphabetically_sorted']\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Access border relationships\n            &gt;&gt;&gt; for border_id, row in border_model.iterrows():\n            ...     print(f\"{border_id}: {row['country_from']} \u2192 {row['country_to']}\")\n        \"\"\"\n        borders = self._identify_borders()\n\n        if not borders:\n            return pd.DataFrame(\n                columns=[\n                    self.border_identifier,\n                    self.source_area_identifier,\n                    self.target_area_identifier,\n                    self.OPPOSITE_BORDER_IDENTIFIER,\n                    self.SORTED_BORDER_IDENTIFIER,\n                    self.NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER,\n                ]\n            )\n\n        border_data = []\n        for area_from, area_to in borders:\n            border_id = self.get_area_border_name(area_from, area_to)\n            opposite_id = self.get_opposite_area_border_name(border_id)\n\n            sorted_border = self.get_alphabetically_sorted_border(border_id)\n\n            border_data.append({\n                self.border_identifier: border_id,\n                self.source_area_identifier: area_from,\n                self.target_area_identifier: area_to,\n                self.OPPOSITE_BORDER_IDENTIFIER: opposite_id,\n                self.SORTED_BORDER_IDENTIFIER: sorted_border,\n                self.NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER: sorted_border == border_id,\n            })\n\n        border_model_df = pd.DataFrame(border_data).set_index(self.border_identifier)\n\n        return border_model_df\n\n    def _identify_borders(self) -&gt; set[tuple[str, str]]:\n        \"\"\"Identify borders from line topology.\n\n        Analyzes line connectivity to find areas that are connected by\n        lines, creating bidirectional border relationships.\n\n        Returns:\n            set: Set of (area_from, area_to) tuples representing directional borders.\n                Includes both directions for each physical connection.\n\n        Note:\n            - Lines connecting nodes within the same area are ignored\n            - Lines with nodes having None/NaN area assignments are ignored\n            - Both directions (A\u2192B and B\u2192A) are included for each connection\n        \"\"\"\n        borders = set()\n\n        for _, line in self.line_model_df.iterrows():\n            node_from = line[self.node_from_col]\n            node_to = line[self.node_to_col]\n\n            area_from = self.node_to_area_map.get(node_from)\n            area_to = self.node_to_area_map.get(node_to)\n\n            if area_from and area_to and area_from != area_to:\n                borders.add((area_from, area_to))\n                borders.add((area_to, area_from))\n\n        return borders\n\n    def _get_lines_for_border(self, area_from: str, area_to: str) -&gt; list[str]:\n        \"\"\"Get all lines that cross a specific directional border.\n\n        Args:\n            area_from: Source area identifier\n            area_to: Target area identifier\n\n        Returns:\n            list[str]: List of line identifiers that connect the specified areas\n                in the given direction\n\n        Example:\n\n            &gt;&gt;&gt; lines = generator._get_lines_for_border('DE', 'FR')\n            &gt;&gt;&gt; print(f\"Lines from DE to FR: {lines}\")\n        \"\"\"\n        lines = []\n\n        for line_id, line in self.line_model_df.iterrows():\n            node_from = line[self.node_from_col]\n            node_to = line[self.node_to_col]\n\n            node_area_from = self.node_to_area_map.get(node_from)\n            node_area_to = self.node_to_area_map.get(node_to)\n\n            if node_area_from == area_from and node_area_to == area_to:\n                lines.append(line_id)\n\n        return lines\n\n    def get_area_graph(self) -&gt; nx.Graph:\n        \"\"\"Generate NetworkX graph representation of area connectivity.\n\n        Creates an undirected graph where nodes represent areas and edges\n        represent borders. This is useful for network analysis, path finding,\n        and connectivity studies in multi-area energy systems.\n\n        Returns:\n            nx.Graph: Undirected graph with areas as nodes and borders as edges.\n                Graph may contain multiple disconnected components if areas\n                are not fully interconnected.\n\n        Example:\n\n            &gt;&gt;&gt; graph = generator.get_area_graph()\n            &gt;&gt;&gt; print(f\"Areas: {list(graph.nodes())}\")\n            &gt;&gt;&gt; print(f\"Borders: {list(graph.edges())}\")\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Check connectivity\n            &gt;&gt;&gt; connected = nx.is_connected(graph)\n            &gt;&gt;&gt; print(f\"All areas connected: {connected}\")\n        \"\"\"\n        graph = nx.Graph()\n        borders = self._identify_borders()\n\n        for area_from, area_to in borders:\n            if not graph.has_edge(area_from, area_to):\n                graph.add_edge(area_from, area_to)\n\n        return graph\n\n    def enhance_with_geometry(\n        self, \n        border_model_df: pd.DataFrame,\n        area_geometry_calculator: AreaBorderGeometryCalculator\n    ) -&gt; pd.DataFrame:\n        \"\"\"Enhance border model with geometric properties for visualization.\n\n        Integrates with AreaBorderGeometryCalculator to add geometric information\n        to borders, including representative points, directional angles, and\n        line geometries. This enables advanced visualization of energy system borders.\n\n        Args:\n            border_model_df: Border model DataFrame to enhance\n            area_geometry_calculator: Configured geometry calculator with area\n                polygon data for geometric computations\n\n        Returns:\n            pd.DataFrame: Enhanced border model with additional geometric columns:\n                - projection_point: Point for label/arrow placement\n                - azimuth_angle: Directional angle in degrees\n                - is_physical: Boolean indicating if border is physical (touching areas)\n                - geo_line_string: LineString geometry representing the border\n\n        Example:\n\n            &gt;&gt;&gt; # Setup geometry calculator with area polygons\n            &gt;&gt;&gt; geo_calc = AreaBorderGeometryCalculator(area_polygons_gdf)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Enhance border model\n            &gt;&gt;&gt; enhanced_borders = generator.enhance_with_geometry(border_model, geo_calc)\n            &gt;&gt;&gt; print(enhanced_borders.columns)  # Includes geometric properties\n\n        Note:\n            Geometric enhancement may fail for some borders due to missing\n            area geometries or calculation errors. Such failures are logged\n            as warnings without stopping the overall process.\n        \"\"\"\n        enhanced_df = border_model_df.copy()\n\n        for border_id, border in border_model_df.iterrows():\n            area_from = border[self.source_area_identifier]\n            area_to = border[self.target_area_identifier]\n\n            try:\n                geometry_info = area_geometry_calculator.calculate_border_geometry(\n                    area_from, area_to\n                )\n\n                enhanced_df.loc[border_id, self.PROJECTION_POINT_IDENTIFIER] = geometry_info[area_geometry_calculator.PROJECTION_POINT_IDENTIFIER]\n                enhanced_df.loc[border_id, self.AZIMUTH_ANGLE_IDENTIFIER] = geometry_info[area_geometry_calculator.AZIMUTH_ANGLE_IDENTIFIER]\n                enhanced_df.loc[border_id, self.BORDER_IS_PHYSICAL_IDENTIFIER] = geometry_info[area_geometry_calculator.BORDER_IS_PHYSICAL_IDENTIFIER]\n                enhanced_df.loc[border_id, self.BORDER_LINE_STRING_IDENTIFIER] = geometry_info[area_geometry_calculator.BORDER_LINE_STRING_IDENTIFIER]\n\n            except Exception as e:\n                print(f\"Warning: Could not calculate geometry for border {border_id} \"\n                      f\"({area_from} \u2192 {area_to}): {e}\")\n\n        return enhanced_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_generator.AreaBorderModelGenerator.__init__","title":"__init__","text":"<pre><code>__init__(node_model_df: DataFrame, line_model_df: DataFrame, area_column: str, node_from_col: str, node_to_col: str, border_identifier: str = None, source_area_identifier: str = None, target_area_identifier: str = None)\n</code></pre> <p>Initialize the area border model generator.</p> <p>Parameters:</p> Name Type Description Default <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame containing node-level data with area assignments. Must contain area_column with area identifiers for each node.</p> required <code>line_model_df</code> <code>DataFrame</code> <p>DataFrame containing transmission line topology data. Must contain node_from_col and node_to_col with node identifiers.</p> required <code>area_column</code> <code>str</code> <p>Column name in node_model_df containing area assignments (e.g., 'country', 'bidding_zone', 'market_region')</p> required <code>node_from_col</code> <code>str</code> <p>Column name in line_model_df for source node identifiers</p> required <code>node_to_col</code> <code>str</code> <p>Column name in line_model_df for target node identifiers</p> required <code>border_identifier</code> <code>str</code> <p>Custom border column name (optional)</p> <code>None</code> <code>source_area_identifier</code> <code>str</code> <p>Custom source area column name (optional)</p> <code>None</code> <code>target_area_identifier</code> <code>str</code> <p>Custom target area column name (optional)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are not found in input DataFrames</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; generator = AreaBorderModelGenerator(\n...     nodes_df=node_data,\n...     lines_df=transmission_data,\n...     area_column='bidding_zone',\n...     node_from_col='bus_from',\n...     node_to_col='bus_to'\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def __init__(\n    self, \n    node_model_df: pd.DataFrame,\n    line_model_df: pd.DataFrame,\n    area_column: str,\n    node_from_col: str,\n    node_to_col: str,\n    border_identifier: str = None,\n    source_area_identifier: str = None,\n    target_area_identifier: str = None,\n):\n    \"\"\"Initialize the area border model generator.\n\n    Args:\n        node_model_df: DataFrame containing node-level data with area assignments.\n            Must contain area_column with area identifiers for each node.\n        line_model_df: DataFrame containing transmission line topology data.\n            Must contain node_from_col and node_to_col with node identifiers.\n        area_column: Column name in node_model_df containing area assignments\n            (e.g., 'country', 'bidding_zone', 'market_region')\n        node_from_col: Column name in line_model_df for source node identifiers\n        node_to_col: Column name in line_model_df for target node identifiers\n        border_identifier: Custom border column name (optional)\n        source_area_identifier: Custom source area column name (optional)\n        target_area_identifier: Custom target area column name (optional)\n\n    Raises:\n        ValueError: If required columns are not found in input DataFrames\n\n    Example:\n\n        &gt;&gt;&gt; generator = AreaBorderModelGenerator(\n        ...     nodes_df=node_data,\n        ...     lines_df=transmission_data,\n        ...     area_column='bidding_zone',\n        ...     node_from_col='bus_from',\n        ...     node_to_col='bus_to'\n        ... )\n    \"\"\"\n    super().__init__(area_column, border_identifier, source_area_identifier, target_area_identifier)\n    self.line_model_df = line_model_df\n    self.node_model_df = node_model_df\n    self.node_from_col = node_from_col\n    self.node_to_col = node_to_col\n\n    self._validate_inputs()\n    self.node_to_area_map = self._create_node_to_area_map()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_generator.AreaBorderModelGenerator.generate_area_border_model","title":"generate_area_border_model","text":"<pre><code>generate_area_border_model() -&gt; DataFrame\n</code></pre> <p>Generate comprehensive border model with all relationship data.</p> <p>Analyzes transmission line topology to identify borders between areas, creating a comprehensive DataFrame with directional relationships, naming conventions, and reference data for further analysis.</p> The generated model includes <ul> <li>Border identifiers in both directions (A\u2192B and B\u2192A)</li> <li>Source and target area columns</li> <li>Opposite border references for bidirectional analysis</li> <li>Alphabetically sorted canonical border names</li> <li>Boolean indicators for alphabetical sorting</li> </ul> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Comprehensive border model indexed by border identifiers. Returns empty DataFrame with proper column structure if no borders found.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; border_model = generator.generate_area_border_model()\n&gt;&gt;&gt; print(border_model.columns)\n['country_from', 'country_to', 'opposite_border', 'sorted_border', 'name_is_alphabetically_sorted']\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Access border relationships\n&gt;&gt;&gt; for border_id, row in border_model.iterrows():\n...     print(f\"{border_id}: {row['country_from']} \u2192 {row['country_to']}\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def generate_area_border_model(self) -&gt; pd.DataFrame:\n    \"\"\"Generate comprehensive border model with all relationship data.\n\n    Analyzes transmission line topology to identify borders between areas,\n    creating a comprehensive DataFrame with directional relationships,\n    naming conventions, and reference data for further analysis.\n\n    The generated model includes:\n        - Border identifiers in both directions (A\u2192B and B\u2192A)\n        - Source and target area columns\n        - Opposite border references for bidirectional analysis\n        - Alphabetically sorted canonical border names\n        - Boolean indicators for alphabetical sorting\n\n    Returns:\n        pd.DataFrame: Comprehensive border model indexed by border identifiers.\n            Returns empty DataFrame with proper column structure if no borders found.\n\n    Example:\n\n        &gt;&gt;&gt; border_model = generator.generate_area_border_model()\n        &gt;&gt;&gt; print(border_model.columns)\n        ['country_from', 'country_to', 'opposite_border', 'sorted_border', 'name_is_alphabetically_sorted']\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Access border relationships\n        &gt;&gt;&gt; for border_id, row in border_model.iterrows():\n        ...     print(f\"{border_id}: {row['country_from']} \u2192 {row['country_to']}\")\n    \"\"\"\n    borders = self._identify_borders()\n\n    if not borders:\n        return pd.DataFrame(\n            columns=[\n                self.border_identifier,\n                self.source_area_identifier,\n                self.target_area_identifier,\n                self.OPPOSITE_BORDER_IDENTIFIER,\n                self.SORTED_BORDER_IDENTIFIER,\n                self.NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER,\n            ]\n        )\n\n    border_data = []\n    for area_from, area_to in borders:\n        border_id = self.get_area_border_name(area_from, area_to)\n        opposite_id = self.get_opposite_area_border_name(border_id)\n\n        sorted_border = self.get_alphabetically_sorted_border(border_id)\n\n        border_data.append({\n            self.border_identifier: border_id,\n            self.source_area_identifier: area_from,\n            self.target_area_identifier: area_to,\n            self.OPPOSITE_BORDER_IDENTIFIER: opposite_id,\n            self.SORTED_BORDER_IDENTIFIER: sorted_border,\n            self.NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER: sorted_border == border_id,\n        })\n\n    border_model_df = pd.DataFrame(border_data).set_index(self.border_identifier)\n\n    return border_model_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_generator.AreaBorderModelGenerator.get_area_graph","title":"get_area_graph","text":"<pre><code>get_area_graph() -&gt; Graph\n</code></pre> <p>Generate NetworkX graph representation of area connectivity.</p> <p>Creates an undirected graph where nodes represent areas and edges represent borders. This is useful for network analysis, path finding, and connectivity studies in multi-area energy systems.</p> <p>Returns:</p> Type Description <code>Graph</code> <p>nx.Graph: Undirected graph with areas as nodes and borders as edges. Graph may contain multiple disconnected components if areas are not fully interconnected.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; graph = generator.get_area_graph()\n&gt;&gt;&gt; print(f\"Areas: {list(graph.nodes())}\")\n&gt;&gt;&gt; print(f\"Borders: {list(graph.edges())}\")\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Check connectivity\n&gt;&gt;&gt; connected = nx.is_connected(graph)\n&gt;&gt;&gt; print(f\"All areas connected: {connected}\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def get_area_graph(self) -&gt; nx.Graph:\n    \"\"\"Generate NetworkX graph representation of area connectivity.\n\n    Creates an undirected graph where nodes represent areas and edges\n    represent borders. This is useful for network analysis, path finding,\n    and connectivity studies in multi-area energy systems.\n\n    Returns:\n        nx.Graph: Undirected graph with areas as nodes and borders as edges.\n            Graph may contain multiple disconnected components if areas\n            are not fully interconnected.\n\n    Example:\n\n        &gt;&gt;&gt; graph = generator.get_area_graph()\n        &gt;&gt;&gt; print(f\"Areas: {list(graph.nodes())}\")\n        &gt;&gt;&gt; print(f\"Borders: {list(graph.edges())}\")\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Check connectivity\n        &gt;&gt;&gt; connected = nx.is_connected(graph)\n        &gt;&gt;&gt; print(f\"All areas connected: {connected}\")\n    \"\"\"\n    graph = nx.Graph()\n    borders = self._identify_borders()\n\n    for area_from, area_to in borders:\n        if not graph.has_edge(area_from, area_to):\n            graph.add_edge(area_from, area_to)\n\n    return graph\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_generator.AreaBorderModelGenerator.enhance_with_geometry","title":"enhance_with_geometry","text":"<pre><code>enhance_with_geometry(border_model_df: DataFrame, area_geometry_calculator: AreaBorderGeometryCalculator) -&gt; DataFrame\n</code></pre> <p>Enhance border model with geometric properties for visualization.</p> <p>Integrates with AreaBorderGeometryCalculator to add geometric information to borders, including representative points, directional angles, and line geometries. This enables advanced visualization of energy system borders.</p> <p>Parameters:</p> Name Type Description Default <code>border_model_df</code> <code>DataFrame</code> <p>Border model DataFrame to enhance</p> required <code>area_geometry_calculator</code> <code>AreaBorderGeometryCalculator</code> <p>Configured geometry calculator with area polygon data for geometric computations</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Enhanced border model with additional geometric columns: - projection_point: Point for label/arrow placement - azimuth_angle: Directional angle in degrees - is_physical: Boolean indicating if border is physical (touching areas) - geo_line_string: LineString geometry representing the border</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Setup geometry calculator with area polygons\n&gt;&gt;&gt; geo_calc = AreaBorderGeometryCalculator(area_polygons_gdf)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Enhance border model\n&gt;&gt;&gt; enhanced_borders = generator.enhance_with_geometry(border_model, geo_calc)\n&gt;&gt;&gt; print(enhanced_borders.columns)  # Includes geometric properties\n</code></pre> Note <p>Geometric enhancement may fail for some borders due to missing area geometries or calculation errors. Such failures are logged as warnings without stopping the overall process.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def enhance_with_geometry(\n    self, \n    border_model_df: pd.DataFrame,\n    area_geometry_calculator: AreaBorderGeometryCalculator\n) -&gt; pd.DataFrame:\n    \"\"\"Enhance border model with geometric properties for visualization.\n\n    Integrates with AreaBorderGeometryCalculator to add geometric information\n    to borders, including representative points, directional angles, and\n    line geometries. This enables advanced visualization of energy system borders.\n\n    Args:\n        border_model_df: Border model DataFrame to enhance\n        area_geometry_calculator: Configured geometry calculator with area\n            polygon data for geometric computations\n\n    Returns:\n        pd.DataFrame: Enhanced border model with additional geometric columns:\n            - projection_point: Point for label/arrow placement\n            - azimuth_angle: Directional angle in degrees\n            - is_physical: Boolean indicating if border is physical (touching areas)\n            - geo_line_string: LineString geometry representing the border\n\n    Example:\n\n        &gt;&gt;&gt; # Setup geometry calculator with area polygons\n        &gt;&gt;&gt; geo_calc = AreaBorderGeometryCalculator(area_polygons_gdf)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Enhance border model\n        &gt;&gt;&gt; enhanced_borders = generator.enhance_with_geometry(border_model, geo_calc)\n        &gt;&gt;&gt; print(enhanced_borders.columns)  # Includes geometric properties\n\n    Note:\n        Geometric enhancement may fail for some borders due to missing\n        area geometries or calculation errors. Such failures are logged\n        as warnings without stopping the overall process.\n    \"\"\"\n    enhanced_df = border_model_df.copy()\n\n    for border_id, border in border_model_df.iterrows():\n        area_from = border[self.source_area_identifier]\n        area_to = border[self.target_area_identifier]\n\n        try:\n            geometry_info = area_geometry_calculator.calculate_border_geometry(\n                area_from, area_to\n            )\n\n            enhanced_df.loc[border_id, self.PROJECTION_POINT_IDENTIFIER] = geometry_info[area_geometry_calculator.PROJECTION_POINT_IDENTIFIER]\n            enhanced_df.loc[border_id, self.AZIMUTH_ANGLE_IDENTIFIER] = geometry_info[area_geometry_calculator.AZIMUTH_ANGLE_IDENTIFIER]\n            enhanced_df.loc[border_id, self.BORDER_IS_PHYSICAL_IDENTIFIER] = geometry_info[area_geometry_calculator.BORDER_IS_PHYSICAL_IDENTIFIER]\n            enhanced_df.loc[border_id, self.BORDER_LINE_STRING_IDENTIFIER] = geometry_info[area_geometry_calculator.BORDER_LINE_STRING_IDENTIFIER]\n\n        except Exception as e:\n            print(f\"Warning: Could not calculate geometry for border {border_id} \"\n                  f\"({area_from} \u2192 {area_to}): {e}\")\n\n    return enhanced_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator","title":"AreaBorderGeometryCalculator","text":"<p>               Bases: <code>GeoModelGeneratorBase</code></p> <p>Advanced geometric calculator for energy system area border analysis.</p> <p>This class provides sophisticated geometric calculations for borders between energy system areas, handling both physical borders (adjacent areas sharing geographic boundaries) and logical borders (non-adjacent areas requiring connection paths, e.g. through the sea). It's specifically designed to generate properties for energy market cross-border visualizations.</p> <p>The calculator combines multiple geometric algorithms: - Physical border extraction using geometric intersection - Logical geo-line-border path finding with obstacle avoidance - Representative point computation using pole of inaccessibility for label placements on maps - Azimuth angle calculation for flow icon (arrow) visualization - Geometric validation and optimization</p> Key Features <ul> <li>Automatic detection of physical vs logical borders</li> <li>Optimal path finding for non-crossing connections</li> <li>Representative point calculation for label placement</li> <li>Directional angle computation for arrow orientation</li> <li>Performance optimization with geometric caching</li> <li>Integration with MESCAL area accounting workflows</li> </ul> Energy Domain Applications <ul> <li>Visualization of cross-border (cross-country, cross-biddingzone, cross-macroregion) variables (flows, spreads, capacities, ...)</li> </ul> <p>Attributes:</p> Name Type Description <code>area_model_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with area polygon geometries</p> <code>non_crossing_path_finder</code> <code>NonCrossingPathFinder</code> <p>Path optimization engine</p> <code>_centroid_cache</code> <code>dict</code> <p>Cached representative points for performance</p> <code>_line_cache</code> <code>dict</code> <p>Cached border lines for repeated calculations</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; from shapely.geometry import box\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Setup area geometries\n&gt;&gt;&gt; areas = gpd.GeoDataFrame({\n...     'geometry': [box(0, 0, 1, 1), box(2, 0, 3, 1)]  # Two separate areas\n... }, index=['Area_A', 'Area_B'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Calculate border geometry\n&gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas)\n&gt;&gt;&gt; border_info = calculator.calculate_border_geometry('Area_A', 'Area_B')\n&gt;&gt;&gt; print(f\"Border type: {'Physical' if border_info['is_physical'] else 'Logical'}\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>class AreaBorderGeometryCalculator(GeoModelGeneratorBase):\n    \"\"\"Advanced geometric calculator for energy system area border analysis.\n\n    This class provides sophisticated geometric calculations for borders between\n    energy system areas, handling both physical borders (adjacent areas sharing\n    geographic boundaries) and logical borders (non-adjacent areas requiring\n    connection paths, e.g. through the sea). It's specifically designed to generate\n    properties for energy market cross-border visualizations.\n\n    The calculator combines multiple geometric algorithms:\n    - Physical border extraction using geometric intersection\n    - Logical geo-line-border path finding with obstacle avoidance\n    - Representative point computation using pole of inaccessibility for label placements on maps\n    - Azimuth angle calculation for flow icon (arrow) visualization\n    - Geometric validation and optimization\n\n    Key Features:\n        - Automatic detection of physical vs logical borders\n        - Optimal path finding for non-crossing connections\n        - Representative point calculation for label placement\n        - Directional angle computation for arrow orientation\n        - Performance optimization with geometric caching\n        - Integration with MESCAL area accounting workflows\n\n    Energy Domain Applications:\n        - Visualization of cross-border (cross-country, cross-biddingzone, cross-macroregion) variables (flows, spreads, capacities, ...)\n\n    Attributes:\n        area_model_gdf (gpd.GeoDataFrame): GeoDataFrame with area polygon geometries\n        non_crossing_path_finder (NonCrossingPathFinder): Path optimization engine\n        _centroid_cache (dict): Cached representative points for performance\n        _line_cache (dict): Cached border lines for repeated calculations\n\n    Example:\n\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; from shapely.geometry import box\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Setup area geometries\n        &gt;&gt;&gt; areas = gpd.GeoDataFrame({\n        ...     'geometry': [box(0, 0, 1, 1), box(2, 0, 3, 1)]  # Two separate areas\n        ... }, index=['Area_A', 'Area_B'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Calculate border geometry\n        &gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas)\n        &gt;&gt;&gt; border_info = calculator.calculate_border_geometry('Area_A', 'Area_B')\n        &gt;&gt;&gt; print(f\"Border type: {'Physical' if border_info['is_physical'] else 'Logical'}\")\n    \"\"\"\n\n    PROJECTION_POINT_IDENTIFIER = 'projection_point'\n    AZIMUTH_ANGLE_IDENTIFIER = 'azimuth_angle'\n    BORDER_IS_PHYSICAL_IDENTIFIER = 'is_physical'\n    BORDER_LINE_STRING_IDENTIFIER = 'geo_line_string'\n\n    def __init__(self, area_model_gdf: gpd.GeoDataFrame, non_crossing_path_finder: 'NonCrossingPathFinder' = None):\n        \"\"\"Initialize the border geometry calculator.\n\n        Args:\n            area_model_gdf: GeoDataFrame containing area geometries with polygon\n                boundaries. Index should contain area identifiers (e.g., country codes,\n                bidding zone names). Must contain valid polygon geometries in 'geometry' column.\n            non_crossing_path_finder: Optional custom path finder for logical borders.\n                If None, creates default NonCrossingPathFinder with standard parameters.\n\n        Raises:\n            ValueError: If geometries are invalid or area_model_gdf lacks required structure\n\n        Example:\n\n            &gt;&gt;&gt; areas_gdf = gpd.read_file('countries.geojson').set_index('ISO_A2')\n            &gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas_gdf)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Custom path finder for specific requirements\n            &gt;&gt;&gt; custom_finder = NonCrossingPathFinder(num_points=200, min_clearance=10000)\n            &gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas_gdf, custom_finder)\n\n        Note:\n            Invalid geometries are automatically cleaned using buffer(0) operation.\n            Large area datasets benefit from using projected coordinate systems\n            for accurate geometric calculations.\n        \"\"\"\n        self.area_model_gdf = area_model_gdf\n        self.non_crossing_path_finder = non_crossing_path_finder or NonCrossingPathFinder()\n        self._validate_geometries()\n\n        self._centroid_cache: dict[str, Point] = {}\n        self._line_cache: dict[Tuple[str, str], LineString] = {}\n\n    def _validate_geometries(self):\n        \"\"\"Validate and clean area geometries for reliable calculations.\n\n        Applies buffer(0) operation to fix invalid geometries (self-intersections,\n        unclosed rings, etc.) that could cause calculation failures. This is\n        particularly important for real-world geographic data that may have\n        topology issues.\n\n        Note:\n            The buffer(0) operation is a common technique for fixing invalid\n            polygon geometries without changing their fundamental shape.\n        \"\"\"\n        self.area_model_gdf['geometry'] = self.area_model_gdf['geometry'].apply(\n            lambda geom: geom if geom.is_valid else geom.buffer(0)\n        )\n\n    def calculate_border_geometry(\n        self, \n        area_from: str, \n        area_to: str\n    ) -&gt; dict[str, Union[Point, float, LineString, bool]]:\n        \"\"\"Calculate comprehensive geometric properties for an area border.\n\n        This is the main interface method that computes all geometric properties\n        needed for border visualization and analysis. It automatically detects\n        whether areas are physically adjacent or logically connected and applies\n        appropriate geometric algorithms.\n\n        Processing Logic:\n            1. Detect if areas share physical boundary (touching/intersecting)\n            2. For physical borders: extract shared boundary line\n            3. For logical borders: compute optimal connection path\n            4. Calculate representative point for label/arrow placement\n            5. Compute azimuth angle for arrow icon visualization\n\n        Args:\n            area_from: Source area identifier (must exist in area_model_gdf index)\n            area_to: Target area identifier (must exist in area_model_gdf index)\n\n        Returns:\n            dict: Comprehensive border geometry information containing:\n                - 'projection_point' (Point): Optimal point for label/arrow placement\n                - 'azimuth_angle' (float): Directional angle in degrees (0-360)\n                - 'geo_line_string' (LineString): Border line geometry\n                - 'is_physical' (bool): True for touching areas, False for logical borders\n\n        Raises:\n            KeyError: If area_from or area_to not found in area_model_gdf\n            ValueError: If geometric calculations fail\n\n        Example:\n\n            &gt;&gt;&gt; border_info = calculator.calculate_border_geometry('DE', 'FR')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Use for visualization\n            &gt;&gt;&gt; point = border_info['projection_point']\n            &gt;&gt;&gt; angle = border_info['azimuth_angle']\n            &gt;&gt;&gt; is_physical = border_info['is_physical']\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; print(f\"Border DE\u2192FR: {point} at {angle}\u00b0 ({'physical' if is_physical else 'logical'})\")\n        \"\"\"\n        midpoint, angle = self.get_area_border_midpoint_and_angle(area_from, area_to)\n\n        if self.areas_touch(area_from, area_to) or self.areas_intersect(area_from, area_to):\n            geom_from = self.get_area_geometry(area_from)\n            geom_to = self.get_area_geometry(area_to)\n            border_line = self._get_continuous_border_line(geom_from, geom_to)\n            is_physical = True\n        else:\n            border_line = self.get_straight_line_between_areas(area_from, area_to)\n            is_physical = False\n\n        return {\n            self.PROJECTION_POINT_IDENTIFIER: midpoint,\n            self.AZIMUTH_ANGLE_IDENTIFIER: angle,\n            self.BORDER_LINE_STRING_IDENTIFIER: border_line,\n            self.BORDER_IS_PHYSICAL_IDENTIFIER: is_physical\n        }\n\n    def areas_touch(self, area_from: str, area_to: str) -&gt; bool:\n        \"\"\"Check if two areas share a common physical (geographic) border.\n\n        Uses Shapely's touches() method to determine if area boundaries\n        intersect without overlapping. This is the standard definition\n        of physical adjacency for energy market regions.\n\n        Args:\n            area_from: Source area identifier\n            area_to: Target area identifier\n\n        Returns:\n            bool: True if areas share a common boundary, False otherwise\n\n        Example:\n\n            &gt;&gt;&gt; touching = calculator.areas_touch('DE', 'FR')  # True for neighboring countries\n            &gt;&gt;&gt; separated = calculator.areas_touch('DE', 'GB')  # False for non-adjacent countries\n        \"\"\"\n        geom_from = self.get_area_geometry(area_from)\n        geom_to = self.get_area_geometry(area_to)\n        return geom_from.touches(geom_to)\n\n    def areas_intersect(self, area_from: str, area_to: str) -&gt; bool:\n        \"\"\"Check if two areas have any geometric intersection.\n\n        Uses Shapely's intersects() method to check for any form of geometric\n        intersection, including touching, overlapping, or containment. This is\n        broader than the touches() check and handles edge cases in geographic data.\n\n        Args:\n            area_from: Source area identifier\n            area_to: Target area identifier\n\n        Returns:\n            bool: True if areas have any geometric intersection, False otherwise\n\n        Note:\n            This method is used as a fallback for areas_touch() to handle\n            geographic data with small overlaps or slight topology inconsistencies\n            that are common in real-world boundary datasets.\n        \"\"\"\n        geom_from = self.get_area_geometry(area_from)\n        geom_to = self.get_area_geometry(area_to)\n        return geom_from.intersects(geom_to)\n\n    def get_area_border_midpoint_and_angle(\n        self, \n        area_from: str, \n        area_to: str\n    ) -&gt; tuple[Point, float]:\n        \"\"\"Calculate representative point and directional angle for border.\n\n        Computes the optimal point for placing directional indicators (arrows,\n        labels) and the corresponding angle for proper orientation. The algorithm\n        adapts to both physical and logical borders to ensure optimal placement.\n\n        For Physical Borders:\n            - Uses midpoint of shared boundary line\n            - Angle is perpendicular to boundary, pointing toward target area\n\n        For Logical Borders:\n            - Uses midpoint of optimal connection line\n            - Angle follows connection direction from source to target\n\n        Args:\n            area_from: Source area identifier\n            area_to: Target area identifier\n\n        Returns:\n            tuple[Point, float]: Representative point and directional angle in degrees.\n                Angle range: 0-360 degrees, where 0\u00b0 is North, 90\u00b0 is East.\n\n        Example:\n\n            &gt;&gt;&gt; point, angle = calculator.get_area_border_midpoint_and_angle('DE', 'FR')\n            &gt;&gt;&gt; print(f\"Place arrow at {point} oriented at {angle}\u00b0 for DE\u2192FR flow\")\n        \"\"\"\n        geom_from = self.get_area_geometry(area_from)\n        geom_to = self.get_area_geometry(area_to)\n\n        if self.areas_touch(area_from, area_to) or self.areas_intersect(area_from, area_to):\n            midpoint, angle = self._get_midpoint_and_angle_for_touching_areas(geom_from, geom_to)\n        else:\n            straight_line = self.get_straight_line_between_areas(area_from, area_to)\n            midpoint, angle = self._get_midpoint_and_angle_from_line(straight_line)\n\n        # Ensure angle points from area_from to area_to\n        if not self._angle_points_to_target(geom_from, geom_to, midpoint, angle):\n            angle = (angle + 180) % 360\n\n        return midpoint, angle\n\n    def get_area_geometry(self, area: str) -&gt; Union[Polygon, MultiPolygon]:\n        \"\"\"Retrieve and validate geometry for a specified area.\n\n        Args:\n            area: Area identifier that must exist in area_model_gdf index\n\n        Returns:\n            Union[Polygon, MultiPolygon]: Cleaned geometry with buffer(0) applied\n                to ensure validity for geometric operations\n\n        Raises:\n            KeyError: If area is not found in area_model_gdf\n\n        Note:\n            The buffer(0) operation ensures geometric validity for complex\n            calculations, which is essential for reliable border analysis.\n        \"\"\"\n        return self.area_model_gdf.loc[area].geometry.buffer(0)\n\n    def get_straight_line_between_areas(self, area_from: str, area_to: str) -&gt; LineString:\n        \"\"\"Compute optimal straight-line connection between non-adjacent areas.\n\n        Creates a direct line connection between area boundaries, with intelligent\n        path optimization to avoid crossing other areas when possible. This is\n        particularly important for non-physical borders.\n\n        Algorithm:\n            1. Find representative points for both areas\n            2. Create line connecting area centroids\n            3. Calculate intersection points with area boundaries  \n            4. Check for conflicts with other areas\n            5. Apply non-crossing path optimization if needed\n\n        Args:\n            area_from: Source area identifier\n            area_to: Target area identifier\n\n        Returns:\n            LineString: Optimized connection line between area boundaries.\n                Line endpoints touch the area boundaries, not the centroids.\n\n        Raises:\n            ValueError: If areas are touching (should use physical border instead)\n\n        Example:\n\n            &gt;&gt;&gt; # Connect non-adjacent areas (e.g., Germany to UK)\n            &gt;&gt;&gt; line = calculator.get_straight_line_between_areas('DE', 'GB')\n            &gt;&gt;&gt; print(f\"Connection length: {line.length:.0f} km\")\n\n        Performance Note:\n            Results are cached to improve performance for repeated calculations.\n            Path optimization can be computationally intensive for complex geometries.\n        \"\"\"\n        key = tuple(sorted((area_from, area_to)))\n        if key in self._line_cache:\n            return self._line_cache[key]\n\n        if self.areas_touch(area_from, area_to):\n            raise ValueError(f\"Areas {area_from} and {area_to} touch - use border line instead\")\n\n        geom_from = self._get_largest_polygon(self.get_area_geometry(area_from))\n        geom_to = self._get_largest_polygon(self.get_area_geometry(area_to))\n\n        centroid_from = self.get_representative_area_point(geom_from)\n        centroid_to = self.get_representative_area_point(geom_to)\n\n        line_full = LineString([centroid_from, centroid_to])\n\n        # Find intersection points with area boundaries\n        intersection_from = self._get_boundary_intersection(geom_from, line_full, centroid_to)\n        intersection_to = self._get_boundary_intersection(geom_to, line_full, centroid_from)\n\n        straight_line = LineString([intersection_from, intersection_to])\n\n        # Check if line crosses other areas\n        if self._line_crosses_other_areas(straight_line, area_from, area_to):\n            # Try to find alternative path\n            better_line = self._find_non_crossing_line(area_from, area_to)\n            if better_line is not None:\n                straight_line = better_line\n\n        self._line_cache[key] = straight_line\n        return straight_line\n\n    def _get_midpoint_and_angle_for_touching_areas(\n        self,\n        geom_from: Union[Polygon, MultiPolygon],\n        geom_to: Union[Polygon, MultiPolygon]\n    ) -&gt; tuple[Point, float]:\n        \"\"\"Calculate midpoint and angle for physically adjacent areas.\n\n        For areas that share a physical boundary, this method extracts the\n        shared border line and computes the optimal point and angle for\n        directional indicators.\n\n        Args:\n            geom_from: Source area geometry\n            geom_to: Target area geometry\n\n        Returns:\n            tuple[Point, float]: Midpoint of shared border and perpendicular angle\n                pointing from source toward target area\n\n        Algorithm:\n            1. Extract continuous border line from geometric intersection\n            2. Find midpoint along border line (50% interpolation)\n            3. Calculate border bearing and perpendicular angle\n            4. Ensure angle points from source to target area\n        \"\"\"\n        border_line = self._get_continuous_border_line(geom_from, geom_to)\n        midpoint = border_line.interpolate(0.5, normalized=True)\n\n        # Get angle perpendicular to border\n        start_to_end = self._get_straight_line_from_endpoints(border_line)\n        border_bearing = self._calculate_bearing(start_to_end)\n        perpendicular_angle = (border_bearing + 90) % 360\n\n        return midpoint, perpendicular_angle\n\n    def _get_midpoint_and_angle_from_line(self, line: LineString) -&gt; tuple[Point, float]:\n        \"\"\"Extract midpoint and directional angle from a LineString.\n\n        Args:\n            line: Input LineString geometry\n\n        Returns:\n            tuple[Point, float]: Midpoint and bearing angle in degrees\n\n        Note:\n            Uses 50% interpolation to find the midpoint, ensuring consistent\n            positioning regardless of coordinate density along the line.\n        \"\"\"\n        midpoint = line.interpolate(0.5, normalized=True)\n        angle = self._calculate_bearing(line)\n        return midpoint, angle\n\n    def _angle_points_to_target(\n        self,\n        geom_from: Union[Polygon, MultiPolygon],\n        geom_to: Union[Polygon, MultiPolygon],\n        midpoint: Point,\n        angle: float\n    ) -&gt; bool:\n        \"\"\"Validate that computed angle points from source toward target area.\n\n        Ensures directional consistency by checking if the angle points closer\n        to the target area than to the source area. This is essential for\n        correct arrow orientation in energy flow visualization.\n\n        Args:\n            geom_from: Source area geometry\n            geom_to: Target area geometry  \n            midpoint: Reference point for angle measurement\n            angle: Angle to validate (in degrees)\n\n        Returns:\n            bool: True if angle points toward target, False if it points toward source\n\n        Algorithm:\n            1. Calculate bearings from midpoint to both area centroids\n            2. Compute angular differences between proposed angle and both bearings\n            3. Return True if angle is closer to target bearing than source bearing\n        \"\"\"\n        centroid_from = self.get_representative_area_point(geom_from)\n        centroid_to = self.get_representative_area_point(geom_to)\n\n        bearing_to_from = self._calculate_bearing(LineString([midpoint, centroid_from]))\n        bearing_to_to = self._calculate_bearing(LineString([midpoint, centroid_to]))\n\n        angle_diff_from = self._angular_difference(bearing_to_from, angle)\n        angle_diff_to = self._angular_difference(bearing_to_to, angle)\n\n        return angle_diff_to &lt; angle_diff_from\n\n    def _get_continuous_border_line(\n        self,\n        geom_a: Union[Polygon, MultiPolygon],\n        geom_b: Union[Polygon, MultiPolygon]\n    ) -&gt; LineString:\n        \"\"\"Extract shared boundary line between touching geometries.\n\n        Computes the geometric intersection between two touching areas and\n        converts the result into a continuous LineString representing the\n        shared border. Handles complex intersection geometries including\n        multiple segments and mixed geometry types.\n\n        Args:\n            geom_a: First area geometry\n            geom_b: Second area geometry\n\n        Returns:\n            LineString: Continuous line representing the shared boundary\n\n        Raises:\n            ValueError: If geometries don't touch or intersect\n            TypeError: If intersection cannot be converted to LineString\n\n        Algorithm:\n            1. Compute geometric intersection of the two areas\n            2. Handle GeometryCollection by extracting line components\n            3. Convert Polygon boundaries to LineString if needed\n            4. Merge multiple LineStrings into continuous representation\n            5. Handle MultiLineString by connecting segments optimally\n\n        Note:\n            This method handles the complexity of real-world geographic boundaries\n            which may result in complex intersection geometries.\n        \"\"\"\n        \"\"\"Get the shared border between two touching geometries.\"\"\"\n        if not (geom_a.touches(geom_b) or geom_a.intersects(geom_b)):\n            raise ValueError(\"Geometries do not touch or intersect\")\n\n        border = geom_a.intersection(geom_b)\n\n        if isinstance(border, GeometryCollection):\n            extracted_lines = []\n\n            for g in border.geoms:\n                if isinstance(g, LineString):\n                    extracted_lines.append(g)\n                elif isinstance(g, Polygon):\n                    extracted_lines.append(g.boundary)\n                elif isinstance(g, MultiLineString):\n                    extracted_lines.extend(g.geoms)\n                elif isinstance(g, MultiPolygon):\n                    extracted_lines.extend([p.boundary for p in g.geoms])\n\n            if not extracted_lines:\n                raise TypeError(f\"GeometryCollection could not be converted into line: {type(border)}\")\n\n            border = linemerge(extracted_lines)\n\n        if isinstance(border, MultiPolygon):\n            border = linemerge([p.boundary for p in border.geoms])\n\n        if isinstance(border, Polygon):\n            border = border.boundary\n\n        if isinstance(border, MultiLineString):\n            border = self._merge_multilinestring(border)\n\n        if isinstance(border, LineString):\n            return border\n\n        raise TypeError(f\"Unexpected border type: {type(border)}\")\n\n    def _get_boundary_intersection(\n        self,\n        geom: Polygon,\n        line: LineString,\n        target_point: Point\n    ) -&gt; Point:\n        \"\"\"Find optimal intersection point between line and polygon boundary.\n\n        When a line intersects a polygon boundary at multiple points, this method\n        selects the point that is closest to a specified target point. This is\n        essential for creating clean border connections.\n\n        Args:\n            geom: Polygon whose boundary to intersect with\n            line: LineString to intersect with polygon boundary\n            target_point: Reference point for choosing among multiple intersections\n\n        Returns:\n            Point: Intersection point closest to target_point\n\n        Raises:\n            TypeError: If intersection geometry type is unexpected\n\n        Algorithm:\n            1. Compute intersection between line and polygon boundary\n            2. Handle different intersection geometry types (Point, MultiPoint, LineString)\n            3. For multiple options, select point closest to target\n            4. Extract coordinates and create Point geometry\n        \"\"\"\n        \"\"\"Find where a line intersects a polygon boundary, choosing the point closest to target.\"\"\"\n        intersection = geom.boundary.intersection(line)\n\n        if isinstance(intersection, Point):\n            return intersection\n        elif isinstance(intersection, MultiPoint):\n            return min(intersection.geoms, key=lambda p: p.distance(target_point))\n        elif isinstance(intersection, (LineString, MultiLineString)):\n            # Get all coordinates and find closest\n            coords = []\n            if isinstance(intersection, LineString):\n                coords = list(intersection.coords)\n            else:\n                for line in intersection.geoms:\n                    coords.extend(list(line.coords))\n            return Point(min(coords, key=lambda c: Point(c).distance(target_point)))\n        else:\n            raise TypeError(f\"Unexpected intersection type: {type(intersection)}\")\n\n    def _line_crosses_other_areas(\n        self,\n        line: LineString,\n        *exclude_areas: str\n    ) -&gt; bool:\n        \"\"\"Check if a line crosses through any areas except specified exclusions.\n\n        This method is crucial for validating logical border connections to ensure\n        they don't inappropriately cross through other energy market areas, which\n        would be misleading in visualization.\n\n        Args:\n            line: LineString to test for crossings\n            *exclude_areas: Area identifiers to exclude from crossing check\n                (typically the source and target areas of the line)\n\n        Returns:\n            bool: True if line crosses any non-excluded areas, False otherwise\n        \"\"\"\n        other_areas = self.area_model_gdf.drop(list(exclude_areas))\n        return other_areas.geometry.crosses(line).any()\n\n    def _find_non_crossing_line(\n        self,\n        area_from: str,\n        area_to: str\n    ) -&gt; Union[LineString, None]:\n        \"\"\"Find optimal connection path that avoids crossing other areas.\n\n        Uses the NonCrossingPathFinder to compute the shortest connection between\n        two areas that maintains minimum clearance from other areas. This creates\n        clean visualization paths for logical borders.\n\n        Args:\n            area_from: Source area identifier\n            area_to: Target area identifier\n\n        Returns:\n            LineString or None: Optimal non-crossing path, or None if no suitable\n                path found within the configured constraints\n\n        Algorithm:\n            1. Extract largest polygons from MultiPolygon geometries\n            2. Create exclusion set of all other areas\n            3. Apply NonCrossingPathFinder algorithm\n            4. Return shortest valid path or None if impossible\n\n        Performance Note:\n            This operation can be computationally intensive for complex geometries\n            and large numbers of areas. Results are cached for efficiency.\n        \"\"\"\n        poly_from = self._get_largest_polygon(self.get_area_geometry(area_from))\n        poly_to = self._get_largest_polygon(self.get_area_geometry(area_to))\n\n        return self.non_crossing_path_finder.find_shortest_path(\n            poly_from,\n            poly_to,\n            self.area_model_gdf.drop([area_from, area_to]),\n            f\"{area_from} to {area_to}\"\n        )\n\n    def _get_largest_polygon(self, geom: Union[Polygon, MultiPolygon]) -&gt; Polygon:\n        \"\"\"Extract largest polygon component from MultiPolygon geometry.\n\n        For MultiPolygon geometries (e.g., countries with islands), this method\n        returns the largest polygon by area, which is typically the main landmass.\n        This simplifies calculations while focusing on the most significant\n        geographic component.\n\n        Args:\n            geom: Input geometry (Polygon returned as-is, MultiPolygon simplified)\n\n        Returns:\n            Polygon: Largest polygon component by area\n        \"\"\"\n        if isinstance(geom, Polygon):\n            return geom\n        return max(geom.geoms, key=lambda p: p.area)\n\n    def _merge_multilinestring(self, mls: MultiLineString) -&gt; LineString:\n        \"\"\"Merge disconnected LineString segments into continuous line.\n\n        Converts a MultiLineString into a single continuous LineString by\n        intelligently connecting segments to minimize total length while\n        preserving the overall geometric relationship.\n\n        Args:\n            mls: MultiLineString with potentially disconnected segments\n\n        Returns:\n            LineString: Continuous line connecting all segments optimally\n\n        Algorithm:\n            1. Attempt automatic merge using Shapely's linemerge\n            2. If segments remain disconnected, apply iterative connection:\n               - Find closest pair of segment endpoints\n               - Connect segments with optimal orientation\n               - Repeat until single continuous line achieved\n\n        Note:\n            This method is particularly important for complex international\n            borders that may be represented as multiple disconnected segments\n            in geographic datasets.\n        \"\"\"\n        merged = linemerge(list(mls.geoms))\n\n        if isinstance(merged, LineString):\n            return merged\n\n        # Connect disconnected segments\n        lines = list(merged.geoms)\n\n        while len(lines) &gt; 1:\n            # Find closest pair\n            min_dist = float('inf')\n            closest_pair = None\n\n            for i, line1 in enumerate(lines):\n                for j, line2 in enumerate(lines[i+1:], i+1):\n                    p1, p2 = nearest_points(line1, line2)\n                    dist = p1.distance(p2)\n                    if dist &lt; min_dist:\n                        min_dist = dist\n                        closest_pair = (i, j)\n\n            # Connect closest pair\n            i, j = closest_pair\n            line1, line2 = lines[i], lines[j]\n\n            # Create connected line\n            coords1 = list(line1.coords)\n            coords2 = list(line2.coords)\n\n            # Find best connection direction\n            connections = [\n                (coords1 + coords2, LineString(coords1 + coords2).length),\n                (coords1 + coords2[::-1], LineString(coords1 + coords2[::-1]).length),\n                (coords1[::-1] + coords2, LineString(coords1[::-1] + coords2).length),\n                (coords1[::-1] + coords2[::-1], LineString(coords1[::-1] + coords2[::-1]).length)\n            ]\n\n            best_coords = min(connections, key=lambda x: x[1])[0]\n            new_line = LineString(best_coords)\n\n            # Update lines list\n            lines = [l for k, l in enumerate(lines) if k not in (i, j)]\n            lines.append(new_line)\n\n        return lines[0]\n\n    def _get_straight_line_from_endpoints(self, line: LineString) -&gt; LineString:\n        \"\"\"Create straight line connecting first and last coordinates.\n\n        Args:\n            line: Input LineString with potentially complex path\n\n        Returns:\n            LineString: Simplified straight line from start to end\n\n        Use Case:\n            Useful for computing bearing angles from complex border geometries\n            by simplifying to the fundamental start-to-end direction.\n        \"\"\"\n        return LineString([line.coords[0], line.coords[-1]])\n\n    def _calculate_bearing(self, line: LineString) -&gt; float:\n        \"\"\"Calculate compass bearing (Azimuth angle) for a LineString.\n\n        Args:\n            line: LineString from which to calculate bearing\n\n        Returns:\n            float: Compass bearing (Azimuth angle) in degrees (0-360)\n                - 0\u00b0 = North\n                - 90\u00b0 = East  \n                - 180\u00b0 = South\n                - 270\u00b0 = West\n\n        Algorithm:\n            Uses the forward azimuth formula from geodetic calculations:\n            1. Convert coordinates to radians\n            2. Apply spherical trigonometry formulas\n            3. Convert result to compass bearing (0-360\u00b0)\n\n        Note:\n            This implementation assumes coordinates are in geographic (lat/lon)\n            format. For projected coordinates, results approximate true bearings\n            within reasonable accuracy for visualization purposes.\n        \"\"\"\n        start = line.coords[0]\n        end = line.coords[-1]\n\n        lat1 = math.radians(start[1])\n        lat2 = math.radians(end[1])\n\n        diff_lon = math.radians(end[0] - start[0])\n\n        x = math.sin(diff_lon) * math.cos(lat2)\n        y = (math.cos(lat1) * math.sin(lat2) - \n             math.sin(lat1) * math.cos(lat2) * math.cos(diff_lon))\n\n        bearing = math.atan2(x, y)\n        bearing = math.degrees(bearing)\n        compass_bearing = (bearing + 360) % 360\n\n        return compass_bearing\n\n    def _angular_difference(self, angle1: float, angle2: float) -&gt; float:\n        \"\"\"Calculate minimum angular difference between two compass bearings.\n\n        Handles the circular nature of compass bearings to find the shortest\n        angular distance between two directions, accounting for the 0\u00b0/360\u00b0 wraparound.\n\n        Args:\n            angle1: First angle in degrees (0-360)\n            angle2: Second angle in degrees (0-360)\n\n        Returns:\n            float: Minimum angular difference in degrees (0-180)\n\n        Example:\n\n            &gt;&gt;&gt; diff = calculator._angular_difference(10, 350)  # Returns 20, not 340\n            &gt;&gt;&gt; diff = calculator._angular_difference(90, 270)  # Returns 180\n        \"\"\"\n        diff = abs(angle1 - angle2) % 360\n        return min(diff, 360 - diff)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.__init__","title":"__init__","text":"<pre><code>__init__(area_model_gdf: GeoDataFrame, non_crossing_path_finder: NonCrossingPathFinder = None)\n</code></pre> <p>Initialize the border geometry calculator.</p> <p>Parameters:</p> Name Type Description Default <code>area_model_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing area geometries with polygon boundaries. Index should contain area identifiers (e.g., country codes, bidding zone names). Must contain valid polygon geometries in 'geometry' column.</p> required <code>non_crossing_path_finder</code> <code>NonCrossingPathFinder</code> <p>Optional custom path finder for logical borders. If None, creates default NonCrossingPathFinder with standard parameters.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If geometries are invalid or area_model_gdf lacks required structure</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; areas_gdf = gpd.read_file('countries.geojson').set_index('ISO_A2')\n&gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas_gdf)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Custom path finder for specific requirements\n&gt;&gt;&gt; custom_finder = NonCrossingPathFinder(num_points=200, min_clearance=10000)\n&gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas_gdf, custom_finder)\n</code></pre> Note <p>Invalid geometries are automatically cleaned using buffer(0) operation. Large area datasets benefit from using projected coordinate systems for accurate geometric calculations.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def __init__(self, area_model_gdf: gpd.GeoDataFrame, non_crossing_path_finder: 'NonCrossingPathFinder' = None):\n    \"\"\"Initialize the border geometry calculator.\n\n    Args:\n        area_model_gdf: GeoDataFrame containing area geometries with polygon\n            boundaries. Index should contain area identifiers (e.g., country codes,\n            bidding zone names). Must contain valid polygon geometries in 'geometry' column.\n        non_crossing_path_finder: Optional custom path finder for logical borders.\n            If None, creates default NonCrossingPathFinder with standard parameters.\n\n    Raises:\n        ValueError: If geometries are invalid or area_model_gdf lacks required structure\n\n    Example:\n\n        &gt;&gt;&gt; areas_gdf = gpd.read_file('countries.geojson').set_index('ISO_A2')\n        &gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas_gdf)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Custom path finder for specific requirements\n        &gt;&gt;&gt; custom_finder = NonCrossingPathFinder(num_points=200, min_clearance=10000)\n        &gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas_gdf, custom_finder)\n\n    Note:\n        Invalid geometries are automatically cleaned using buffer(0) operation.\n        Large area datasets benefit from using projected coordinate systems\n        for accurate geometric calculations.\n    \"\"\"\n    self.area_model_gdf = area_model_gdf\n    self.non_crossing_path_finder = non_crossing_path_finder or NonCrossingPathFinder()\n    self._validate_geometries()\n\n    self._centroid_cache: dict[str, Point] = {}\n    self._line_cache: dict[Tuple[str, str], LineString] = {}\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.calculate_border_geometry","title":"calculate_border_geometry","text":"<pre><code>calculate_border_geometry(area_from: str, area_to: str) -&gt; dict[str, Union[Point, float, LineString, bool]]\n</code></pre> <p>Calculate comprehensive geometric properties for an area border.</p> <p>This is the main interface method that computes all geometric properties needed for border visualization and analysis. It automatically detects whether areas are physically adjacent or logically connected and applies appropriate geometric algorithms.</p> Processing Logic <ol> <li>Detect if areas share physical boundary (touching/intersecting)</li> <li>For physical borders: extract shared boundary line</li> <li>For logical borders: compute optimal connection path</li> <li>Calculate representative point for label/arrow placement</li> <li>Compute azimuth angle for arrow icon visualization</li> </ol> <p>Parameters:</p> Name Type Description Default <code>area_from</code> <code>str</code> <p>Source area identifier (must exist in area_model_gdf index)</p> required <code>area_to</code> <code>str</code> <p>Target area identifier (must exist in area_model_gdf index)</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Union[Point, float, LineString, bool]]</code> <p>Comprehensive border geometry information containing: - 'projection_point' (Point): Optimal point for label/arrow placement - 'azimuth_angle' (float): Directional angle in degrees (0-360) - 'geo_line_string' (LineString): Border line geometry - 'is_physical' (bool): True for touching areas, False for logical borders</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If area_from or area_to not found in area_model_gdf</p> <code>ValueError</code> <p>If geometric calculations fail</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; border_info = calculator.calculate_border_geometry('DE', 'FR')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Use for visualization\n&gt;&gt;&gt; point = border_info['projection_point']\n&gt;&gt;&gt; angle = border_info['azimuth_angle']\n&gt;&gt;&gt; is_physical = border_info['is_physical']\n&gt;&gt;&gt; \n&gt;&gt;&gt; print(f\"Border DE\u2192FR: {point} at {angle}\u00b0 ({'physical' if is_physical else 'logical'})\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def calculate_border_geometry(\n    self, \n    area_from: str, \n    area_to: str\n) -&gt; dict[str, Union[Point, float, LineString, bool]]:\n    \"\"\"Calculate comprehensive geometric properties for an area border.\n\n    This is the main interface method that computes all geometric properties\n    needed for border visualization and analysis. It automatically detects\n    whether areas are physically adjacent or logically connected and applies\n    appropriate geometric algorithms.\n\n    Processing Logic:\n        1. Detect if areas share physical boundary (touching/intersecting)\n        2. For physical borders: extract shared boundary line\n        3. For logical borders: compute optimal connection path\n        4. Calculate representative point for label/arrow placement\n        5. Compute azimuth angle for arrow icon visualization\n\n    Args:\n        area_from: Source area identifier (must exist in area_model_gdf index)\n        area_to: Target area identifier (must exist in area_model_gdf index)\n\n    Returns:\n        dict: Comprehensive border geometry information containing:\n            - 'projection_point' (Point): Optimal point for label/arrow placement\n            - 'azimuth_angle' (float): Directional angle in degrees (0-360)\n            - 'geo_line_string' (LineString): Border line geometry\n            - 'is_physical' (bool): True for touching areas, False for logical borders\n\n    Raises:\n        KeyError: If area_from or area_to not found in area_model_gdf\n        ValueError: If geometric calculations fail\n\n    Example:\n\n        &gt;&gt;&gt; border_info = calculator.calculate_border_geometry('DE', 'FR')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Use for visualization\n        &gt;&gt;&gt; point = border_info['projection_point']\n        &gt;&gt;&gt; angle = border_info['azimuth_angle']\n        &gt;&gt;&gt; is_physical = border_info['is_physical']\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; print(f\"Border DE\u2192FR: {point} at {angle}\u00b0 ({'physical' if is_physical else 'logical'})\")\n    \"\"\"\n    midpoint, angle = self.get_area_border_midpoint_and_angle(area_from, area_to)\n\n    if self.areas_touch(area_from, area_to) or self.areas_intersect(area_from, area_to):\n        geom_from = self.get_area_geometry(area_from)\n        geom_to = self.get_area_geometry(area_to)\n        border_line = self._get_continuous_border_line(geom_from, geom_to)\n        is_physical = True\n    else:\n        border_line = self.get_straight_line_between_areas(area_from, area_to)\n        is_physical = False\n\n    return {\n        self.PROJECTION_POINT_IDENTIFIER: midpoint,\n        self.AZIMUTH_ANGLE_IDENTIFIER: angle,\n        self.BORDER_LINE_STRING_IDENTIFIER: border_line,\n        self.BORDER_IS_PHYSICAL_IDENTIFIER: is_physical\n    }\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.areas_touch","title":"areas_touch","text":"<pre><code>areas_touch(area_from: str, area_to: str) -&gt; bool\n</code></pre> <p>Check if two areas share a common physical (geographic) border.</p> <p>Uses Shapely's touches() method to determine if area boundaries intersect without overlapping. This is the standard definition of physical adjacency for energy market regions.</p> <p>Parameters:</p> Name Type Description Default <code>area_from</code> <code>str</code> <p>Source area identifier</p> required <code>area_to</code> <code>str</code> <p>Target area identifier</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if areas share a common boundary, False otherwise</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; touching = calculator.areas_touch('DE', 'FR')  # True for neighboring countries\n&gt;&gt;&gt; separated = calculator.areas_touch('DE', 'GB')  # False for non-adjacent countries\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def areas_touch(self, area_from: str, area_to: str) -&gt; bool:\n    \"\"\"Check if two areas share a common physical (geographic) border.\n\n    Uses Shapely's touches() method to determine if area boundaries\n    intersect without overlapping. This is the standard definition\n    of physical adjacency for energy market regions.\n\n    Args:\n        area_from: Source area identifier\n        area_to: Target area identifier\n\n    Returns:\n        bool: True if areas share a common boundary, False otherwise\n\n    Example:\n\n        &gt;&gt;&gt; touching = calculator.areas_touch('DE', 'FR')  # True for neighboring countries\n        &gt;&gt;&gt; separated = calculator.areas_touch('DE', 'GB')  # False for non-adjacent countries\n    \"\"\"\n    geom_from = self.get_area_geometry(area_from)\n    geom_to = self.get_area_geometry(area_to)\n    return geom_from.touches(geom_to)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.areas_intersect","title":"areas_intersect","text":"<pre><code>areas_intersect(area_from: str, area_to: str) -&gt; bool\n</code></pre> <p>Check if two areas have any geometric intersection.</p> <p>Uses Shapely's intersects() method to check for any form of geometric intersection, including touching, overlapping, or containment. This is broader than the touches() check and handles edge cases in geographic data.</p> <p>Parameters:</p> Name Type Description Default <code>area_from</code> <code>str</code> <p>Source area identifier</p> required <code>area_to</code> <code>str</code> <p>Target area identifier</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if areas have any geometric intersection, False otherwise</p> Note <p>This method is used as a fallback for areas_touch() to handle geographic data with small overlaps or slight topology inconsistencies that are common in real-world boundary datasets.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def areas_intersect(self, area_from: str, area_to: str) -&gt; bool:\n    \"\"\"Check if two areas have any geometric intersection.\n\n    Uses Shapely's intersects() method to check for any form of geometric\n    intersection, including touching, overlapping, or containment. This is\n    broader than the touches() check and handles edge cases in geographic data.\n\n    Args:\n        area_from: Source area identifier\n        area_to: Target area identifier\n\n    Returns:\n        bool: True if areas have any geometric intersection, False otherwise\n\n    Note:\n        This method is used as a fallback for areas_touch() to handle\n        geographic data with small overlaps or slight topology inconsistencies\n        that are common in real-world boundary datasets.\n    \"\"\"\n    geom_from = self.get_area_geometry(area_from)\n    geom_to = self.get_area_geometry(area_to)\n    return geom_from.intersects(geom_to)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.get_area_border_midpoint_and_angle","title":"get_area_border_midpoint_and_angle","text":"<pre><code>get_area_border_midpoint_and_angle(area_from: str, area_to: str) -&gt; tuple[Point, float]\n</code></pre> <p>Calculate representative point and directional angle for border.</p> <p>Computes the optimal point for placing directional indicators (arrows, labels) and the corresponding angle for proper orientation. The algorithm adapts to both physical and logical borders to ensure optimal placement.</p> For Physical Borders <ul> <li>Uses midpoint of shared boundary line</li> <li>Angle is perpendicular to boundary, pointing toward target area</li> </ul> For Logical Borders <ul> <li>Uses midpoint of optimal connection line</li> <li>Angle follows connection direction from source to target</li> </ul> <p>Parameters:</p> Name Type Description Default <code>area_from</code> <code>str</code> <p>Source area identifier</p> required <code>area_to</code> <code>str</code> <p>Target area identifier</p> required <p>Returns:</p> Type Description <code>tuple[Point, float]</code> <p>tuple[Point, float]: Representative point and directional angle in degrees. Angle range: 0-360 degrees, where 0\u00b0 is North, 90\u00b0 is East.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; point, angle = calculator.get_area_border_midpoint_and_angle('DE', 'FR')\n&gt;&gt;&gt; print(f\"Place arrow at {point} oriented at {angle}\u00b0 for DE\u2192FR flow\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def get_area_border_midpoint_and_angle(\n    self, \n    area_from: str, \n    area_to: str\n) -&gt; tuple[Point, float]:\n    \"\"\"Calculate representative point and directional angle for border.\n\n    Computes the optimal point for placing directional indicators (arrows,\n    labels) and the corresponding angle for proper orientation. The algorithm\n    adapts to both physical and logical borders to ensure optimal placement.\n\n    For Physical Borders:\n        - Uses midpoint of shared boundary line\n        - Angle is perpendicular to boundary, pointing toward target area\n\n    For Logical Borders:\n        - Uses midpoint of optimal connection line\n        - Angle follows connection direction from source to target\n\n    Args:\n        area_from: Source area identifier\n        area_to: Target area identifier\n\n    Returns:\n        tuple[Point, float]: Representative point and directional angle in degrees.\n            Angle range: 0-360 degrees, where 0\u00b0 is North, 90\u00b0 is East.\n\n    Example:\n\n        &gt;&gt;&gt; point, angle = calculator.get_area_border_midpoint_and_angle('DE', 'FR')\n        &gt;&gt;&gt; print(f\"Place arrow at {point} oriented at {angle}\u00b0 for DE\u2192FR flow\")\n    \"\"\"\n    geom_from = self.get_area_geometry(area_from)\n    geom_to = self.get_area_geometry(area_to)\n\n    if self.areas_touch(area_from, area_to) or self.areas_intersect(area_from, area_to):\n        midpoint, angle = self._get_midpoint_and_angle_for_touching_areas(geom_from, geom_to)\n    else:\n        straight_line = self.get_straight_line_between_areas(area_from, area_to)\n        midpoint, angle = self._get_midpoint_and_angle_from_line(straight_line)\n\n    # Ensure angle points from area_from to area_to\n    if not self._angle_points_to_target(geom_from, geom_to, midpoint, angle):\n        angle = (angle + 180) % 360\n\n    return midpoint, angle\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.get_area_geometry","title":"get_area_geometry","text":"<pre><code>get_area_geometry(area: str) -&gt; Union[Polygon, MultiPolygon]\n</code></pre> <p>Retrieve and validate geometry for a specified area.</p> <p>Parameters:</p> Name Type Description Default <code>area</code> <code>str</code> <p>Area identifier that must exist in area_model_gdf index</p> required <p>Returns:</p> Type Description <code>Union[Polygon, MultiPolygon]</code> <p>Union[Polygon, MultiPolygon]: Cleaned geometry with buffer(0) applied to ensure validity for geometric operations</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If area is not found in area_model_gdf</p> Note <p>The buffer(0) operation ensures geometric validity for complex calculations, which is essential for reliable border analysis.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def get_area_geometry(self, area: str) -&gt; Union[Polygon, MultiPolygon]:\n    \"\"\"Retrieve and validate geometry for a specified area.\n\n    Args:\n        area: Area identifier that must exist in area_model_gdf index\n\n    Returns:\n        Union[Polygon, MultiPolygon]: Cleaned geometry with buffer(0) applied\n            to ensure validity for geometric operations\n\n    Raises:\n        KeyError: If area is not found in area_model_gdf\n\n    Note:\n        The buffer(0) operation ensures geometric validity for complex\n        calculations, which is essential for reliable border analysis.\n    \"\"\"\n    return self.area_model_gdf.loc[area].geometry.buffer(0)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.get_straight_line_between_areas","title":"get_straight_line_between_areas","text":"<pre><code>get_straight_line_between_areas(area_from: str, area_to: str) -&gt; LineString\n</code></pre> <p>Compute optimal straight-line connection between non-adjacent areas.</p> <p>Creates a direct line connection between area boundaries, with intelligent path optimization to avoid crossing other areas when possible. This is particularly important for non-physical borders.</p> Algorithm <ol> <li>Find representative points for both areas</li> <li>Create line connecting area centroids</li> <li>Calculate intersection points with area boundaries  </li> <li>Check for conflicts with other areas</li> <li>Apply non-crossing path optimization if needed</li> </ol> <p>Parameters:</p> Name Type Description Default <code>area_from</code> <code>str</code> <p>Source area identifier</p> required <code>area_to</code> <code>str</code> <p>Target area identifier</p> required <p>Returns:</p> Name Type Description <code>LineString</code> <code>LineString</code> <p>Optimized connection line between area boundaries. Line endpoints touch the area boundaries, not the centroids.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If areas are touching (should use physical border instead)</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Connect non-adjacent areas (e.g., Germany to UK)\n&gt;&gt;&gt; line = calculator.get_straight_line_between_areas('DE', 'GB')\n&gt;&gt;&gt; print(f\"Connection length: {line.length:.0f} km\")\n</code></pre> Performance Note <p>Results are cached to improve performance for repeated calculations. Path optimization can be computationally intensive for complex geometries.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def get_straight_line_between_areas(self, area_from: str, area_to: str) -&gt; LineString:\n    \"\"\"Compute optimal straight-line connection between non-adjacent areas.\n\n    Creates a direct line connection between area boundaries, with intelligent\n    path optimization to avoid crossing other areas when possible. This is\n    particularly important for non-physical borders.\n\n    Algorithm:\n        1. Find representative points for both areas\n        2. Create line connecting area centroids\n        3. Calculate intersection points with area boundaries  \n        4. Check for conflicts with other areas\n        5. Apply non-crossing path optimization if needed\n\n    Args:\n        area_from: Source area identifier\n        area_to: Target area identifier\n\n    Returns:\n        LineString: Optimized connection line between area boundaries.\n            Line endpoints touch the area boundaries, not the centroids.\n\n    Raises:\n        ValueError: If areas are touching (should use physical border instead)\n\n    Example:\n\n        &gt;&gt;&gt; # Connect non-adjacent areas (e.g., Germany to UK)\n        &gt;&gt;&gt; line = calculator.get_straight_line_between_areas('DE', 'GB')\n        &gt;&gt;&gt; print(f\"Connection length: {line.length:.0f} km\")\n\n    Performance Note:\n        Results are cached to improve performance for repeated calculations.\n        Path optimization can be computationally intensive for complex geometries.\n    \"\"\"\n    key = tuple(sorted((area_from, area_to)))\n    if key in self._line_cache:\n        return self._line_cache[key]\n\n    if self.areas_touch(area_from, area_to):\n        raise ValueError(f\"Areas {area_from} and {area_to} touch - use border line instead\")\n\n    geom_from = self._get_largest_polygon(self.get_area_geometry(area_from))\n    geom_to = self._get_largest_polygon(self.get_area_geometry(area_to))\n\n    centroid_from = self.get_representative_area_point(geom_from)\n    centroid_to = self.get_representative_area_point(geom_to)\n\n    line_full = LineString([centroid_from, centroid_to])\n\n    # Find intersection points with area boundaries\n    intersection_from = self._get_boundary_intersection(geom_from, line_full, centroid_to)\n    intersection_to = self._get_boundary_intersection(geom_to, line_full, centroid_from)\n\n    straight_line = LineString([intersection_from, intersection_to])\n\n    # Check if line crosses other areas\n    if self._line_crosses_other_areas(straight_line, area_from, area_to):\n        # Try to find alternative path\n        better_line = self._find_non_crossing_line(area_from, area_to)\n        if better_line is not None:\n            straight_line = better_line\n\n    self._line_cache[key] = straight_line\n    return straight_line\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_geometry_calculator.NonCrossingPathFinder","title":"NonCrossingPathFinder","text":"<p>Optimized path finder for non-crossing connections between areas.</p> <p>This class implements an algorithm for finding the shortest path between two polygon areas while maintaining specified clearance from other areas. It's specifically designed for energy system visualization where geographic border line representations should not misleadingly cross through other market areas.</p> <p>The algorithm uses a brute-force approach to test multiple potential paths and select the optimal solution.</p> Key Features <ul> <li>Configurable boundary point sampling density</li> <li>Adjustable minimum clearance distances</li> <li>Progress tracking for long-running operations</li> <li>Optimization for common geometric scenarios</li> </ul> Performance Characteristics <ul> <li>Time complexity: O(n\u00b2 \u00d7 m) where n=num_points, m=number of other areas</li> <li>Memory usage scales with point sampling density</li> <li>Results improve with higher point sampling but at computational cost</li> </ul> <p>Attributes:</p> Name Type Description <code>num_points</code> <code>int</code> <p>Number of boundary points to sample per area</p> <code>min_clearance</code> <code>float</code> <p>Minimum distance from other areas (in CRS units)</p> <code>show_progress</code> <code>bool</code> <p>Whether to display progress bars for long operations</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # High-precision path finding\n&gt;&gt;&gt; finder = NonCrossingPathFinder(num_points=500, min_clearance=50000)\n&gt;&gt;&gt; path = finder.find_shortest_path(area1_poly, area2_poly, other_areas_gdf)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>class NonCrossingPathFinder:\n    \"\"\"Optimized path finder for non-crossing connections between areas.\n\n    This class implements an algorithm for finding the shortest\n    path between two polygon areas while maintaining specified clearance from\n    other areas. It's specifically designed for energy system visualization\n    where geographic border line representations should not misleadingly\n    cross through other market areas.\n\n    The algorithm uses a brute-force approach to test multiple potential paths\n    and select the optimal solution.\n\n    Key Features:\n        - Configurable boundary point sampling density\n        - Adjustable minimum clearance distances\n        - Progress tracking for long-running operations\n        - Optimization for common geometric scenarios\n\n    Performance Characteristics:\n        - Time complexity: O(n\u00b2 \u00d7 m) where n=num_points, m=number of other areas\n        - Memory usage scales with point sampling density\n        - Results improve with higher point sampling but at computational cost\n\n    Attributes:\n        num_points (int): Number of boundary points to sample per area\n        min_clearance (float): Minimum distance from other areas (in CRS units)\n        show_progress (bool): Whether to display progress bars for long operations\n\n    Example:\n\n        &gt;&gt;&gt; # High-precision path finding\n        &gt;&gt;&gt; finder = NonCrossingPathFinder(num_points=500, min_clearance=50000)\n        &gt;&gt;&gt; path = finder.find_shortest_path(area1_poly, area2_poly, other_areas_gdf)\n    \"\"\"\n\n    def __init__(\n        self,\n        num_points: int = 100,\n        min_clearance: float = 50000,\n        show_progress: bool = True\n    ):\n        \"\"\"Initialize the non-crossing path finder.\n\n        Args:\n            num_points: Number of boundary points to sample per polygon.\n                Higher values improve path quality but increase computation time.\n                Typical range: 50-500 depending on precision requirements.\n            min_clearance: Minimum clearance distance from other areas in\n                coordinate reference system units. For geographic coordinates,\n                this is typically in meters when using projected CRS.\n            show_progress: Whether to display progress bars during computation.\n                Useful for long-running operations with high num_points values.\n\n        Example:\n\n            &gt;&gt;&gt; # High-precision finder for detailed analysis\n            &gt;&gt;&gt; finder = NonCrossingPathFinder(\n            ...     num_points=300,      # High sampling density\n            ...     min_clearance=25000, # 25km minimum clearance\n            ...     show_progress=True   # Show progress for long operations\n            ... )\n        \"\"\"\n        self.num_points = num_points\n        self.min_clearance = min_clearance\n        self.show_progress = show_progress\n\n    def find_shortest_path(\n        self,\n        polygon1: Polygon,\n        polygon2: Polygon,\n        other_areas: gpd.GeoDataFrame,\n        name: str = None\n    ) -&gt; Union[LineString, None]:\n        \"\"\"Find shortest non-crossing path between two polygons.\n\n        Tests all combinations of boundary points between two polygons to find\n        the shortest connection that maintains minimum clearance from other areas.\n        If the algorithm succeedes and finds a non-crossing LineString, it ensures\n        clean visualization paths for energy market border analysis.\n\n        Args:\n            polygon1: Source polygon geometry\n            polygon2: Target polygon geometry\n            other_areas: GeoDataFrame of areas to avoid crossing through\n            name: Optional name for progress tracking display\n\n        Returns:\n            LineString or None: Shortest valid path between polygons, or None\n                if no path meets clearance requirements\n\n        Algorithm:\n            1. Sample boundary points for both polygons\n            2. Buffer other areas by minimum clearance distance\n            3. Test all point-to-point connections\n            4. Filter out paths that cross buffered areas\n            5. Return shortest valid path\n\n        Performance Scaling:\n            - Total paths tested: num_points\u00b2 \n            - With default num_points=100: tests 10,000 potential paths\n            - Computation time scales roughly O(n\u00b2 \u00d7 m) where m = number of other areas\n\n        Example:\n\n            &gt;&gt;&gt; path = finder.find_shortest_path(\n            ...     source_area, target_area, obstacles_gdf, \"Germany to UK\"\n            ... )\n            &gt;&gt;&gt; if path:\n            ...     print(f\"Found path with length: {path.length:.0f} meters\")\n            ... else:\n            ...     print(\"No valid path found with current clearance settings\")\n        \"\"\"\n        buffered_areas = self._buffer_areas(other_areas, self.min_clearance)\n        points1 = self._get_boundary_points(polygon1, self.num_points)\n        points2 = self._get_boundary_points(polygon2, self.num_points)\n\n        lines = [LineString([p1, p2]) for p1, p2 in itertools.product(points1, points2)]\n        shortest_line = None\n        min_length = float('inf')\n\n        iterator = tqdm(lines, desc=f\"Finding path for {name or 'path'}\") if self.show_progress else lines\n\n        for line in iterator:\n            if not buffered_areas.geometry.crosses(line).any():\n                if line.length &lt; min_length:\n                    shortest_line = line\n                    min_length = line.length\n\n        return shortest_line\n\n    def _buffer_areas(self, areas: gpd.GeoDataFrame, buffer_distance: float) -&gt; gpd.GeoDataFrame:\n        \"\"\"Apply buffer operation to create clearance zones around areas.\n\n        Creates expanded geometries around areas to enforce minimum clearance\n        distances. Handles coordinate system transformations to ensure accurate\n        distance-based buffering regardless of input CRS.\n\n        Args:\n            areas: GeoDataFrame containing areas to buffer\n            buffer_distance: Buffer distance in meters\n\n        Returns:\n            gpd.GeoDataFrame: Areas with buffered geometries in original CRS\n\n        Raises:\n            ValueError: If GeoDataFrame lacks valid CRS definition\n\n        Algorithm:\n            1. Check if CRS is geographic (lat/lon)\n            2. If geographic, temporarily project to Web Mercator (EPSG:3857)\n            3. Apply buffer operation in projected coordinates\n            4. Transform back to original CRS\n            5. If already projected, buffer directly\n        \"\"\"\n        areas_copy = areas.copy()\n        original_crs = areas_copy.crs\n\n        if original_crs is None:\n            raise ValueError(\"GeoDataFrame must have a valid CRS defined.\")\n\n        if original_crs.is_geographic:\n            # Use Web Mercator for accurate distance-based buffering\n            projected_crs = \"EPSG:3857\"\n            areas_copy = areas_copy.to_crs(projected_crs)\n            areas_copy['geometry'] = areas_copy.buffer(buffer_distance)\n            areas_copy = areas_copy.to_crs(original_crs)\n        else:\n            # Already in projected coordinates\n            areas_copy['geometry'] = areas_copy.buffer(buffer_distance)\n\n        return areas_copy\n\n    def _get_boundary_points(self, polygon: Polygon, num_points: int) -&gt; list[Point]:\n        \"\"\"Sample evenly distributed points along polygon boundary.\n\n        Creates a uniform sampling of points along the polygon perimeter using\n        interpolation. This provides comprehensive coverage for path-finding\n        while maintaining computational efficiency.\n\n        Args:\n            polygon: Input polygon to sample\n            num_points: Number of points to sample along boundary\n\n        Returns:\n            list[Point]: List of evenly spaced boundary points\n\n        Algorithm:\n            1. Calculate total boundary length\n            2. Divide into equal segments\n            3. Interpolate points at regular intervals\n            4. Return as list of Point geometries\n\n        Note:\n            Points are distributed proportionally to boundary length,\n            ensuring uniform density regardless of polygon complexity.\n        \"\"\"\n        boundary = polygon.boundary\n        total_length = boundary.length\n        # Generate evenly spaced points along boundary\n        return [boundary.interpolate((i / num_points) * total_length) for i in range(num_points)]\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_geometry_calculator.NonCrossingPathFinder.__init__","title":"__init__","text":"<pre><code>__init__(num_points: int = 100, min_clearance: float = 50000, show_progress: bool = True)\n</code></pre> <p>Initialize the non-crossing path finder.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of boundary points to sample per polygon. Higher values improve path quality but increase computation time. Typical range: 50-500 depending on precision requirements.</p> <code>100</code> <code>min_clearance</code> <code>float</code> <p>Minimum clearance distance from other areas in coordinate reference system units. For geographic coordinates, this is typically in meters when using projected CRS.</p> <code>50000</code> <code>show_progress</code> <code>bool</code> <p>Whether to display progress bars during computation. Useful for long-running operations with high num_points values.</p> <code>True</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; # High-precision finder for detailed analysis\n&gt;&gt;&gt; finder = NonCrossingPathFinder(\n...     num_points=300,      # High sampling density\n...     min_clearance=25000, # 25km minimum clearance\n...     show_progress=True   # Show progress for long operations\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def __init__(\n    self,\n    num_points: int = 100,\n    min_clearance: float = 50000,\n    show_progress: bool = True\n):\n    \"\"\"Initialize the non-crossing path finder.\n\n    Args:\n        num_points: Number of boundary points to sample per polygon.\n            Higher values improve path quality but increase computation time.\n            Typical range: 50-500 depending on precision requirements.\n        min_clearance: Minimum clearance distance from other areas in\n            coordinate reference system units. For geographic coordinates,\n            this is typically in meters when using projected CRS.\n        show_progress: Whether to display progress bars during computation.\n            Useful for long-running operations with high num_points values.\n\n    Example:\n\n        &gt;&gt;&gt; # High-precision finder for detailed analysis\n        &gt;&gt;&gt; finder = NonCrossingPathFinder(\n        ...     num_points=300,      # High sampling density\n        ...     min_clearance=25000, # 25km minimum clearance\n        ...     show_progress=True   # Show progress for long operations\n        ... )\n    \"\"\"\n    self.num_points = num_points\n    self.min_clearance = min_clearance\n    self.show_progress = show_progress\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.border_model_geometry_calculator.NonCrossingPathFinder.find_shortest_path","title":"find_shortest_path","text":"<pre><code>find_shortest_path(polygon1: Polygon, polygon2: Polygon, other_areas: GeoDataFrame, name: str = None) -&gt; Union[LineString, None]\n</code></pre> <p>Find shortest non-crossing path between two polygons.</p> <p>Tests all combinations of boundary points between two polygons to find the shortest connection that maintains minimum clearance from other areas. If the algorithm succeedes and finds a non-crossing LineString, it ensures clean visualization paths for energy market border analysis.</p> <p>Parameters:</p> Name Type Description Default <code>polygon1</code> <code>Polygon</code> <p>Source polygon geometry</p> required <code>polygon2</code> <code>Polygon</code> <p>Target polygon geometry</p> required <code>other_areas</code> <code>GeoDataFrame</code> <p>GeoDataFrame of areas to avoid crossing through</p> required <code>name</code> <code>str</code> <p>Optional name for progress tracking display</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[LineString, None]</code> <p>LineString or None: Shortest valid path between polygons, or None if no path meets clearance requirements</p> Algorithm <ol> <li>Sample boundary points for both polygons</li> <li>Buffer other areas by minimum clearance distance</li> <li>Test all point-to-point connections</li> <li>Filter out paths that cross buffered areas</li> <li>Return shortest valid path</li> </ol> Performance Scaling <ul> <li>Total paths tested: num_points\u00b2 </li> <li>With default num_points=100: tests 10,000 potential paths</li> <li>Computation time scales roughly O(n\u00b2 \u00d7 m) where m = number of other areas</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; path = finder.find_shortest_path(\n...     source_area, target_area, obstacles_gdf, \"Germany to UK\"\n... )\n&gt;&gt;&gt; if path:\n...     print(f\"Found path with length: {path.length:.0f} meters\")\n... else:\n...     print(\"No valid path found with current clearance settings\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def find_shortest_path(\n    self,\n    polygon1: Polygon,\n    polygon2: Polygon,\n    other_areas: gpd.GeoDataFrame,\n    name: str = None\n) -&gt; Union[LineString, None]:\n    \"\"\"Find shortest non-crossing path between two polygons.\n\n    Tests all combinations of boundary points between two polygons to find\n    the shortest connection that maintains minimum clearance from other areas.\n    If the algorithm succeedes and finds a non-crossing LineString, it ensures\n    clean visualization paths for energy market border analysis.\n\n    Args:\n        polygon1: Source polygon geometry\n        polygon2: Target polygon geometry\n        other_areas: GeoDataFrame of areas to avoid crossing through\n        name: Optional name for progress tracking display\n\n    Returns:\n        LineString or None: Shortest valid path between polygons, or None\n            if no path meets clearance requirements\n\n    Algorithm:\n        1. Sample boundary points for both polygons\n        2. Buffer other areas by minimum clearance distance\n        3. Test all point-to-point connections\n        4. Filter out paths that cross buffered areas\n        5. Return shortest valid path\n\n    Performance Scaling:\n        - Total paths tested: num_points\u00b2 \n        - With default num_points=100: tests 10,000 potential paths\n        - Computation time scales roughly O(n\u00b2 \u00d7 m) where m = number of other areas\n\n    Example:\n\n        &gt;&gt;&gt; path = finder.find_shortest_path(\n        ...     source_area, target_area, obstacles_gdf, \"Germany to UK\"\n        ... )\n        &gt;&gt;&gt; if path:\n        ...     print(f\"Found path with length: {path.length:.0f} meters\")\n        ... else:\n        ...     print(\"No valid path found with current clearance settings\")\n    \"\"\"\n    buffered_areas = self._buffer_areas(other_areas, self.min_clearance)\n    points1 = self._get_boundary_points(polygon1, self.num_points)\n    points2 = self._get_boundary_points(polygon2, self.num_points)\n\n    lines = [LineString([p1, p2]) for p1, p2 in itertools.product(points1, points2)]\n    shortest_line = None\n    min_length = float('inf')\n\n    iterator = tqdm(lines, desc=f\"Finding path for {name or 'path'}\") if self.show_progress else lines\n\n    for line in iterator:\n        if not buffered_areas.geometry.crosses(line).any():\n            if line.length &lt; min_length:\n                shortest_line = line\n                min_length = line.length\n\n    return shortest_line\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.model_generator_base.GeoModelGeneratorBase","title":"GeoModelGeneratorBase","text":"<p>Base class for generating geometric models with representative points.</p> <p>This class provides common functionality for working with geometric representations of energy system areas, including methods for computing representative points within polygons and multipolygons. It's designed to support energy market analysis where spatial aggregation of nodes into areas is required.</p> <p>The class supports two methods for computing representative points: - 'polylabel': Uses pole of inaccessibility algorithm for optimal label placement - 'representative_point': Uses Shapely's built-in representative point method</p> <p>Attributes:</p> Name Type Description <code>REPRESENTATIVE_POINT_METHOD</code> <code>str</code> <p>Method used for computing representative points ('polylabel' or 'representative_point')</p> <code>_polylabel_cache</code> <code>dict</code> <p>Cache for expensive polylabel calculations</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/model_generator_base.py</code> <pre><code>class GeoModelGeneratorBase:\n    \"\"\"Base class for generating geometric models with representative points.\n\n    This class provides common functionality for working with geometric representations\n    of energy system areas, including methods for computing representative points\n    within polygons and multipolygons. It's designed to support energy market\n    analysis where spatial aggregation of nodes into areas is required.\n\n    The class supports two methods for computing representative points:\n    - 'polylabel': Uses pole of inaccessibility algorithm for optimal label placement\n    - 'representative_point': Uses Shapely's built-in representative point method\n\n    Attributes:\n        REPRESENTATIVE_POINT_METHOD (str): Method used for computing representative\n            points ('polylabel' or 'representative_point')\n        _polylabel_cache (dict): Cache for expensive polylabel calculations\n    \"\"\"\n    REPRESENTATIVE_POINT_METHOD: Literal['polylabel', 'representative_point'] = 'polylabel'\n    _polylabel_cache = {}\n\n    def get_representative_area_point(self, geom: Union[Polygon, MultiPolygon]) -&gt; Point:\n        \"\"\"Get a representative point for a polygon or multipolygon geometry.\n\n        This method computes a point that is guaranteed to be inside the geometry\n        and is suitable for label placement or other visualization purposes in\n        energy system maps. For MultiPolygons, it operates on the largest constituent.\n\n        Args:\n            geom: A Shapely Polygon or MultiPolygon geometry representing an\n                energy system area (e.g., bidding zone, market region)\n\n        Returns:\n            Point: A Shapely Point guaranteed to be inside the input geometry,\n                suitable for map labels or representative location analysis\n\n        Raises:\n            ValueError: If REPRESENTATIVE_POINT_METHOD is not supported\n\n        Example:\n\n            &gt;&gt;&gt; from shapely.geometry import Polygon, Point\n            &gt;&gt;&gt; generator = GeoModelGeneratorBase()\n            &gt;&gt;&gt; area_polygon = Polygon([(0, 0), (10, 0), (10, 10), (0, 10)])\n            &gt;&gt;&gt; rep_point = generator.get_representative_area_point(area_polygon)\n            &gt;&gt;&gt; print(f\"Representative point: {rep_point.x:.2f}, {rep_point.y:.2f}\")\n        \"\"\"\n        if self.REPRESENTATIVE_POINT_METHOD == 'polylabel':\n            return self._get_polylabel_point(geom)\n        elif self.REPRESENTATIVE_POINT_METHOD == 'representative_point':\n            return geom.representative_point()\n        else:\n            raise ValueError(f'REPRESENTATIVE_POINT_METHOD {self.REPRESENTATIVE_POINT_METHOD} not supported')\n\n    def _get_polylabel_point(self, geom: Union[Polygon, MultiPolygon]) -&gt; Point:\n        \"\"\"Compute representative point using the polylabel algorithm.\n\n        The polylabel algorithm finds the pole of inaccessibility - the most distant\n        internal point from the polygon outline. This is particularly useful for\n        placing labels on complex energy system area geometries.\n\n        For MultiPolygons, operates on the largest polygon by area, which is\n        typically the main landmass for country/region representations.\n\n        Args:\n            geom: A Shapely Polygon or MultiPolygon geometry\n\n        Returns:\n            Point: The pole of inaccessibility point, cached for performance\n\n        Note:\n            Results are cached using the geometry's WKT representation as key.\n            The precision parameter (1.0) provides good balance between accuracy\n            and performance for typical energy system area sizes.\n        \"\"\"\n        key = geom.wkt\n        if key in self._polylabel_cache:\n            return self._polylabel_cache[key]\n\n        if isinstance(geom, MultiPolygon):\n            geom = max(geom.geoms, key=lambda g: g.area)\n\n        exterior = list(geom.exterior.coords)\n        holes = [list(ring.coords) for ring in geom.interiors]\n        rings = [exterior] + holes\n\n        point = Point(polylabel(rings, 1.0))\n        self._polylabel_cache[key] = point\n        return point\n\n    @staticmethod\n    def _compute_representative_point_from_cloud_of_2d_points(points: List[Point]) -&gt; Point:\n        \"\"\"Compute geometric centroid from a collection of 2D points.\n\n        This method is particularly useful in energy systems analysis for computing\n        representative locations of energy assets (e.g., power plants, substations)\n        that belong to the same area or region.\n\n        The algorithm adapts based on the number of input points:\n        - 1 point: Returns the point itself\n        - 2 points: Returns the midpoint\n        - \u22653 points: Computes convex hull and returns polygon centroid\n\n        Args:\n            points: List of Shapely Point objects representing energy asset\n                locations or other spatial features within an area\n\n        Returns:\n            Point: Representative point for the collection of input points\n\n        Raises:\n            ValueError: If the input list is empty\n\n        Example:\n\n            &gt;&gt;&gt; from shapely.geometry import Point\n            &gt;&gt;&gt; power_plants = [Point(1, 1), Point(3, 2), Point(2, 4)]\n            &gt;&gt;&gt; centroid = GeoModelGeneratorBase._compute_representative_point_from_cloud_of_2d_points(power_plants)\n            &gt;&gt;&gt; print(f\"Regional centroid: {centroid.x:.2f}, {centroid.y:.2f}\")\n        \"\"\"\n        from scipy.spatial import ConvexHull\n\n        n = len(points)\n        if n == 0:\n            raise ValueError(\"Empty point list provided - cannot compute representative point\")\n        if n == 1:\n            return points[0]\n        if n == 2:\n            return Point((points[0].x + points[1].x) / 2, (points[0].y + points[1].y) / 2)\n\n        # For 3+ points, compute convex hull and return centroid\n        coords = [(p.x, p.y) for p in points]\n        hull = ConvexHull(coords)\n        hull_coords = [coords[i] for i in hull.vertices]\n        polygon = Polygon(hull_coords)\n        return polygon.representative_point()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mescal.energy_data_handling.area_accounting.model_generator_base.GeoModelGeneratorBase.get_representative_area_point","title":"get_representative_area_point","text":"<pre><code>get_representative_area_point(geom: Union[Polygon, MultiPolygon]) -&gt; Point\n</code></pre> <p>Get a representative point for a polygon or multipolygon geometry.</p> <p>This method computes a point that is guaranteed to be inside the geometry and is suitable for label placement or other visualization purposes in energy system maps. For MultiPolygons, it operates on the largest constituent.</p> <p>Parameters:</p> Name Type Description Default <code>geom</code> <code>Union[Polygon, MultiPolygon]</code> <p>A Shapely Polygon or MultiPolygon geometry representing an energy system area (e.g., bidding zone, market region)</p> required <p>Returns:</p> Name Type Description <code>Point</code> <code>Point</code> <p>A Shapely Point guaranteed to be inside the input geometry, suitable for map labels or representative location analysis</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If REPRESENTATIVE_POINT_METHOD is not supported</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from shapely.geometry import Polygon, Point\n&gt;&gt;&gt; generator = GeoModelGeneratorBase()\n&gt;&gt;&gt; area_polygon = Polygon([(0, 0), (10, 0), (10, 10), (0, 10)])\n&gt;&gt;&gt; rep_point = generator.get_representative_area_point(area_polygon)\n&gt;&gt;&gt; print(f\"Representative point: {rep_point.x:.2f}, {rep_point.y:.2f}\")\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/area_accounting/model_generator_base.py</code> <pre><code>def get_representative_area_point(self, geom: Union[Polygon, MultiPolygon]) -&gt; Point:\n    \"\"\"Get a representative point for a polygon or multipolygon geometry.\n\n    This method computes a point that is guaranteed to be inside the geometry\n    and is suitable for label placement or other visualization purposes in\n    energy system maps. For MultiPolygons, it operates on the largest constituent.\n\n    Args:\n        geom: A Shapely Polygon or MultiPolygon geometry representing an\n            energy system area (e.g., bidding zone, market region)\n\n    Returns:\n        Point: A Shapely Point guaranteed to be inside the input geometry,\n            suitable for map labels or representative location analysis\n\n    Raises:\n        ValueError: If REPRESENTATIVE_POINT_METHOD is not supported\n\n    Example:\n\n        &gt;&gt;&gt; from shapely.geometry import Polygon, Point\n        &gt;&gt;&gt; generator = GeoModelGeneratorBase()\n        &gt;&gt;&gt; area_polygon = Polygon([(0, 0), (10, 0), (10, 10), (0, 10)])\n        &gt;&gt;&gt; rep_point = generator.get_representative_area_point(area_polygon)\n        &gt;&gt;&gt; print(f\"Representative point: {rep_point.x:.2f}, {rep_point.y:.2f}\")\n    \"\"\"\n    if self.REPRESENTATIVE_POINT_METHOD == 'polylabel':\n        return self._get_polylabel_point(geom)\n    elif self.REPRESENTATIVE_POINT_METHOD == 'representative_point':\n        return geom.representative_point()\n    else:\n        raise ValueError(f'REPRESENTATIVE_POINT_METHOD {self.REPRESENTATIVE_POINT_METHOD} not supported')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/","title":"MESCAL Membership Model Handling","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/#mescal.energy_data_handling.model_handling","title":"model_handling","text":"<p>Model handling utilities for MESCAL energy data processing.</p> <p>This package provides tools for enriching energy system DataFrames with properties from related model objects and creating combination columns for paired relationships.</p> Key Components <ul> <li>Membership property enrichers for adding related object properties to DataFrames</li> <li>Directional relationship handling for from/to column pairs in network data</li> <li>Membership pairs appenders for creating combination identifiers from paired columns</li> </ul> Example use cases <ul> <li>Enriching generator data with node properties   (e.g. generator_model_df then has node_voltage, node_country, ...)</li> <li>Enriching line_model_df with node_from and node_to characteristics   (e.g. region_from, region_to, voltage_from, voltage_to, ...)</li> </ul>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/","title":"MESCAL Membership Pairs Appender","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mescal.energy_data_handling.model_handling.membership_pairs_appender.BaseMembershipPairsAppender","title":"BaseMembershipPairsAppender","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for creating combination identifiers from paired energy system relationships.</p> <p>In energy system modeling, many entities have directional relationships that require unique identification for analysis. This class provides a unified framework for creating combination identifiers from paired columns, particularly useful for:</p> <ul> <li>Transmission lines connecting nodes (node_from/node_to combinations)</li> <li>Regional trade flows (region_from/region_to pairs)</li> <li>Pipeline connections (hub_from/hub_to relationships)</li> <li>Market interconnections (market_from/market_to links)</li> </ul> <p>The class supports three distinct combination strategies:</p> <ol> <li> <p>Directional: Preserves relationship direction (A\u2192B \u2260 B\u2192A)</p> <ul> <li>Essential for analyzing flow directions, capacity constraints, and directional costs</li> </ul> </li> <li> <p>Sorted: Creates bidirectional identifiers (A\u2192B = B\u2192A becomes A-B)</p> <ul> <li>Useful for identifying unique connections regardless of direction</li> </ul> </li> <li> <p>Opposite: Reverses relationship direction (A\u2192B becomes B\u2192A)</p> <ul> <li>Enables reverse flow analysis and bidirectional modeling</li> </ul> </li> </ol> <p>This abstraction enables different implementation strategies (string concatenation, tuple creation, etc.) while maintaining consistent naming patterns across MESCAL energy data models.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str</code> <p>Suffix/prefix identifying source/origin columns. Defaults to '_from'.</p> <code>'_from'</code> <code>to_identifier</code> <code>str</code> <p>Suffix/prefix identifying destination/target columns. Defaults to '_to'.</p> <code>'_to'</code> <code>combo_col_suffix</code> <code>str</code> <p>Suffix for directional combination column names. Defaults to '_combo'.</p> <code>'_combo'</code> <code>combo_col_prefix</code> <code>str</code> <p>Prefix for directional combination column names. Defaults to None.</p> <code>None</code> <code>sorted_combo_col_suffix</code> <code>str</code> <p>Suffix for sorted combination column names. Defaults to '_combo_sorted'.</p> <code>'_combo_sorted'</code> <code>sorted_combo_col_prefix</code> <code>str</code> <p>Prefix for sorted combination column names. Defaults to None.</p> <code>None</code> <code>opposite_combo_col_suffix</code> <code>str</code> <p>Suffix for opposite combination column names. Defaults to '_combo_opposite'.</p> <code>'_combo_opposite'</code> <code>opposite_combo_col_prefix</code> <code>str</code> <p>Prefix for opposite combination column names. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither suffix nor prefix is provided for any combination type.</p> Note <p>Either suffix or prefix must be specified for each combination type to ensure proper column naming conventions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # For transmission line analysis\n&gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n&gt;&gt;&gt; lines_df = appender.append_combo_columns(transmission_df)\n&gt;&gt;&gt; # Creates 'node_combo' column: 'NodeA \u2192 NodeB'\n\n&gt;&gt;&gt; # For bidirectional connections\n&gt;&gt;&gt; lines_df = appender.append_sorted_combo_columns(transmission_df)\n&gt;&gt;&gt; # Creates 'node_combo_sorted' column: 'NodeA \u2192 NodeB' (alphabetical)\n&gt;&gt;&gt; lines_df = appender.append_opposite_combo_columns(transmission_df)\n&gt;&gt;&gt; # Creates 'node_combo_opposite' column: 'NodeB \u2192 NodeA' (alphabetical)\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>class BaseMembershipPairsAppender(ABC):\n    \"\"\"Abstract base class for creating combination identifiers from paired energy system relationships.\n\n    In energy system modeling, many entities have directional relationships that require\n    unique identification for analysis. This class provides a unified framework for creating\n    combination identifiers from paired columns, particularly useful for:\n\n    - Transmission lines connecting nodes (node_from/node_to combinations)\n    - Regional trade flows (region_from/region_to pairs)\n    - Pipeline connections (hub_from/hub_to relationships)\n    - Market interconnections (market_from/market_to links)\n\n    The class supports three distinct combination strategies:\n\n    1. **Directional**: Preserves relationship direction (A\u2192B \u2260 B\u2192A)\n        - Essential for analyzing flow directions, capacity constraints, and directional costs\n\n    2. **Sorted**: Creates bidirectional identifiers (A\u2192B = B\u2192A becomes A-B)\n        - Useful for identifying unique connections regardless of direction\n\n    3. **Opposite**: Reverses relationship direction (A\u2192B becomes B\u2192A)\n        - Enables reverse flow analysis and bidirectional modeling\n\n    This abstraction enables different implementation strategies (string concatenation,\n    tuple creation, etc.) while maintaining consistent naming patterns across MESCAL\n    energy data models.\n\n    Args:\n        from_identifier: Suffix/prefix identifying source/origin columns. Defaults to '_from'.\n        to_identifier: Suffix/prefix identifying destination/target columns. Defaults to '_to'.\n        combo_col_suffix: Suffix for directional combination column names. Defaults to '_combo'.\n        combo_col_prefix: Prefix for directional combination column names. Defaults to None.\n        sorted_combo_col_suffix: Suffix for sorted combination column names. Defaults to '_combo_sorted'.\n        sorted_combo_col_prefix: Prefix for sorted combination column names. Defaults to None.\n        opposite_combo_col_suffix: Suffix for opposite combination column names. Defaults to '_combo_opposite'.\n        opposite_combo_col_prefix: Prefix for opposite combination column names. Defaults to None.\n\n    Raises:\n        ValueError: If neither suffix nor prefix is provided for any combination type.\n\n    Note:\n        Either suffix or prefix must be specified for each combination type to ensure\n        proper column naming conventions.\n\n    Examples:\n\n        &gt;&gt;&gt; # For transmission line analysis\n        &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n        &gt;&gt;&gt; lines_df = appender.append_combo_columns(transmission_df)\n        &gt;&gt;&gt; # Creates 'node_combo' column: 'NodeA \u2192 NodeB'\n\n        &gt;&gt;&gt; # For bidirectional connections\n        &gt;&gt;&gt; lines_df = appender.append_sorted_combo_columns(transmission_df)\n        &gt;&gt;&gt; # Creates 'node_combo_sorted' column: 'NodeA \u2192 NodeB' (alphabetical)\n        &gt;&gt;&gt; lines_df = appender.append_opposite_combo_columns(transmission_df)\n        &gt;&gt;&gt; # Creates 'node_combo_opposite' column: 'NodeB \u2192 NodeA' (alphabetical)\n    \"\"\"\n    def __init__(\n            self,\n            from_identifier: str = '_from',\n            to_identifier: str = '_to',\n            combo_col_suffix: str = '_combo',\n            combo_col_prefix: str = None,\n            sorted_combo_col_suffix: str = '_combo_sorted',\n            sorted_combo_col_prefix: str = None,\n            opposite_combo_col_suffix: str = '_combo_opposite',\n            opposite_combo_col_prefix: str = None,\n    ):\n        self._from_identifier = from_identifier\n        self._to_identifier = to_identifier\n\n        self._combo_col_suffix = combo_col_suffix or ''\n        self._combo_col_prefix = combo_col_prefix or ''\n        if not any([self._combo_col_suffix, self._combo_col_prefix]):\n            raise ValueError(\"Must specify either suffix or prefix for combo columns\")\n\n        self._sorted_combo_col_suffix = sorted_combo_col_suffix or ''\n        self._sorted_combo_col_prefix = sorted_combo_col_prefix or ''\n        if not any([self._sorted_combo_col_suffix, self._sorted_combo_col_prefix]):\n            raise ValueError(\"Must specify either suffix or prefix for sorted combo columns\")\n\n        self._opposite_combo_col_suffix = opposite_combo_col_suffix or ''\n        self._opposite_combo_col_prefix = opposite_combo_col_prefix or ''\n        if not any([self._opposite_combo_col_suffix, self._opposite_combo_col_prefix]):\n            raise ValueError(\"Must specify either suffix or prefix for opposite combo columns\")\n\n        self._common_base_key_finder = CommonBaseKeyFinder(from_identifier, to_identifier)\n\n    def append_combo_columns(\n            self,\n            df_with_from_to_columns: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Creates directional combination columns preserving relationship direction.\n\n        Generates combination identifiers that maintain the original direction of\n        relationships, essential for energy system analysis where flow direction,\n        capacity constraints, and directional costs matter.\n\n        This method automatically identifies all paired columns (those with matching\n        base names plus from/to identifiers) and creates directional combinations\n        using the configured naming strategy.\n\n        Args:\n            df_with_from_to_columns: DataFrame containing paired relationship columns\n                                   (e.g., 'node_from'/'node_to', 'region_from'/'region_to')\n\n        Returns:\n            Enhanced DataFrame with directional combination columns added.\n            Original data preserved, new columns follow configured naming pattern.\n\n        Examples:\n            Transmission line directional analysis:\n\n            &gt;&gt;&gt; # DataFrame with node connections\n            &gt;&gt;&gt; lines_df = pd.DataFrame({\n            ...     'line_id': ['L1', 'L2', 'L3'],\n            ...     'node_from': ['NodeA', 'NodeB', 'NodeC'],\n            ...     'node_to': ['NodeB', 'NodeA', 'NodeA'],\n            ...     'capacity_mw': [1000, 800, 600]\n            ... })\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n            &gt;&gt;&gt; result = appender.append_combo_columns(lines_df)\n            &gt;&gt;&gt; # Result includes 'node_combo': ['NodeA \u2192 NodeB', 'NodeB \u2192 NodeA', 'NodeC \u2192 NodeA']\n        \"\"\"\n        return self._append_combo_columns(df_with_from_to_columns, 'directional')\n\n    def append_sorted_combo_columns(\n            self,\n            df_with_from_to_columns: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Creates bidirectional combination columns with alphabetical ordering.\n\n        Generates combination identifiers that treat relationships as bidirectional\n        by sorting the paired values alphabetically. This is particularly useful for\n        identifying unique connections regardless of direction, such as:\n\n        - Transmission line corridors (same physical connection)\n        - Regional trade partnerships (bidirectional trade agreements)\n        - Pipeline systems (flow can be reversed)\n        - Market coupling arrangements (mutual price influence)\n\n        Args:\n            df_with_from_to_columns: DataFrame containing paired relationship columns\n                                   with potential bidirectional connections\n\n        Returns:\n            Enhanced DataFrame with sorted combination columns added.\n            Bidirectional relationships receive identical identifiers.\n\n        Examples:\n            Bidirectional connection analysis:\n\n            &gt;&gt;&gt; # DataFrame with potentially bidirectional connections\n            &gt;&gt;&gt; connections_df = pd.DataFrame({\n            ...     'connection_id': ['C1', 'C2', 'C3'],\n            ...     'region_from': ['DE', 'FR', 'DE'],\n            ...     'region_to': ['FR', 'DE', 'NL'],\n            ...     'trade_capacity': [2000, 2000, 1500]\n            ... })\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator='-')\n            &gt;&gt;&gt; result = appender.append_sorted_combo_columns(connections_df)\n            &gt;&gt;&gt; # Result includes 'region_combo_sorted': ['DE-FR', 'DE-FR', 'DE-NL']\n            &gt;&gt;&gt; # Note: 'DE\u2192FR' and 'FR\u2192DE' both become 'DE-FR'\n        \"\"\"\n        return self._append_combo_columns(df_with_from_to_columns, 'sorted')\n\n    def append_opposite_combo_columns(\n            self,\n            df_with_from_to_columns: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Creates reverse-direction combination columns for opposite flow analysis.\n\n        Generates combination identifiers with reversed direction, enabling analysis\n        of reverse flows, return paths, and bidirectional modeling scenarios. This\n        is particularly valuable for:\n\n        - Reverse power flows in transmission networks\n        - Return commodity flows in pipeline systems\n        - Opposite direction trade flows\n        - Backup routing analysis\n\n        The method swaps the from/to values before creating combinations, effectively\n        creating the opposite directional identifier for each relationship.\n\n        Args:\n            df_with_from_to_columns: DataFrame containing directional relationships\n                                   where reverse analysis is needed\n\n        Returns:\n            Enhanced DataFrame with opposite-direction combination columns added.\n            Each relationship receives its reverse-direction identifier.\n\n        Examples:\n            Reverse flow analysis:\n\n            &gt;&gt;&gt; # DataFrame with primary flow directions\n            &gt;&gt;&gt; flows_df = pd.DataFrame({\n            ...     'flow_id': ['F1', 'F2', 'F3'],\n            ...     'hub_from': ['HubA', 'HubB', 'HubC'],\n            ...     'hub_to': ['HubB', 'HubC', 'HubA'],\n            ...     'primary_flow': [100, 150, 80]\n            ... })\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2190 ')\n            &gt;&gt;&gt; result = appender.append_opposite_combo_columns(flows_df)\n            &gt;&gt;&gt; # Result includes 'hub_combo_opposite': ['HubB \u2190 HubA', 'HubC \u2190 HubB', 'HubA \u2190 HubC']\n            &gt;&gt;&gt; # Useful for modeling reverse flow scenarios\n        \"\"\"\n        return self._append_combo_columns(df_with_from_to_columns, 'opposite')\n\n    def _append_combo_columns(\n            self,\n            df_with_from_to_columns: pd.DataFrame,\n            which_combo: Literal['directional', 'sorted', 'opposite']\n    ) -&gt; pd.DataFrame:\n        from_id = self._from_identifier\n        to_id = self._to_identifier\n\n        _col_names = df_with_from_to_columns.columns\n        base_columns = self._common_base_key_finder.get_keys_for_which_all_association_tags_appear(_col_names)\n\n        for base in base_columns:\n            from_col = f'{base}{from_id}'\n            to_col = f'{base}{to_id}'\n\n            _from_values = df_with_from_to_columns[from_col]\n            _to_values = df_with_from_to_columns[to_col]\n\n            if which_combo == 'directional':\n                new_col = f'{self._combo_col_prefix}{base}{self._combo_col_suffix}'\n                new_values = self._combine_values(_from_values, _to_values)\n            elif which_combo == 'sorted':\n                new_col = f'{self._sorted_combo_col_prefix}{base}{self._sorted_combo_col_suffix}'\n                new_values = self._combine_sorted_values(_from_values, _to_values)\n            elif which_combo == 'opposite':\n                new_col = f'{self._opposite_combo_col_prefix}{base}{self._opposite_combo_col_suffix}'\n                new_values = self._combine_values(_to_values, _from_values)\n            else:\n                raise NotImplementedError\n\n            df_with_from_to_columns = set_column(df_with_from_to_columns, new_col, new_values)\n\n        return df_with_from_to_columns\n\n    @abstractmethod\n    def _combine_values(self, a: pd.Series, b: pd.Series) -&gt; pd.Series:\n        pass\n\n    @abstractmethod\n    def _combine_sorted_values(self, a: pd.Series, b: pd.Series) -&gt; pd.Series:\n        pass\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mescal.energy_data_handling.model_handling.membership_pairs_appender.BaseMembershipPairsAppender.append_combo_columns","title":"append_combo_columns","text":"<pre><code>append_combo_columns(df_with_from_to_columns: DataFrame) -&gt; DataFrame\n</code></pre> <p>Creates directional combination columns preserving relationship direction.</p> <p>Generates combination identifiers that maintain the original direction of relationships, essential for energy system analysis where flow direction, capacity constraints, and directional costs matter.</p> <p>This method automatically identifies all paired columns (those with matching base names plus from/to identifiers) and creates directional combinations using the configured naming strategy.</p> <p>Parameters:</p> Name Type Description Default <code>df_with_from_to_columns</code> <code>DataFrame</code> <p>DataFrame containing paired relationship columns                    (e.g., 'node_from'/'node_to', 'region_from'/'region_to')</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced DataFrame with directional combination columns added.</p> <code>DataFrame</code> <p>Original data preserved, new columns follow configured naming pattern.</p> <p>Examples:</p> <p>Transmission line directional analysis:</p> <pre><code>&gt;&gt;&gt; # DataFrame with node connections\n&gt;&gt;&gt; lines_df = pd.DataFrame({\n...     'line_id': ['L1', 'L2', 'L3'],\n...     'node_from': ['NodeA', 'NodeB', 'NodeC'],\n...     'node_to': ['NodeB', 'NodeA', 'NodeA'],\n...     'capacity_mw': [1000, 800, 600]\n... })\n&gt;&gt;&gt; \n&gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n&gt;&gt;&gt; result = appender.append_combo_columns(lines_df)\n&gt;&gt;&gt; # Result includes 'node_combo': ['NodeA \u2192 NodeB', 'NodeB \u2192 NodeA', 'NodeC \u2192 NodeA']\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>def append_combo_columns(\n        self,\n        df_with_from_to_columns: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Creates directional combination columns preserving relationship direction.\n\n    Generates combination identifiers that maintain the original direction of\n    relationships, essential for energy system analysis where flow direction,\n    capacity constraints, and directional costs matter.\n\n    This method automatically identifies all paired columns (those with matching\n    base names plus from/to identifiers) and creates directional combinations\n    using the configured naming strategy.\n\n    Args:\n        df_with_from_to_columns: DataFrame containing paired relationship columns\n                               (e.g., 'node_from'/'node_to', 'region_from'/'region_to')\n\n    Returns:\n        Enhanced DataFrame with directional combination columns added.\n        Original data preserved, new columns follow configured naming pattern.\n\n    Examples:\n        Transmission line directional analysis:\n\n        &gt;&gt;&gt; # DataFrame with node connections\n        &gt;&gt;&gt; lines_df = pd.DataFrame({\n        ...     'line_id': ['L1', 'L2', 'L3'],\n        ...     'node_from': ['NodeA', 'NodeB', 'NodeC'],\n        ...     'node_to': ['NodeB', 'NodeA', 'NodeA'],\n        ...     'capacity_mw': [1000, 800, 600]\n        ... })\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n        &gt;&gt;&gt; result = appender.append_combo_columns(lines_df)\n        &gt;&gt;&gt; # Result includes 'node_combo': ['NodeA \u2192 NodeB', 'NodeB \u2192 NodeA', 'NodeC \u2192 NodeA']\n    \"\"\"\n    return self._append_combo_columns(df_with_from_to_columns, 'directional')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mescal.energy_data_handling.model_handling.membership_pairs_appender.BaseMembershipPairsAppender.append_sorted_combo_columns","title":"append_sorted_combo_columns","text":"<pre><code>append_sorted_combo_columns(df_with_from_to_columns: DataFrame) -&gt; DataFrame\n</code></pre> <p>Creates bidirectional combination columns with alphabetical ordering.</p> <p>Generates combination identifiers that treat relationships as bidirectional by sorting the paired values alphabetically. This is particularly useful for identifying unique connections regardless of direction, such as:</p> <ul> <li>Transmission line corridors (same physical connection)</li> <li>Regional trade partnerships (bidirectional trade agreements)</li> <li>Pipeline systems (flow can be reversed)</li> <li>Market coupling arrangements (mutual price influence)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>df_with_from_to_columns</code> <code>DataFrame</code> <p>DataFrame containing paired relationship columns                    with potential bidirectional connections</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced DataFrame with sorted combination columns added.</p> <code>DataFrame</code> <p>Bidirectional relationships receive identical identifiers.</p> <p>Examples:</p> <p>Bidirectional connection analysis:</p> <pre><code>&gt;&gt;&gt; # DataFrame with potentially bidirectional connections\n&gt;&gt;&gt; connections_df = pd.DataFrame({\n...     'connection_id': ['C1', 'C2', 'C3'],\n...     'region_from': ['DE', 'FR', 'DE'],\n...     'region_to': ['FR', 'DE', 'NL'],\n...     'trade_capacity': [2000, 2000, 1500]\n... })\n&gt;&gt;&gt; \n&gt;&gt;&gt; appender = StringMembershipPairsAppender(separator='-')\n&gt;&gt;&gt; result = appender.append_sorted_combo_columns(connections_df)\n&gt;&gt;&gt; # Result includes 'region_combo_sorted': ['DE-FR', 'DE-FR', 'DE-NL']\n&gt;&gt;&gt; # Note: 'DE\u2192FR' and 'FR\u2192DE' both become 'DE-FR'\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>def append_sorted_combo_columns(\n        self,\n        df_with_from_to_columns: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Creates bidirectional combination columns with alphabetical ordering.\n\n    Generates combination identifiers that treat relationships as bidirectional\n    by sorting the paired values alphabetically. This is particularly useful for\n    identifying unique connections regardless of direction, such as:\n\n    - Transmission line corridors (same physical connection)\n    - Regional trade partnerships (bidirectional trade agreements)\n    - Pipeline systems (flow can be reversed)\n    - Market coupling arrangements (mutual price influence)\n\n    Args:\n        df_with_from_to_columns: DataFrame containing paired relationship columns\n                               with potential bidirectional connections\n\n    Returns:\n        Enhanced DataFrame with sorted combination columns added.\n        Bidirectional relationships receive identical identifiers.\n\n    Examples:\n        Bidirectional connection analysis:\n\n        &gt;&gt;&gt; # DataFrame with potentially bidirectional connections\n        &gt;&gt;&gt; connections_df = pd.DataFrame({\n        ...     'connection_id': ['C1', 'C2', 'C3'],\n        ...     'region_from': ['DE', 'FR', 'DE'],\n        ...     'region_to': ['FR', 'DE', 'NL'],\n        ...     'trade_capacity': [2000, 2000, 1500]\n        ... })\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator='-')\n        &gt;&gt;&gt; result = appender.append_sorted_combo_columns(connections_df)\n        &gt;&gt;&gt; # Result includes 'region_combo_sorted': ['DE-FR', 'DE-FR', 'DE-NL']\n        &gt;&gt;&gt; # Note: 'DE\u2192FR' and 'FR\u2192DE' both become 'DE-FR'\n    \"\"\"\n    return self._append_combo_columns(df_with_from_to_columns, 'sorted')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mescal.energy_data_handling.model_handling.membership_pairs_appender.BaseMembershipPairsAppender.append_opposite_combo_columns","title":"append_opposite_combo_columns","text":"<pre><code>append_opposite_combo_columns(df_with_from_to_columns: DataFrame) -&gt; DataFrame\n</code></pre> <p>Creates reverse-direction combination columns for opposite flow analysis.</p> <p>Generates combination identifiers with reversed direction, enabling analysis of reverse flows, return paths, and bidirectional modeling scenarios. This is particularly valuable for:</p> <ul> <li>Reverse power flows in transmission networks</li> <li>Return commodity flows in pipeline systems</li> <li>Opposite direction trade flows</li> <li>Backup routing analysis</li> </ul> <p>The method swaps the from/to values before creating combinations, effectively creating the opposite directional identifier for each relationship.</p> <p>Parameters:</p> Name Type Description Default <code>df_with_from_to_columns</code> <code>DataFrame</code> <p>DataFrame containing directional relationships                    where reverse analysis is needed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced DataFrame with opposite-direction combination columns added.</p> <code>DataFrame</code> <p>Each relationship receives its reverse-direction identifier.</p> <p>Examples:</p> <p>Reverse flow analysis:</p> <pre><code>&gt;&gt;&gt; # DataFrame with primary flow directions\n&gt;&gt;&gt; flows_df = pd.DataFrame({\n...     'flow_id': ['F1', 'F2', 'F3'],\n...     'hub_from': ['HubA', 'HubB', 'HubC'],\n...     'hub_to': ['HubB', 'HubC', 'HubA'],\n...     'primary_flow': [100, 150, 80]\n... })\n&gt;&gt;&gt; \n&gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2190 ')\n&gt;&gt;&gt; result = appender.append_opposite_combo_columns(flows_df)\n&gt;&gt;&gt; # Result includes 'hub_combo_opposite': ['HubB \u2190 HubA', 'HubC \u2190 HubB', 'HubA \u2190 HubC']\n&gt;&gt;&gt; # Useful for modeling reverse flow scenarios\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>def append_opposite_combo_columns(\n        self,\n        df_with_from_to_columns: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Creates reverse-direction combination columns for opposite flow analysis.\n\n    Generates combination identifiers with reversed direction, enabling analysis\n    of reverse flows, return paths, and bidirectional modeling scenarios. This\n    is particularly valuable for:\n\n    - Reverse power flows in transmission networks\n    - Return commodity flows in pipeline systems\n    - Opposite direction trade flows\n    - Backup routing analysis\n\n    The method swaps the from/to values before creating combinations, effectively\n    creating the opposite directional identifier for each relationship.\n\n    Args:\n        df_with_from_to_columns: DataFrame containing directional relationships\n                               where reverse analysis is needed\n\n    Returns:\n        Enhanced DataFrame with opposite-direction combination columns added.\n        Each relationship receives its reverse-direction identifier.\n\n    Examples:\n        Reverse flow analysis:\n\n        &gt;&gt;&gt; # DataFrame with primary flow directions\n        &gt;&gt;&gt; flows_df = pd.DataFrame({\n        ...     'flow_id': ['F1', 'F2', 'F3'],\n        ...     'hub_from': ['HubA', 'HubB', 'HubC'],\n        ...     'hub_to': ['HubB', 'HubC', 'HubA'],\n        ...     'primary_flow': [100, 150, 80]\n        ... })\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2190 ')\n        &gt;&gt;&gt; result = appender.append_opposite_combo_columns(flows_df)\n        &gt;&gt;&gt; # Result includes 'hub_combo_opposite': ['HubB \u2190 HubA', 'HubC \u2190 HubB', 'HubA \u2190 HubC']\n        &gt;&gt;&gt; # Useful for modeling reverse flow scenarios\n    \"\"\"\n    return self._append_combo_columns(df_with_from_to_columns, 'opposite')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mescal.energy_data_handling.model_handling.membership_pairs_appender.StringMembershipPairsAppender","title":"StringMembershipPairsAppender","text":"<p>               Bases: <code>BaseMembershipPairsAppender</code></p> <p>String-based implementation for creating combination identifiers from energy system relationships.</p> <p>This concrete implementation creates human-readable string combinations from paired relationships using configurable separators. Particularly well-suited for:</p> <ul> <li>Data visualization and reporting (readable connection labels)</li> <li>User interface displays (network connection names)</li> <li>Export formats requiring string identifiers</li> <li>Debugging and data exploration</li> </ul> <p>The class inherits all combination strategies from the base class while implementing string-specific combination logic with customizable separators for different use cases.</p> <p>Parameters:</p> Name Type Description Default <code>separator</code> <code>str</code> <p>String used to join paired values in combinations. Defaults to ' - '.       Common patterns: ' \u2192 ' (directional), '-' (neutral), ' &lt;-&gt; ' (bidirectional)</p> <code>' - '</code> <code>**kwargs</code> <p>All arguments from BaseMembershipPairsAppender for column naming configuration</p> required <p>Examples:</p> <p>Energy system string combinations:</p> <pre><code>&gt;&gt;&gt; # Transmission line connections with directional separator\n&gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n&gt;&gt;&gt; lines_with_combos = appender.append_combo_columns(transmission_df)\n&gt;&gt;&gt; # Creates readable labels: 'NodeA \u2192 NodeB', 'NodeB \u2192 NodeC'\n</code></pre> <pre><code>&gt;&gt;&gt; # Regional trade connections with neutral separator\n&gt;&gt;&gt; trade_appender = StringMembershipPairsAppender(separator='-')\n&gt;&gt;&gt; trade_with_combos = trade_appender.append_sorted_combo_columns(trade_df)\n&gt;&gt;&gt; # Creates trade corridor labels: 'DE-FR', 'FR-NL'\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>class StringMembershipPairsAppender(BaseMembershipPairsAppender):\n    \"\"\"String-based implementation for creating combination identifiers from energy system relationships.\n\n    This concrete implementation creates human-readable string combinations from paired\n    relationships using configurable separators. Particularly well-suited for:\n\n    - Data visualization and reporting (readable connection labels)\n    - User interface displays (network connection names)\n    - Export formats requiring string identifiers\n    - Debugging and data exploration\n\n    The class inherits all combination strategies from the base class while implementing\n    string-specific combination logic with customizable separators for different use cases.\n\n    Args:\n        separator: String used to join paired values in combinations. Defaults to ' - '.\n                  Common patterns: ' \u2192 ' (directional), '-' (neutral), ' &lt;-&gt; ' (bidirectional)\n        **kwargs: All arguments from BaseMembershipPairsAppender for column naming configuration\n\n    Examples:\n        Energy system string combinations:\n\n        &gt;&gt;&gt; # Transmission line connections with directional separator\n        &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n        &gt;&gt;&gt; lines_with_combos = appender.append_combo_columns(transmission_df)\n        &gt;&gt;&gt; # Creates readable labels: 'NodeA \u2192 NodeB', 'NodeB \u2192 NodeC'\n\n        &gt;&gt;&gt; # Regional trade connections with neutral separator\n        &gt;&gt;&gt; trade_appender = StringMembershipPairsAppender(separator='-')\n        &gt;&gt;&gt; trade_with_combos = trade_appender.append_sorted_combo_columns(trade_df)\n        &gt;&gt;&gt; # Creates trade corridor labels: 'DE-FR', 'FR-NL'\n    \"\"\"\n    def __init__(\n            self,\n            from_identifier: str = '_from',\n            to_identifier: str = '_to',\n            combo_col_suffix: str = '_combo',\n            combo_col_prefix: str = None,\n            sorted_combo_col_suffix: str = '_combo_sorted',\n            sorted_combo_col_prefix: str = None,\n            opposite_combo_col_suffix: str = '_combo_opposite',\n            opposite_combo_col_prefix: str = None,\n            separator: str = ' - ',\n    ):\n        \"\"\"Initialize the string-based membership pairs appender.\n\n        Args:\n            from_identifier: Suffix/prefix identifying source/origin columns. Defaults to '_from'.\n            to_identifier: Suffix/prefix identifying destination/target columns. Defaults to '_to'.\n            combo_col_suffix: Suffix for directional combination column names. Defaults to '_combo'.\n            combo_col_prefix: Prefix for directional combination column names. Defaults to None.\n            sorted_combo_col_suffix: Suffix for sorted combination column names. Defaults to '_combo_sorted'.\n            sorted_combo_col_prefix: Prefix for sorted combination column names. Defaults to None.\n            opposite_combo_col_suffix: Suffix for opposite combination column names. Defaults to '_combo_opposite'.\n            opposite_combo_col_prefix: Prefix for opposite combination column names. Defaults to None.\n            separator: String separator for joining paired values. Defaults to ' - '.\n                      Use ' \u2192 ' for directional flows, '-' for neutral connections,\n                      ' &lt;-&gt; ' for bidirectional relationships.\n        \"\"\"\n        super().__init__(\n            from_identifier=from_identifier,\n            to_identifier=to_identifier,\n            combo_col_suffix=combo_col_suffix,\n            combo_col_prefix=combo_col_prefix,\n            sorted_combo_col_suffix=sorted_combo_col_suffix,\n            sorted_combo_col_prefix=sorted_combo_col_prefix,\n            opposite_combo_col_suffix=opposite_combo_col_suffix,\n            opposite_combo_col_prefix=opposite_combo_col_prefix,\n        )\n        self._separator = separator\n\n    def _combine_values(self, a: pd.Series, b: pd.Series) -&gt; pd.Series:\n        \"\"\"Combines two series into directional string combinations.\n\n        Args:\n            a: Source/origin values (from column)\n            b: Destination/target values (to column)\n\n        Returns:\n            Series with string combinations preserving a\u2192b direction\n        \"\"\"\n        return a.astype(str) + self._separator + b.astype(str)\n\n    def _combine_sorted_values(self, a: pd.Series, b: pd.Series) -&gt; pd.Series:\n        \"\"\"Combines two series into bidirectional string combinations with alphabetical ordering.\n\n        Args:\n            a: First set of relationship values\n            b: Second set of relationship values\n\n        Returns:\n            Series with alphabetically sorted string combinations (bidirectional)\n        \"\"\"\n        return pd.DataFrame({\n            'a': a.astype(str),\n            'b': b.astype(str)\n        }).apply(lambda x: self._separator.join(sorted([x['a'], x['b']])), axis=1)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mescal.energy_data_handling.model_handling.membership_pairs_appender.StringMembershipPairsAppender.__init__","title":"__init__","text":"<pre><code>__init__(from_identifier: str = '_from', to_identifier: str = '_to', combo_col_suffix: str = '_combo', combo_col_prefix: str = None, sorted_combo_col_suffix: str = '_combo_sorted', sorted_combo_col_prefix: str = None, opposite_combo_col_suffix: str = '_combo_opposite', opposite_combo_col_prefix: str = None, separator: str = ' - ')\n</code></pre> <p>Initialize the string-based membership pairs appender.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str</code> <p>Suffix/prefix identifying source/origin columns. Defaults to '_from'.</p> <code>'_from'</code> <code>to_identifier</code> <code>str</code> <p>Suffix/prefix identifying destination/target columns. Defaults to '_to'.</p> <code>'_to'</code> <code>combo_col_suffix</code> <code>str</code> <p>Suffix for directional combination column names. Defaults to '_combo'.</p> <code>'_combo'</code> <code>combo_col_prefix</code> <code>str</code> <p>Prefix for directional combination column names. Defaults to None.</p> <code>None</code> <code>sorted_combo_col_suffix</code> <code>str</code> <p>Suffix for sorted combination column names. Defaults to '_combo_sorted'.</p> <code>'_combo_sorted'</code> <code>sorted_combo_col_prefix</code> <code>str</code> <p>Prefix for sorted combination column names. Defaults to None.</p> <code>None</code> <code>opposite_combo_col_suffix</code> <code>str</code> <p>Suffix for opposite combination column names. Defaults to '_combo_opposite'.</p> <code>'_combo_opposite'</code> <code>opposite_combo_col_prefix</code> <code>str</code> <p>Prefix for opposite combination column names. Defaults to None.</p> <code>None</code> <code>separator</code> <code>str</code> <p>String separator for joining paired values. Defaults to ' - '.       Use ' \u2192 ' for directional flows, '-' for neutral connections,       ' &lt;-&gt; ' for bidirectional relationships.</p> <code>' - '</code> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>def __init__(\n        self,\n        from_identifier: str = '_from',\n        to_identifier: str = '_to',\n        combo_col_suffix: str = '_combo',\n        combo_col_prefix: str = None,\n        sorted_combo_col_suffix: str = '_combo_sorted',\n        sorted_combo_col_prefix: str = None,\n        opposite_combo_col_suffix: str = '_combo_opposite',\n        opposite_combo_col_prefix: str = None,\n        separator: str = ' - ',\n):\n    \"\"\"Initialize the string-based membership pairs appender.\n\n    Args:\n        from_identifier: Suffix/prefix identifying source/origin columns. Defaults to '_from'.\n        to_identifier: Suffix/prefix identifying destination/target columns. Defaults to '_to'.\n        combo_col_suffix: Suffix for directional combination column names. Defaults to '_combo'.\n        combo_col_prefix: Prefix for directional combination column names. Defaults to None.\n        sorted_combo_col_suffix: Suffix for sorted combination column names. Defaults to '_combo_sorted'.\n        sorted_combo_col_prefix: Prefix for sorted combination column names. Defaults to None.\n        opposite_combo_col_suffix: Suffix for opposite combination column names. Defaults to '_combo_opposite'.\n        opposite_combo_col_prefix: Prefix for opposite combination column names. Defaults to None.\n        separator: String separator for joining paired values. Defaults to ' - '.\n                  Use ' \u2192 ' for directional flows, '-' for neutral connections,\n                  ' &lt;-&gt; ' for bidirectional relationships.\n    \"\"\"\n    super().__init__(\n        from_identifier=from_identifier,\n        to_identifier=to_identifier,\n        combo_col_suffix=combo_col_suffix,\n        combo_col_prefix=combo_col_prefix,\n        sorted_combo_col_suffix=sorted_combo_col_suffix,\n        sorted_combo_col_prefix=sorted_combo_col_prefix,\n        opposite_combo_col_suffix=opposite_combo_col_suffix,\n        opposite_combo_col_prefix=opposite_combo_col_prefix,\n    )\n    self._separator = separator\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mescal.energy_data_handling.model_handling.membership_pairs_appender.TupleMembershipPairsAppender","title":"TupleMembershipPairsAppender","text":"<p>               Bases: <code>BaseMembershipPairsAppender</code></p> <p>Tuple-based implementation for creating combination identifiers from energy system relationships.</p> <p>This concrete implementation creates tuple combinations from paired relationships, offering several advantages for programmatic use:</p> <ul> <li>Memory efficiency (no string concatenation overhead)</li> <li>Fast equality comparisons and set operations</li> <li>Preservation of original data types</li> <li>Direct use as dictionary keys or index values</li> <li>Integration with pandas MultiIndex structures</li> </ul> <p>Particularly valuable for:</p> <ul> <li>High-performance energy system simulations</li> <li>Large-scale network analysis</li> <li>Optimization model formulations</li> <li>Internal data processing pipelines</li> </ul> <p>The tuple format maintains the exact relationship structure while enabling efficient programmatic manipulation of connection data.</p> <p>Examples:</p> <p>Energy system tuple combinations:</p> <pre><code>&gt;&gt;&gt; # Transmission network with tuple identifiers\n&gt;&gt;&gt; appender = TupleMembershipPairsAppender()\n&gt;&gt;&gt; lines_with_tuples = appender.append_combo_columns(transmission_df)\n&gt;&gt;&gt; # Creates tuple identifiers: ('NodeA', 'NodeB'), ('NodeB', 'NodeC')\n</code></pre> <pre><code>&gt;&gt;&gt; # Bidirectional connections for optimization\n&gt;&gt;&gt; lines_bidirectional = appender.append_sorted_combo_columns(transmission_df)\n&gt;&gt;&gt; # Creates sorted tuples: ('NodeA', 'NodeB'), ('NodeB', 'NodeC')\n&gt;&gt;&gt; # Useful as keys in optimization constraints\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>class TupleMembershipPairsAppender(BaseMembershipPairsAppender):\n    \"\"\"Tuple-based implementation for creating combination identifiers from energy system relationships.\n\n    This concrete implementation creates tuple combinations from paired relationships,\n    offering several advantages for programmatic use:\n\n    - Memory efficiency (no string concatenation overhead)\n    - Fast equality comparisons and set operations\n    - Preservation of original data types\n    - Direct use as dictionary keys or index values\n    - Integration with pandas MultiIndex structures\n\n    Particularly valuable for:\n\n    - High-performance energy system simulations\n    - Large-scale network analysis\n    - Optimization model formulations\n    - Internal data processing pipelines\n\n    The tuple format maintains the exact relationship structure while enabling\n    efficient programmatic manipulation of connection data.\n\n    Examples:\n        Energy system tuple combinations:\n\n        &gt;&gt;&gt; # Transmission network with tuple identifiers\n        &gt;&gt;&gt; appender = TupleMembershipPairsAppender()\n        &gt;&gt;&gt; lines_with_tuples = appender.append_combo_columns(transmission_df)\n        &gt;&gt;&gt; # Creates tuple identifiers: ('NodeA', 'NodeB'), ('NodeB', 'NodeC')\n\n        &gt;&gt;&gt; # Bidirectional connections for optimization\n        &gt;&gt;&gt; lines_bidirectional = appender.append_sorted_combo_columns(transmission_df)\n        &gt;&gt;&gt; # Creates sorted tuples: ('NodeA', 'NodeB'), ('NodeB', 'NodeC')\n        &gt;&gt;&gt; # Useful as keys in optimization constraints\n    \"\"\"\n    def _combine_values(self, a: pd.Series, b: pd.Series) -&gt; pd.Series:\n        \"\"\"Combines two series into directional tuple combinations.\n\n        Args:\n            a: Source/origin values (from column)\n            b: Destination/target values (to column)\n\n        Returns:\n            Series with tuple combinations preserving (a, b) direction\n        \"\"\"\n        return pd.Series([tuple(x) for x in zip(a, b)], index=a.index)\n\n    def _combine_sorted_values(self, a: pd.Series, b: pd.Series) -&gt; pd.Series:\n        \"\"\"Combines two series into bidirectional tuple combinations with alphabetical ordering.\n\n        Args:\n            a: First set of relationship values\n            b: Second set of relationship values\n\n        Returns:\n            Series with alphabetically sorted tuple combinations (bidirectional)\n        \"\"\"\n        return pd.Series([tuple(sorted([x, y])) for x, y in zip(a, b)], index=a.index)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/","title":"MESCAL Membership Property Enrichers","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mescal.energy_data_handling.model_handling.membership_property_enrichers.MembershipTagging","title":"MembershipTagging","text":"<p>               Bases: <code>Enum</code></p> <p>Controls how enriched property names are tagged when added to target DataFrames.</p> <p>In energy system modeling, objects often have relationships to other model components (e.g., generators belong to nodes, lines connect nodes). When enriching a DataFrame with properties from related objects, this enum controls naming conventions to avoid column name conflicts and maintain clarity about property origins.</p> Values <ul> <li>NONE: Property names remain unchanged (may cause conflicts with existing columns)</li> <li>PREFIX: Property names get membership name as prefix (e.g., 'node_voltage' from 'voltage')</li> <li>SUFFIX: Property names get membership name as suffix (e.g., 'voltage_node' from 'voltage')</li> </ul> <p>Examples:</p> <p>For a generator DataFrame (target_df) with 'node' membership, enriching with node properties:</p> <pre><code>&gt;&gt;&gt; MembershipPropertyEnricher().append_properties(target_df, dataset, MembershipTagging.NONE)\n&gt;&gt;&gt; # Returns target_df with new columns ['voltage', 'load'] (original names from node DataFrame)\n&gt;&gt;&gt;\n&gt;&gt;&gt; MembershipPropertyEnricher().append_properties(target_df, dataset, MembershipTagging.PREFIX)\n&gt;&gt;&gt; # Returns target_df with new columns ['node_voltage', 'node_load']\n&gt;&gt;&gt;\n&gt;&gt;&gt; MembershipPropertyEnricher().append_properties(target_df, dataset, MembershipTagging.SUFFIX)\n&gt;&gt;&gt; # Returns target_df with new columns ['voltage_node', 'load_node']\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>class MembershipTagging(Enum):\n    \"\"\"\n    Controls how enriched property names are tagged when added to target DataFrames.\n\n    In energy system modeling, objects often have relationships to other model components\n    (e.g., generators belong to nodes, lines connect nodes). When enriching a DataFrame\n    with properties from related objects, this enum controls naming conventions to avoid\n    column name conflicts and maintain clarity about property origins.\n\n    Values:\n        - NONE: Property names remain unchanged (may cause conflicts with existing columns)\n        - PREFIX: Property names get membership name as prefix (e.g., 'node_voltage' from 'voltage')\n        - SUFFIX: Property names get membership name as suffix (e.g., 'voltage_node' from 'voltage')\n\n    Examples:\n        For a generator DataFrame (target_df) with 'node' membership, enriching with node properties:\n        &gt;&gt;&gt; MembershipPropertyEnricher().append_properties(target_df, dataset, MembershipTagging.NONE)\n        &gt;&gt;&gt; # Returns target_df with new columns ['voltage', 'load'] (original names from node DataFrame)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; MembershipPropertyEnricher().append_properties(target_df, dataset, MembershipTagging.PREFIX)\n        &gt;&gt;&gt; # Returns target_df with new columns ['node_voltage', 'node_load']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; MembershipPropertyEnricher().append_properties(target_df, dataset, MembershipTagging.SUFFIX)\n        &gt;&gt;&gt; # Returns target_df with new columns ['voltage_node', 'load_node']\n    \"\"\"\n    NONE = \"none\"\n    PREFIX = \"prefix\"\n    SUFFIX = \"suffix\"\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mescal.energy_data_handling.model_handling.membership_property_enrichers.MembershipPropertyEnricher","title":"MembershipPropertyEnricher","text":"<p>Enriches energy system DataFrames with properties from related model objects.</p> <p>In energy system modeling, entities often have membership relationships to other model components. For example: - Generators belong to nodes and have fuel types - Lines connect between nodes - Storage units are located at nodes and have technology types</p> <p>This enricher automatically identifies membership columns in a target DataFrame and adds all properties from the corresponding model DataFrames. This enables comprehensive analysis by combining object properties with their relationships.</p> <p>Key Features: - Automatic identification of membership columns using MESCAL's flag index system - Support for multiple simultaneous memberships (node, fuel_type, company, etc.) - Preservation of NaN memberships in enriched data - Configurable property naming to avoid column conflicts - Integration with MESCAL Dataset architecture</p> <p>Parameters:</p> Name Type Description Default <code>membership_tag_separator</code> <code>str</code> <p>Separator used between membership name and property                       when PREFIX or SUFFIX tagging is applied</p> <code>'_'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; enricher = MembershipPropertyEnricher()\n&gt;&gt;&gt; # Generator DataFrame with 'node' column linking to node objects\n&gt;&gt;&gt; enriched_gen_df = enricher.append_properties(\n...     generator_df, dataset, MembershipTagging.PREFIX\n... )\n&gt;&gt;&gt; # enriched_gen_df now includes 'node_voltage', 'node_area' columns from node properties\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>class MembershipPropertyEnricher:\n    \"\"\"\n    Enriches energy system DataFrames with properties from related model objects.\n\n    In energy system modeling, entities often have membership relationships to other\n    model components. For example:\n    - Generators belong to nodes and have fuel types\n    - Lines connect between nodes\n    - Storage units are located at nodes and have technology types\n\n    This enricher automatically identifies membership columns in a target DataFrame\n    and adds all properties from the corresponding model DataFrames. This enables\n    comprehensive analysis by combining object properties with their relationships.\n\n    Key Features:\n    - Automatic identification of membership columns using MESCAL's flag index system\n    - Support for multiple simultaneous memberships (node, fuel_type, company, etc.)\n    - Preservation of NaN memberships in enriched data\n    - Configurable property naming to avoid column conflicts\n    - Integration with MESCAL Dataset architecture\n\n    Args:\n        membership_tag_separator: Separator used between membership name and property\n                                  when PREFIX or SUFFIX tagging is applied\n\n    Examples:\n        &gt;&gt;&gt; enricher = MembershipPropertyEnricher()\n        &gt;&gt;&gt; # Generator DataFrame with 'node' column linking to node objects\n        &gt;&gt;&gt; enriched_gen_df = enricher.append_properties(\n        ...     generator_df, dataset, MembershipTagging.PREFIX\n        ... )\n        &gt;&gt;&gt; # enriched_gen_df now includes 'node_voltage', 'node_area' columns from node properties\n    \"\"\"\n    def __init__(self, membership_tag_separator: str = '_'):\n        \"\"\"\n        Initialize the membership property enricher.\n\n        Args:\n            membership_tag_separator: Character(s) used to separate membership names\n                                     from property names in PREFIX/SUFFIX modes\n        \"\"\"\n        self._membership_tag_separator = membership_tag_separator\n\n    def identify_membership_columns(self, column_names: list[str], dataset: Dataset) -&gt; list[str]:\n        \"\"\"\n        Identifies columns that represent memberships to other model objects.\n\n        Uses MESCAL's flag index system to determine which columns in the target\n        DataFrame represent relationships to other model components. This enables\n        automatic discovery of enrichment opportunities without manual specification.\n\n        Args:\n            column_names: List of column names from the target DataFrame\n            dataset: MESCAL Dataset containing model definitions and flag mappings\n\n        Returns:\n            List of column names that represent memberships to other model objects\n\n        Examples:\n            For a generator DataFrame with columns ['name', 'capacity', 'node', 'fuel_type']:\n            &gt;&gt;&gt; membership_cols = enricher.identify_membership_columns(\n            ...     generator_df.columns, dataset\n            ... )\n            &gt;&gt;&gt; print(membership_cols)  # ['node', 'fuel_type']\n        \"\"\"\n        return [\n            col for col in column_names\n            if dataset.flag_index.column_name_in_model_describes_membership(col)\n        ]\n\n    def append_properties(\n            self,\n            target_df: pd.DataFrame,\n            dataset: Dataset,\n            membership_tagging: MembershipTagging = MembershipTagging.NONE\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Enriches target DataFrame with properties from all linked model objects.\n\n        Performs comprehensive enrichment by automatically identifying all membership\n        relationships and adding corresponding properties. This is the primary method\n        for energy system DataFrame enrichment, enabling complex multi-dimensional\n        analysis by combining object properties with their relationships.\n\n        The method preserves all original data while adding new property columns.\n        Missing relationships (NaN memberships) are handled gracefully by preserving\n        NaN values in the enriched properties.\n\n        Args:\n            target_df: DataFrame to enrich (e.g., generator, line, storage data)\n            dataset: MESCAL Dataset containing linked model DataFrames with properties\n            membership_tagging: Strategy for naming enriched properties to avoid conflicts\n\n        Returns:\n            Enhanced DataFrame with all properties from linked model objects added.\n            Original columns are preserved, new columns added based on memberships.\n\n        Raises:\n            Warning: Logged when membership objects are missing from source DataFrames\n\n        Examples:\n            Energy system use cases:\n\n            &gt;&gt;&gt; # Enrich generator data with node and fuel properties\n            &gt;&gt;&gt; enriched_generators = enricher.append_properties(\n            ...     generators_df, dataset, MembershipTagging.PREFIX\n            ... )\n            &gt;&gt;&gt; # Result: original columns + 'node_voltage', 'node_area', 'fuel_co2_rate', etc.\n\n            &gt;&gt;&gt; # Enrich transmission data with node characteristics\n            &gt;&gt;&gt; enriched_lines = enricher.append_properties(\n            ...     transmission_df, dataset, MembershipTagging.SUFFIX\n            ... )\n            &gt;&gt;&gt; # Result: line properties + node properties with '_node' suffix\n        \"\"\"\n        membership_columns = self.identify_membership_columns(target_df.columns, dataset)\n        result_df = target_df.copy()\n\n        for column in membership_columns:\n            result_df = self.append_single_membership_properties(\n                result_df,\n                dataset,\n                column,\n                membership_tagging\n            )\n\n        return result_df\n\n    def append_single_membership_properties(\n            self,\n            target_df: pd.DataFrame,\n            dataset: Dataset,\n            membership_column: str,\n            membership_tagging: MembershipTagging = MembershipTagging.NONE\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Enriches target DataFrame with properties from a specific membership relationship.\n\n        This method provides fine-grained control over property enrichment by handling\n        a single membership column. Useful when custom logic is needed for specific\n        relationships or when processing memberships sequentially with different\n        tagging strategies.\n\n        The method uses MESCAL's flag index to determine the source model DataFrame\n        for the membership column, then performs a left join to preserve all target\n        records while adding available properties.\n\n        Args:\n            target_df: DataFrame to enrich (must contain the membership column)\n            dataset: MESCAL Dataset with access to linked model DataFrames\n            membership_column: Name of column containing object references (e.g., 'node', 'fuel_type')\n            membership_tagging: Strategy for naming enriched properties\n\n        Returns:\n            DataFrame with properties from the linked model objects added.\n            All original rows preserved; NaN memberships result in NaN properties.\n\n        Raises:\n            Warning: Logged when referenced objects are missing from the source DataFrame\n\n        Examples:\n            Targeted enrichment scenarios:\n\n            &gt;&gt;&gt; # Add only node properties to generators\n            &gt;&gt;&gt; gen_with_nodes = enricher.append_single_membership_properties(\n            ...     generators_df, dataset, 'node', MembershipTagging.PREFIX\n            ... )\n            &gt;&gt;&gt; # Result: generators + 'node_voltage', 'node_area', etc.\n\n            &gt;&gt;&gt; # Sequential enrichment with different tagging\n            &gt;&gt;&gt; result = generators_df.copy()\n            &gt;&gt;&gt; result = enricher.append_single_membership_properties(\n            ...     result, dataset, 'node', MembershipTagging.PREFIX\n            ... )\n            &gt;&gt;&gt; result = enricher.append_single_membership_properties(\n            ...     result, dataset, 'fuel_type', MembershipTagging.SUFFIX\n            ... )\n        \"\"\"\n        source_flag = dataset.flag_index.get_linked_model_flag_for_membership_column(membership_column)\n        source_df = dataset.fetch(source_flag)\n\n        membership_objects = target_df[membership_column].dropna().unique()\n        missing_objects = set(membership_objects) - set(source_df.index)\n\n        if missing_objects:\n            self._log_missing_objects_warning(missing_objects, membership_column)\n\n        source_properties = source_df.copy()\n\n        match membership_tagging:\n            case MembershipTagging.PREFIX:\n                source_properties = source_properties.add_prefix(f\"{membership_column}{self._membership_tag_separator}\")\n            case MembershipTagging.SUFFIX:\n                source_properties = source_properties.add_suffix(f\"{self._membership_tag_separator}{membership_column}\")\n\n        result_df = target_df.merge(\n            source_properties,\n            left_on=membership_column,\n            right_index=True,\n            how=\"left\"\n        )\n\n        return result_df\n\n    def _log_missing_objects_warning(\n            self,\n            missing_objects: set,\n            membership_column: str,\n            max_show: int = 5\n    ):\n        \"\"\"\n        Logs warning about missing objects in the source DataFrame.\n\n        In energy system modeling, missing references can indicate data quality\n        issues, model inconsistencies, or incomplete datasets. This method provides\n        informative warnings to help identify and resolve such issues.\n\n        Args:\n            missing_objects: Set of object identifiers missing from source DataFrame\n            membership_column: Name of the membership column being processed\n            max_show: Maximum number of missing objects to display in the warning\n        \"\"\"\n        num_missing = len(missing_objects)\n        warning_suffix = \", and more\" if num_missing &gt; max_show else \"\"\n        logger.warning(\n            f\"{num_missing} objects missing in source dataframe for {membership_column}: \"\n            f\"{list(missing_objects)[:max_show]}{warning_suffix}.\"\n        )\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mescal.energy_data_handling.model_handling.membership_property_enrichers.MembershipPropertyEnricher.__init__","title":"__init__","text":"<pre><code>__init__(membership_tag_separator: str = '_')\n</code></pre> <p>Initialize the membership property enricher.</p> <p>Parameters:</p> Name Type Description Default <code>membership_tag_separator</code> <code>str</code> <p>Character(s) used to separate membership names                      from property names in PREFIX/SUFFIX modes</p> <code>'_'</code> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def __init__(self, membership_tag_separator: str = '_'):\n    \"\"\"\n    Initialize the membership property enricher.\n\n    Args:\n        membership_tag_separator: Character(s) used to separate membership names\n                                 from property names in PREFIX/SUFFIX modes\n    \"\"\"\n    self._membership_tag_separator = membership_tag_separator\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mescal.energy_data_handling.model_handling.membership_property_enrichers.MembershipPropertyEnricher.identify_membership_columns","title":"identify_membership_columns","text":"<pre><code>identify_membership_columns(column_names: list[str], dataset: Dataset) -&gt; list[str]\n</code></pre> <p>Identifies columns that represent memberships to other model objects.</p> <p>Uses MESCAL's flag index system to determine which columns in the target DataFrame represent relationships to other model components. This enables automatic discovery of enrichment opportunities without manual specification.</p> <p>Parameters:</p> Name Type Description Default <code>column_names</code> <code>list[str]</code> <p>List of column names from the target DataFrame</p> required <code>dataset</code> <code>Dataset</code> <p>MESCAL Dataset containing model definitions and flag mappings</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of column names that represent memberships to other model objects</p> <p>Examples:</p> <p>For a generator DataFrame with columns ['name', 'capacity', 'node', 'fuel_type']:</p> <pre><code>&gt;&gt;&gt; membership_cols = enricher.identify_membership_columns(\n...     generator_df.columns, dataset\n... )\n&gt;&gt;&gt; print(membership_cols)  # ['node', 'fuel_type']\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def identify_membership_columns(self, column_names: list[str], dataset: Dataset) -&gt; list[str]:\n    \"\"\"\n    Identifies columns that represent memberships to other model objects.\n\n    Uses MESCAL's flag index system to determine which columns in the target\n    DataFrame represent relationships to other model components. This enables\n    automatic discovery of enrichment opportunities without manual specification.\n\n    Args:\n        column_names: List of column names from the target DataFrame\n        dataset: MESCAL Dataset containing model definitions and flag mappings\n\n    Returns:\n        List of column names that represent memberships to other model objects\n\n    Examples:\n        For a generator DataFrame with columns ['name', 'capacity', 'node', 'fuel_type']:\n        &gt;&gt;&gt; membership_cols = enricher.identify_membership_columns(\n        ...     generator_df.columns, dataset\n        ... )\n        &gt;&gt;&gt; print(membership_cols)  # ['node', 'fuel_type']\n    \"\"\"\n    return [\n        col for col in column_names\n        if dataset.flag_index.column_name_in_model_describes_membership(col)\n    ]\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mescal.energy_data_handling.model_handling.membership_property_enrichers.MembershipPropertyEnricher.append_properties","title":"append_properties","text":"<pre><code>append_properties(target_df: DataFrame, dataset: Dataset, membership_tagging: MembershipTagging = NONE) -&gt; DataFrame\n</code></pre> <p>Enriches target DataFrame with properties from all linked model objects.</p> <p>Performs comprehensive enrichment by automatically identifying all membership relationships and adding corresponding properties. This is the primary method for energy system DataFrame enrichment, enabling complex multi-dimensional analysis by combining object properties with their relationships.</p> <p>The method preserves all original data while adding new property columns. Missing relationships (NaN memberships) are handled gracefully by preserving NaN values in the enriched properties.</p> <p>Parameters:</p> Name Type Description Default <code>target_df</code> <code>DataFrame</code> <p>DataFrame to enrich (e.g., generator, line, storage data)</p> required <code>dataset</code> <code>Dataset</code> <p>MESCAL Dataset containing linked model DataFrames with properties</p> required <code>membership_tagging</code> <code>MembershipTagging</code> <p>Strategy for naming enriched properties to avoid conflicts</p> <code>NONE</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced DataFrame with all properties from linked model objects added.</p> <code>DataFrame</code> <p>Original columns are preserved, new columns added based on memberships.</p> <p>Raises:</p> Type Description <code>Warning</code> <p>Logged when membership objects are missing from source DataFrames</p> <p>Examples:</p> <p>Energy system use cases:</p> <pre><code>&gt;&gt;&gt; # Enrich generator data with node and fuel properties\n&gt;&gt;&gt; enriched_generators = enricher.append_properties(\n...     generators_df, dataset, MembershipTagging.PREFIX\n... )\n&gt;&gt;&gt; # Result: original columns + 'node_voltage', 'node_area', 'fuel_co2_rate', etc.\n</code></pre> <pre><code>&gt;&gt;&gt; # Enrich transmission data with node characteristics\n&gt;&gt;&gt; enriched_lines = enricher.append_properties(\n...     transmission_df, dataset, MembershipTagging.SUFFIX\n... )\n&gt;&gt;&gt; # Result: line properties + node properties with '_node' suffix\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def append_properties(\n        self,\n        target_df: pd.DataFrame,\n        dataset: Dataset,\n        membership_tagging: MembershipTagging = MembershipTagging.NONE\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Enriches target DataFrame with properties from all linked model objects.\n\n    Performs comprehensive enrichment by automatically identifying all membership\n    relationships and adding corresponding properties. This is the primary method\n    for energy system DataFrame enrichment, enabling complex multi-dimensional\n    analysis by combining object properties with their relationships.\n\n    The method preserves all original data while adding new property columns.\n    Missing relationships (NaN memberships) are handled gracefully by preserving\n    NaN values in the enriched properties.\n\n    Args:\n        target_df: DataFrame to enrich (e.g., generator, line, storage data)\n        dataset: MESCAL Dataset containing linked model DataFrames with properties\n        membership_tagging: Strategy for naming enriched properties to avoid conflicts\n\n    Returns:\n        Enhanced DataFrame with all properties from linked model objects added.\n        Original columns are preserved, new columns added based on memberships.\n\n    Raises:\n        Warning: Logged when membership objects are missing from source DataFrames\n\n    Examples:\n        Energy system use cases:\n\n        &gt;&gt;&gt; # Enrich generator data with node and fuel properties\n        &gt;&gt;&gt; enriched_generators = enricher.append_properties(\n        ...     generators_df, dataset, MembershipTagging.PREFIX\n        ... )\n        &gt;&gt;&gt; # Result: original columns + 'node_voltage', 'node_area', 'fuel_co2_rate', etc.\n\n        &gt;&gt;&gt; # Enrich transmission data with node characteristics\n        &gt;&gt;&gt; enriched_lines = enricher.append_properties(\n        ...     transmission_df, dataset, MembershipTagging.SUFFIX\n        ... )\n        &gt;&gt;&gt; # Result: line properties + node properties with '_node' suffix\n    \"\"\"\n    membership_columns = self.identify_membership_columns(target_df.columns, dataset)\n    result_df = target_df.copy()\n\n    for column in membership_columns:\n        result_df = self.append_single_membership_properties(\n            result_df,\n            dataset,\n            column,\n            membership_tagging\n        )\n\n    return result_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mescal.energy_data_handling.model_handling.membership_property_enrichers.MembershipPropertyEnricher.append_single_membership_properties","title":"append_single_membership_properties","text":"<pre><code>append_single_membership_properties(target_df: DataFrame, dataset: Dataset, membership_column: str, membership_tagging: MembershipTagging = NONE) -&gt; DataFrame\n</code></pre> <p>Enriches target DataFrame with properties from a specific membership relationship.</p> <p>This method provides fine-grained control over property enrichment by handling a single membership column. Useful when custom logic is needed for specific relationships or when processing memberships sequentially with different tagging strategies.</p> <p>The method uses MESCAL's flag index to determine the source model DataFrame for the membership column, then performs a left join to preserve all target records while adding available properties.</p> <p>Parameters:</p> Name Type Description Default <code>target_df</code> <code>DataFrame</code> <p>DataFrame to enrich (must contain the membership column)</p> required <code>dataset</code> <code>Dataset</code> <p>MESCAL Dataset with access to linked model DataFrames</p> required <code>membership_column</code> <code>str</code> <p>Name of column containing object references (e.g., 'node', 'fuel_type')</p> required <code>membership_tagging</code> <code>MembershipTagging</code> <p>Strategy for naming enriched properties</p> <code>NONE</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with properties from the linked model objects added.</p> <code>DataFrame</code> <p>All original rows preserved; NaN memberships result in NaN properties.</p> <p>Raises:</p> Type Description <code>Warning</code> <p>Logged when referenced objects are missing from the source DataFrame</p> <p>Examples:</p> <p>Targeted enrichment scenarios:</p> <pre><code>&gt;&gt;&gt; # Add only node properties to generators\n&gt;&gt;&gt; gen_with_nodes = enricher.append_single_membership_properties(\n...     generators_df, dataset, 'node', MembershipTagging.PREFIX\n... )\n&gt;&gt;&gt; # Result: generators + 'node_voltage', 'node_area', etc.\n</code></pre> <pre><code>&gt;&gt;&gt; # Sequential enrichment with different tagging\n&gt;&gt;&gt; result = generators_df.copy()\n&gt;&gt;&gt; result = enricher.append_single_membership_properties(\n...     result, dataset, 'node', MembershipTagging.PREFIX\n... )\n&gt;&gt;&gt; result = enricher.append_single_membership_properties(\n...     result, dataset, 'fuel_type', MembershipTagging.SUFFIX\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def append_single_membership_properties(\n        self,\n        target_df: pd.DataFrame,\n        dataset: Dataset,\n        membership_column: str,\n        membership_tagging: MembershipTagging = MembershipTagging.NONE\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Enriches target DataFrame with properties from a specific membership relationship.\n\n    This method provides fine-grained control over property enrichment by handling\n    a single membership column. Useful when custom logic is needed for specific\n    relationships or when processing memberships sequentially with different\n    tagging strategies.\n\n    The method uses MESCAL's flag index to determine the source model DataFrame\n    for the membership column, then performs a left join to preserve all target\n    records while adding available properties.\n\n    Args:\n        target_df: DataFrame to enrich (must contain the membership column)\n        dataset: MESCAL Dataset with access to linked model DataFrames\n        membership_column: Name of column containing object references (e.g., 'node', 'fuel_type')\n        membership_tagging: Strategy for naming enriched properties\n\n    Returns:\n        DataFrame with properties from the linked model objects added.\n        All original rows preserved; NaN memberships result in NaN properties.\n\n    Raises:\n        Warning: Logged when referenced objects are missing from the source DataFrame\n\n    Examples:\n        Targeted enrichment scenarios:\n\n        &gt;&gt;&gt; # Add only node properties to generators\n        &gt;&gt;&gt; gen_with_nodes = enricher.append_single_membership_properties(\n        ...     generators_df, dataset, 'node', MembershipTagging.PREFIX\n        ... )\n        &gt;&gt;&gt; # Result: generators + 'node_voltage', 'node_area', etc.\n\n        &gt;&gt;&gt; # Sequential enrichment with different tagging\n        &gt;&gt;&gt; result = generators_df.copy()\n        &gt;&gt;&gt; result = enricher.append_single_membership_properties(\n        ...     result, dataset, 'node', MembershipTagging.PREFIX\n        ... )\n        &gt;&gt;&gt; result = enricher.append_single_membership_properties(\n        ...     result, dataset, 'fuel_type', MembershipTagging.SUFFIX\n        ... )\n    \"\"\"\n    source_flag = dataset.flag_index.get_linked_model_flag_for_membership_column(membership_column)\n    source_df = dataset.fetch(source_flag)\n\n    membership_objects = target_df[membership_column].dropna().unique()\n    missing_objects = set(membership_objects) - set(source_df.index)\n\n    if missing_objects:\n        self._log_missing_objects_warning(missing_objects, membership_column)\n\n    source_properties = source_df.copy()\n\n    match membership_tagging:\n        case MembershipTagging.PREFIX:\n            source_properties = source_properties.add_prefix(f\"{membership_column}{self._membership_tag_separator}\")\n        case MembershipTagging.SUFFIX:\n            source_properties = source_properties.add_suffix(f\"{self._membership_tag_separator}{membership_column}\")\n\n    result_df = target_df.merge(\n        source_properties,\n        left_on=membership_column,\n        right_index=True,\n        how=\"left\"\n    )\n\n    return result_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mescal.energy_data_handling.model_handling.membership_property_enrichers.DirectionalMembershipPropertyEnricher","title":"DirectionalMembershipPropertyEnricher","text":"<p>Enriches energy system DataFrames with properties for directional relationships.</p> <p>Energy networks inherently contain directional relationships - transmission lines connect from one node to another, flows have origins and destinations, and trade occurs between regions. This enricher handles such bidirectional memberships by identifying from/to column pairs and enriching with appropriate directional tags.</p> <p>Common energy system applications: - Transmission lines: 'node_from' and 'node_to' linking to node properties - Inter-regional flows: 'region_from' and 'region_to' for trade analysis - Pipeline systems: 'hub_from' and 'hub_to' for gas network modeling - Market connections: 'market_from' and 'market_to' for price analysis</p> <p>The enricher automatically identifies directional column pairs using configurable identifiers (default: '_from' and '_to') and adds properties from the linked model objects with appropriate directional suffixes.</p> Key Features <ul> <li>Automatic identification of from/to column pairs</li> <li>Flexible directional identifiers (customizable beyond '_from'/'_to')</li> <li>Support for multiple directional relationships in one DataFrame</li> <li>Preservation of NaN relationships</li> <li>Integration with MESCAL's model flag system</li> </ul> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str</code> <p>Suffix/prefix identifying 'from' direction (default: '_from')</p> <code>'_from'</code> <code>to_identifier</code> <code>str</code> <p>Suffix/prefix identifying 'to' direction (default: '_to')</p> <code>'_to'</code> <code>membership_tag_separator</code> <code>str</code> <p>Separator for property name construction</p> <code>'_'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; enricher = DirectionalMembershipPropertyEnricher()\n&gt;&gt;&gt; # Line DataFrame with 'node_from', 'node_to' columns\n&gt;&gt;&gt; enriched_lines = enricher.append_properties(\n...     line_df, dataset, MembershipTagging.NONE\n... )\n&gt;&gt;&gt; # Result includes node properties with '_from' and '_to' suffixes\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>class DirectionalMembershipPropertyEnricher:\n    \"\"\"\n    Enriches energy system DataFrames with properties for directional relationships.\n\n    Energy networks inherently contain directional relationships - transmission lines\n    connect from one node to another, flows have origins and destinations, and trade\n    occurs between regions. This enricher handles such bidirectional memberships by\n    identifying from/to column pairs and enriching with appropriate directional tags.\n\n    Common energy system applications:\n    - Transmission lines: 'node_from' and 'node_to' linking to node properties\n    - Inter-regional flows: 'region_from' and 'region_to' for trade analysis\n    - Pipeline systems: 'hub_from' and 'hub_to' for gas network modeling\n    - Market connections: 'market_from' and 'market_to' for price analysis\n\n    The enricher automatically identifies directional column pairs using configurable\n    identifiers (default: '_from' and '_to') and adds properties from the linked\n    model objects with appropriate directional suffixes.\n\n    Key Features:\n        - Automatic identification of from/to column pairs\n        - Flexible directional identifiers (customizable beyond '_from'/'_to')\n        - Support for multiple directional relationships in one DataFrame\n        - Preservation of NaN relationships\n        - Integration with MESCAL's model flag system\n\n    Args:\n        from_identifier: Suffix/prefix identifying 'from' direction (default: '_from')\n        to_identifier: Suffix/prefix identifying 'to' direction (default: '_to')\n        membership_tag_separator: Separator for property name construction\n\n    Examples:\n\n        &gt;&gt;&gt; enricher = DirectionalMembershipPropertyEnricher()\n        &gt;&gt;&gt; # Line DataFrame with 'node_from', 'node_to' columns\n        &gt;&gt;&gt; enriched_lines = enricher.append_properties(\n        ...     line_df, dataset, MembershipTagging.NONE\n        ... )\n        &gt;&gt;&gt; # Result includes node properties with '_from' and '_to' suffixes\n    \"\"\"\n    def __init__(\n            self,\n            from_identifier: str = \"_from\",\n            to_identifier: str = \"_to\",\n            membership_tag_separator: str = '_',\n    ):\n        \"\"\"\n        Initialize the directional membership property enricher.\n\n        Args:\n            from_identifier: String identifying source/origin columns (e.g., '_from', 'source_')\n            to_identifier: String identifying destination/target columns (e.g., '_to', 'dest_')\n            membership_tag_separator: Character(s) separating membership names from properties\n        \"\"\"\n        self._from_identifier = from_identifier\n        self._to_identifier = to_identifier\n        self._membership_tag_separator = membership_tag_separator\n        self._tag_finder = CommonBaseKeyFinder(from_identifier, to_identifier)\n\n    def identify_from_to_columns(self, column_names: list[str], dataset: Dataset) -&gt; list[str]:\n        \"\"\"\n        Identifies base names for directional membership column pairs.\n\n        Analyzes column names to find base membership types that have both 'from'\n        and 'to' variants. For example, identifies 'node' as a base when both\n        'node_from' and 'node_to' columns exist and represent valid memberships.\n\n        Args:\n            column_names: List of column names from the target DataFrame\n            dataset: MESCAL Dataset for membership validation\n\n        Returns:\n            List of base column names that have both from/to variants\n\n        Examples:\n            For a line DataFrame with ['name', 'capacity', 'node_from', 'node_to']:\n            &gt;&gt;&gt; base_columns = enricher.identify_from_to_columns(\n            ...     line_df.columns, dataset\n            ... )\n            &gt;&gt;&gt; print(base_columns)  # ['node']\n\n            For inter-regional trade with ['flow', 'region_from', 'region_to', 'market_from']:\n            &gt;&gt;&gt; base_columns = enricher.identify_from_to_columns(\n            ...     trade_df.columns, dataset  \n            ... )\n            &gt;&gt;&gt; print(base_columns)  # ['region'] (market missing 'market_to')\n        \"\"\"\n        potential_columns = self._tag_finder.get_keys_for_which_all_association_tags_appear(column_names)\n        return [\n            col for col in potential_columns\n            if dataset.flag_index.column_name_in_model_describes_membership(col)\n        ]\n\n    def append_properties(\n            self,\n            target_df: pd.DataFrame,\n            dataset: Dataset,\n            membership_tagging: MembershipTagging = MembershipTagging.NONE\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Enriches DataFrame with properties from all directional relationships.\n\n        Performs comprehensive directional enrichment by identifying all from/to\n        column pairs and adding properties from both directions. Essential for\n        network analysis where understanding characteristics of connected nodes,\n        regions, or components is crucial for energy system modeling.\n\n        Each directional relationship results in two sets of enriched properties:\n        one for the 'from' direction and one for the 'to' direction, clearly\n        distinguished by directional suffixes.\n\n        Args:\n            target_df: DataFrame with directional relationships (e.g., transmission lines)\n            dataset: MESCAL Dataset containing model objects and their properties\n            membership_tagging: Strategy for property naming (applied before directional tags)\n\n        Returns:\n            Enhanced DataFrame with directional properties added. Original data preserved,\n            new columns follow pattern: [prefix_]property_name[_suffix]_direction\n\n        Raises:\n            Warning: Logged when referenced objects missing from source DataFrames\n\n        Examples:\n            Network transmission analysis:\n\n            &gt;&gt;&gt; # Transmission lines with node endpoints\n            &gt;&gt;&gt; enriched_lines = enricher.append_properties(\n            ...     transmission_df, dataset, MembershipTagging.NONE\n            ... )\n            &gt;&gt;&gt; # Result: original columns + 'voltage_from', 'voltage_to', \n            &gt;&gt;&gt; #         'area_from', 'area_to', etc.\n\n            &gt;&gt;&gt; # Inter-regional trade flows\n            &gt;&gt;&gt; enriched_trade = enricher.append_properties(\n            ...     trade_df, dataset, MembershipTagging.PREFIX\n            ... )\n            &gt;&gt;&gt; # Result: trade data + 'region_gdp_from', 'region_gdp_to', etc.\n        \"\"\"\n        membership_base_columns = self.identify_from_to_columns(target_df.columns, dataset)\n        result_df = target_df.copy()\n\n        for base_column in membership_base_columns:\n            result_df = self.append_directional_properties(\n                result_df,\n                dataset,\n                base_column,\n                membership_tagging\n            )\n\n        return result_df\n\n    def append_directional_properties(\n            self,\n            target_df: pd.DataFrame,\n            dataset: Dataset,\n            base_column: str,\n            membership_tagging: MembershipTagging = MembershipTagging.NONE\n    ) -&gt; pd.DataFrame:\n        source_flag = dataset.flag_index.get_linked_model_flag_for_membership_column(base_column)\n        source_df = dataset.fetch(source_flag)\n        return self.append_directional_properties_in_source_to_target_df(\n            target_df,\n            source_df,\n            base_column,\n            membership_tagging,\n        )\n\n    def append_directional_properties_in_source_to_target_df(\n            self,\n            target_df: pd.DataFrame,\n            source_df: pd.DataFrame,\n            base_column: str,\n            membership_tagging: MembershipTagging = MembershipTagging.NONE\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Enriches DataFrame with properties from a directional relationship.\n\n        Handles a single from/to membership pair by adding the corresponding model\n        DataFrame's properties with directional tags.\n\n        The method processes both directions (from/to) for the specified base column,\n        adding properties with appropriate directional suffixes. Missing references\n        are handled gracefully with NaN preservation.\n\n        Args:\n            target_df: DataFrame containing the directional columns\n            source_df: DataFrame containing the properties\n            base_column: Base membership name (e.g., 'node' for 'node_from'/'node_to')\n            membership_tagging: Property naming strategy (applied before directional tags)\n\n        Returns:\n            DataFrame with directional properties added for the specified relationship.\n            Properties follow naming pattern: [prefix_]property[_suffix]_direction\n\n        Raises:\n            Warning: Logged when referenced objects are missing from source DataFrame\n\n        Examples:\n            Targeted directional enrichment:\n\n            &gt;&gt;&gt; # Add only node properties to transmission lines\n            &gt;&gt;&gt; lines_with_nodes = enricher.append_directional_properties(\n            ...     line_df, node_df, 'node', MembershipTagging.NONE\n            ... )\n            &gt;&gt;&gt; # Result: lines + 'voltage_from', 'voltage_to', 'area_from', 'area_to'\n        \"\"\"\n        result_df = target_df.copy()\n        for tag in [self._from_identifier, self._to_identifier]:\n            membership_column = self._get_full_column_name(base_column, tag, target_df.columns)\n\n            if membership_column not in target_df.columns:\n                continue\n\n            membership_objects = target_df[membership_column].dropna().unique()\n            missing_objects = set(membership_objects) - set(source_df.index)\n\n            if missing_objects:\n                self._log_missing_objects_warning(missing_objects, membership_column)\n\n            source_properties = source_df.copy()\n\n            match membership_tagging:\n                case MembershipTagging.PREFIX:\n                    source_properties = source_properties.add_prefix(f\"{base_column}{self._membership_tag_separator}\")\n                case MembershipTagging.SUFFIX:\n                    source_properties = source_properties.add_suffix(f\"{self._membership_tag_separator}{base_column}\")\n\n            source_properties = source_properties.add_suffix(tag)\n\n            result_df = result_df.merge(\n                source_properties,\n                left_on=membership_column,\n                right_index=True,\n                how=\"left\"\n            )\n        return result_df\n\n    def _get_full_column_name(self, base_column: str, tag: str, df_columns: list[str]) -&gt; str:\n        \"\"\"\n        Determines the actual column name for a directional membership.\n\n        Handles flexibility in directional column naming by testing both\n        suffix and prefix patterns. Supports various naming conventions\n        used across different energy modeling platforms.\n\n        Args:\n            base_column: Base membership name (e.g., 'node')\n            tag: Directional identifier (e.g., '_from', '_to')\n            df_columns: List of actual column names in the DataFrame\n\n        Returns:\n            Actual column name found in the DataFrame\n\n        Examples:\n            &gt;&gt;&gt; # For base_column='node', tag='_from'\n            &gt;&gt;&gt; # Tests 'node_from' first, then 'from_node'\n            &gt;&gt;&gt; name = enricher._get_full_column_name('node', '_from', df.columns)\n        \"\"\"\n        test_suffix = f\"{base_column}{tag}\"\n        return test_suffix if test_suffix in df_columns else f\"{tag}{base_column}\"\n\n    def _log_missing_objects_warning(\n            self,\n            missing_objects: set,\n            membership_column: str,\n            max_show: int = 5\n    ):\n        num_missing = len(missing_objects)\n        warning_suffix = \", and more\" if num_missing &gt; max_show else \"\"\n        logger.warning(\n            f\"{num_missing} objects missing in source dataframe for {membership_column}: \"\n            f\"{list(missing_objects)[:max_show]}{warning_suffix}.\"\n        )\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mescal.energy_data_handling.model_handling.membership_property_enrichers.DirectionalMembershipPropertyEnricher.__init__","title":"__init__","text":"<pre><code>__init__(from_identifier: str = '_from', to_identifier: str = '_to', membership_tag_separator: str = '_')\n</code></pre> <p>Initialize the directional membership property enricher.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str</code> <p>String identifying source/origin columns (e.g., 'from', 'source')</p> <code>'_from'</code> <code>to_identifier</code> <code>str</code> <p>String identifying destination/target columns (e.g., 'to', 'dest')</p> <code>'_to'</code> <code>membership_tag_separator</code> <code>str</code> <p>Character(s) separating membership names from properties</p> <code>'_'</code> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def __init__(\n        self,\n        from_identifier: str = \"_from\",\n        to_identifier: str = \"_to\",\n        membership_tag_separator: str = '_',\n):\n    \"\"\"\n    Initialize the directional membership property enricher.\n\n    Args:\n        from_identifier: String identifying source/origin columns (e.g., '_from', 'source_')\n        to_identifier: String identifying destination/target columns (e.g., '_to', 'dest_')\n        membership_tag_separator: Character(s) separating membership names from properties\n    \"\"\"\n    self._from_identifier = from_identifier\n    self._to_identifier = to_identifier\n    self._membership_tag_separator = membership_tag_separator\n    self._tag_finder = CommonBaseKeyFinder(from_identifier, to_identifier)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mescal.energy_data_handling.model_handling.membership_property_enrichers.DirectionalMembershipPropertyEnricher.identify_from_to_columns","title":"identify_from_to_columns","text":"<pre><code>identify_from_to_columns(column_names: list[str], dataset: Dataset) -&gt; list[str]\n</code></pre> <p>Identifies base names for directional membership column pairs.</p> <p>Analyzes column names to find base membership types that have both 'from' and 'to' variants. For example, identifies 'node' as a base when both 'node_from' and 'node_to' columns exist and represent valid memberships.</p> <p>Parameters:</p> Name Type Description Default <code>column_names</code> <code>list[str]</code> <p>List of column names from the target DataFrame</p> required <code>dataset</code> <code>Dataset</code> <p>MESCAL Dataset for membership validation</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of base column names that have both from/to variants</p> <p>Examples:</p> <p>For a line DataFrame with ['name', 'capacity', 'node_from', 'node_to']:</p> <pre><code>&gt;&gt;&gt; base_columns = enricher.identify_from_to_columns(\n...     line_df.columns, dataset\n... )\n&gt;&gt;&gt; print(base_columns)  # ['node']\n</code></pre> <p>For inter-regional trade with ['flow', 'region_from', 'region_to', 'market_from']:</p> <pre><code>&gt;&gt;&gt; base_columns = enricher.identify_from_to_columns(\n...     trade_df.columns, dataset  \n... )\n&gt;&gt;&gt; print(base_columns)  # ['region'] (market missing 'market_to')\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def identify_from_to_columns(self, column_names: list[str], dataset: Dataset) -&gt; list[str]:\n    \"\"\"\n    Identifies base names for directional membership column pairs.\n\n    Analyzes column names to find base membership types that have both 'from'\n    and 'to' variants. For example, identifies 'node' as a base when both\n    'node_from' and 'node_to' columns exist and represent valid memberships.\n\n    Args:\n        column_names: List of column names from the target DataFrame\n        dataset: MESCAL Dataset for membership validation\n\n    Returns:\n        List of base column names that have both from/to variants\n\n    Examples:\n        For a line DataFrame with ['name', 'capacity', 'node_from', 'node_to']:\n        &gt;&gt;&gt; base_columns = enricher.identify_from_to_columns(\n        ...     line_df.columns, dataset\n        ... )\n        &gt;&gt;&gt; print(base_columns)  # ['node']\n\n        For inter-regional trade with ['flow', 'region_from', 'region_to', 'market_from']:\n        &gt;&gt;&gt; base_columns = enricher.identify_from_to_columns(\n        ...     trade_df.columns, dataset  \n        ... )\n        &gt;&gt;&gt; print(base_columns)  # ['region'] (market missing 'market_to')\n    \"\"\"\n    potential_columns = self._tag_finder.get_keys_for_which_all_association_tags_appear(column_names)\n    return [\n        col for col in potential_columns\n        if dataset.flag_index.column_name_in_model_describes_membership(col)\n    ]\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mescal.energy_data_handling.model_handling.membership_property_enrichers.DirectionalMembershipPropertyEnricher.append_properties","title":"append_properties","text":"<pre><code>append_properties(target_df: DataFrame, dataset: Dataset, membership_tagging: MembershipTagging = NONE) -&gt; DataFrame\n</code></pre> <p>Enriches DataFrame with properties from all directional relationships.</p> <p>Performs comprehensive directional enrichment by identifying all from/to column pairs and adding properties from both directions. Essential for network analysis where understanding characteristics of connected nodes, regions, or components is crucial for energy system modeling.</p> <p>Each directional relationship results in two sets of enriched properties: one for the 'from' direction and one for the 'to' direction, clearly distinguished by directional suffixes.</p> <p>Parameters:</p> Name Type Description Default <code>target_df</code> <code>DataFrame</code> <p>DataFrame with directional relationships (e.g., transmission lines)</p> required <code>dataset</code> <code>Dataset</code> <p>MESCAL Dataset containing model objects and their properties</p> required <code>membership_tagging</code> <code>MembershipTagging</code> <p>Strategy for property naming (applied before directional tags)</p> <code>NONE</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced DataFrame with directional properties added. Original data preserved,</p> <code>DataFrame</code> <p>new columns follow pattern: [prefix_]property_name[_suffix]_direction</p> <p>Raises:</p> Type Description <code>Warning</code> <p>Logged when referenced objects missing from source DataFrames</p> <p>Examples:</p> <p>Network transmission analysis:</p> <pre><code>&gt;&gt;&gt; # Transmission lines with node endpoints\n&gt;&gt;&gt; enriched_lines = enricher.append_properties(\n...     transmission_df, dataset, MembershipTagging.NONE\n... )\n&gt;&gt;&gt; # Result: original columns + 'voltage_from', 'voltage_to', \n&gt;&gt;&gt; #         'area_from', 'area_to', etc.\n</code></pre> <pre><code>&gt;&gt;&gt; # Inter-regional trade flows\n&gt;&gt;&gt; enriched_trade = enricher.append_properties(\n...     trade_df, dataset, MembershipTagging.PREFIX\n... )\n&gt;&gt;&gt; # Result: trade data + 'region_gdp_from', 'region_gdp_to', etc.\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def append_properties(\n        self,\n        target_df: pd.DataFrame,\n        dataset: Dataset,\n        membership_tagging: MembershipTagging = MembershipTagging.NONE\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Enriches DataFrame with properties from all directional relationships.\n\n    Performs comprehensive directional enrichment by identifying all from/to\n    column pairs and adding properties from both directions. Essential for\n    network analysis where understanding characteristics of connected nodes,\n    regions, or components is crucial for energy system modeling.\n\n    Each directional relationship results in two sets of enriched properties:\n    one for the 'from' direction and one for the 'to' direction, clearly\n    distinguished by directional suffixes.\n\n    Args:\n        target_df: DataFrame with directional relationships (e.g., transmission lines)\n        dataset: MESCAL Dataset containing model objects and their properties\n        membership_tagging: Strategy for property naming (applied before directional tags)\n\n    Returns:\n        Enhanced DataFrame with directional properties added. Original data preserved,\n        new columns follow pattern: [prefix_]property_name[_suffix]_direction\n\n    Raises:\n        Warning: Logged when referenced objects missing from source DataFrames\n\n    Examples:\n        Network transmission analysis:\n\n        &gt;&gt;&gt; # Transmission lines with node endpoints\n        &gt;&gt;&gt; enriched_lines = enricher.append_properties(\n        ...     transmission_df, dataset, MembershipTagging.NONE\n        ... )\n        &gt;&gt;&gt; # Result: original columns + 'voltage_from', 'voltage_to', \n        &gt;&gt;&gt; #         'area_from', 'area_to', etc.\n\n        &gt;&gt;&gt; # Inter-regional trade flows\n        &gt;&gt;&gt; enriched_trade = enricher.append_properties(\n        ...     trade_df, dataset, MembershipTagging.PREFIX\n        ... )\n        &gt;&gt;&gt; # Result: trade data + 'region_gdp_from', 'region_gdp_to', etc.\n    \"\"\"\n    membership_base_columns = self.identify_from_to_columns(target_df.columns, dataset)\n    result_df = target_df.copy()\n\n    for base_column in membership_base_columns:\n        result_df = self.append_directional_properties(\n            result_df,\n            dataset,\n            base_column,\n            membership_tagging\n        )\n\n    return result_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mescal.energy_data_handling.model_handling.membership_property_enrichers.DirectionalMembershipPropertyEnricher.append_directional_properties_in_source_to_target_df","title":"append_directional_properties_in_source_to_target_df","text":"<pre><code>append_directional_properties_in_source_to_target_df(target_df: DataFrame, source_df: DataFrame, base_column: str, membership_tagging: MembershipTagging = NONE) -&gt; DataFrame\n</code></pre> <p>Enriches DataFrame with properties from a directional relationship.</p> <p>Handles a single from/to membership pair by adding the corresponding model DataFrame's properties with directional tags.</p> <p>The method processes both directions (from/to) for the specified base column, adding properties with appropriate directional suffixes. Missing references are handled gracefully with NaN preservation.</p> <p>Parameters:</p> Name Type Description Default <code>target_df</code> <code>DataFrame</code> <p>DataFrame containing the directional columns</p> required <code>source_df</code> <code>DataFrame</code> <p>DataFrame containing the properties</p> required <code>base_column</code> <code>str</code> <p>Base membership name (e.g., 'node' for 'node_from'/'node_to')</p> required <code>membership_tagging</code> <code>MembershipTagging</code> <p>Property naming strategy (applied before directional tags)</p> <code>NONE</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with directional properties added for the specified relationship.</p> <code>DataFrame</code> <p>Properties follow naming pattern: [prefix_]property[_suffix]_direction</p> <p>Raises:</p> Type Description <code>Warning</code> <p>Logged when referenced objects are missing from source DataFrame</p> <p>Examples:</p> <p>Targeted directional enrichment:</p> <pre><code>&gt;&gt;&gt; # Add only node properties to transmission lines\n&gt;&gt;&gt; lines_with_nodes = enricher.append_directional_properties(\n...     line_df, node_df, 'node', MembershipTagging.NONE\n... )\n&gt;&gt;&gt; # Result: lines + 'voltage_from', 'voltage_to', 'area_from', 'area_to'\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def append_directional_properties_in_source_to_target_df(\n        self,\n        target_df: pd.DataFrame,\n        source_df: pd.DataFrame,\n        base_column: str,\n        membership_tagging: MembershipTagging = MembershipTagging.NONE\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Enriches DataFrame with properties from a directional relationship.\n\n    Handles a single from/to membership pair by adding the corresponding model\n    DataFrame's properties with directional tags.\n\n    The method processes both directions (from/to) for the specified base column,\n    adding properties with appropriate directional suffixes. Missing references\n    are handled gracefully with NaN preservation.\n\n    Args:\n        target_df: DataFrame containing the directional columns\n        source_df: DataFrame containing the properties\n        base_column: Base membership name (e.g., 'node' for 'node_from'/'node_to')\n        membership_tagging: Property naming strategy (applied before directional tags)\n\n    Returns:\n        DataFrame with directional properties added for the specified relationship.\n        Properties follow naming pattern: [prefix_]property[_suffix]_direction\n\n    Raises:\n        Warning: Logged when referenced objects are missing from source DataFrame\n\n    Examples:\n        Targeted directional enrichment:\n\n        &gt;&gt;&gt; # Add only node properties to transmission lines\n        &gt;&gt;&gt; lines_with_nodes = enricher.append_directional_properties(\n        ...     line_df, node_df, 'node', MembershipTagging.NONE\n        ... )\n        &gt;&gt;&gt; # Result: lines + 'voltage_from', 'voltage_to', 'area_from', 'area_to'\n    \"\"\"\n    result_df = target_df.copy()\n    for tag in [self._from_identifier, self._to_identifier]:\n        membership_column = self._get_full_column_name(base_column, tag, target_df.columns)\n\n        if membership_column not in target_df.columns:\n            continue\n\n        membership_objects = target_df[membership_column].dropna().unique()\n        missing_objects = set(membership_objects) - set(source_df.index)\n\n        if missing_objects:\n            self._log_missing_objects_warning(missing_objects, membership_column)\n\n        source_properties = source_df.copy()\n\n        match membership_tagging:\n            case MembershipTagging.PREFIX:\n                source_properties = source_properties.add_prefix(f\"{base_column}{self._membership_tag_separator}\")\n            case MembershipTagging.SUFFIX:\n                source_properties = source_properties.add_suffix(f\"{self._membership_tag_separator}{base_column}\")\n\n        source_properties = source_properties.add_suffix(tag)\n\n        result_df = result_df.merge(\n            source_properties,\n            left_on=membership_column,\n            right_index=True,\n            how=\"left\"\n        )\n    return result_df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/","title":"MESCAL Energy Variable Utils","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/#mescal.energy_data_handling.variable_utils","title":"variable_utils","text":"<p>MESCAL Energy Data Variable Utilities</p> <p>This package provides specialized utilities for processing and transforming energy system variables in MESCAL (Modular Energy Scenario Comparison Analysis Library). These utilities handle common operations on energy data including flow aggregations, price calculations, congestion analysis, and bidirectional data processing.</p> Key Components <ul> <li>RegionalTradeBalanceCalculator: Aggregates bidirectional power flows between regions</li> <li>CongestionRentCalculator: Calculates congestion rents for transmission lines</li> <li>AggregatedColumnAppender: Aggregates columns by common identifiers (e.g., technology types)</li> <li>UpDownNetAppender: Processes bidirectional data to create net and total columns</li> </ul>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/","title":"MESCAL Congestion Rent Calculator","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mescal.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator","title":"CongestionRentCalculator  <code>dataclass</code>","text":"<p>Calculates congestion rents for electricity transmission lines.</p> <p>Congestion rent represents the economic value captured by transmission assets due to price differences between nodes. It's calculated as the product of power flow, price spread, and time granularity.</p> <p>The calculator handles bidirectional flows (up/down) and accounts for transmission losses by using separate sent and received quantities. This provides accurate congestion rent calculations that reflect actual market conditions and physical constraints.</p> <p>Mathematical formulation: - Congestion rent (up) = granularity \u00d7 (received_up \u00d7 price_to - sent_up \u00d7 price_from) - Congestion rent (down) = granularity \u00d7 (received_down \u00d7 price_from - sent_down \u00d7 price_to) - Total congestion rent = congestion_rent_up + congestion_rent_down</p> <p>Attributes:</p> Name Type Description <code>sent_up</code> <code>Series</code> <p>Power sent in up direction (MW or MWh)</p> <code>received_up</code> <code>Series</code> <p>Power received in up direction after losses (MW or MWh)  </p> <code>sent_down</code> <code>Series</code> <p>Power sent in down direction (MW or MWh)</p> <code>received_down</code> <code>Series</code> <p>Power received in down direction after losses (MW or MWh)</p> <code>price_node_from</code> <code>Series</code> <p>Price at sending node (\u20ac/MWh)</p> <code>price_node_to</code> <code>Series</code> <p>Price at receiving node (\u20ac/MWh)</p> <code>granularity_hrs</code> <code>Series | float</code> <p>Time granularity in hours (auto-detected if None)</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Time series data\n&gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=3, freq='h')\n&gt;&gt;&gt; # Flow and price data\n&gt;&gt;&gt; calc = CongestionRentCalculator(\n...     sent_up=pd.Series([100, 150, 200], index=index),\n...     received_up=pd.Series([95, 142, 190], index=index),  # 5% losses\n...     sent_down=pd.Series([50, 75, 100], index=index),\n...     received_down=pd.Series([48, 71, 95], index=index),  # 4% losses\n...     price_node_from=pd.Series([45, 50, 55], index=index),\n...     price_node_to=pd.Series([65, 70, 75], index=index)\n... )\n&gt;&gt;&gt; total_rent = calc.calculate()\n&gt;&gt;&gt; print(total_rent)  # Congestion rent in \u20ac\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/congestion_rent.py</code> <pre><code>@dataclass\nclass CongestionRentCalculator:\n    \"\"\"Calculates congestion rents for electricity transmission lines.\n\n    Congestion rent represents the economic value captured by transmission\n    assets due to price differences between nodes. It's calculated as the\n    product of power flow, price spread, and time granularity.\n\n    The calculator handles bidirectional flows (up/down) and accounts for\n    transmission losses by using separate sent and received quantities.\n    This provides accurate congestion rent calculations that reflect actual\n    market conditions and physical constraints.\n\n    Mathematical formulation:\n    - Congestion rent (up) = granularity \u00d7 (received_up \u00d7 price_to - sent_up \u00d7 price_from)\n    - Congestion rent (down) = granularity \u00d7 (received_down \u00d7 price_from - sent_down \u00d7 price_to)\n    - Total congestion rent = congestion_rent_up + congestion_rent_down\n\n    Attributes:\n        sent_up: Power sent in up direction (MW or MWh)\n        received_up: Power received in up direction after losses (MW or MWh)  \n        sent_down: Power sent in down direction (MW or MWh)\n        received_down: Power received in down direction after losses (MW or MWh)\n        price_node_from: Price at sending node (\u20ac/MWh)\n        price_node_to: Price at receiving node (\u20ac/MWh)\n        granularity_hrs: Time granularity in hours (auto-detected if None)\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; # Time series data\n        &gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=3, freq='h')\n        &gt;&gt;&gt; # Flow and price data\n        &gt;&gt;&gt; calc = CongestionRentCalculator(\n        ...     sent_up=pd.Series([100, 150, 200], index=index),\n        ...     received_up=pd.Series([95, 142, 190], index=index),  # 5% losses\n        ...     sent_down=pd.Series([50, 75, 100], index=index),\n        ...     received_down=pd.Series([48, 71, 95], index=index),  # 4% losses\n        ...     price_node_from=pd.Series([45, 50, 55], index=index),\n        ...     price_node_to=pd.Series([65, 70, 75], index=index)\n        ... )\n        &gt;&gt;&gt; total_rent = calc.calculate()\n        &gt;&gt;&gt; print(total_rent)  # Congestion rent in \u20ac\n    \"\"\"\n    sent_up: pd.Series\n    received_up: pd.Series\n    sent_down: pd.Series\n    received_down: pd.Series\n    price_node_from: pd.Series\n    price_node_to: pd.Series\n    granularity_hrs: pd.Series | float = None\n\n    def __post_init__(self):\n        \"\"\"Initialize granularity and validate input consistency.\"\"\"\n        if self.granularity_hrs is None:\n            if isinstance(self.sent_up.index, pd.DatetimeIndex):\n                if len(self.sent_up.index) &gt; 0:\n                    from mescal.energy_data_handling.granularity_analyzer import TimeSeriesGranularityAnalyzer\n                    analyzer = TimeSeriesGranularityAnalyzer(strict_mode=False)\n                    self.granularity_hrs = analyzer.get_granularity_as_series_of_hours(self.sent_up.index)\n                else:\n                    self.granularity_hrs = 0\n            else:\n                logger.warning(f'Granularity for CongestionRentCalculator is defaulting back to 1 hrs.')\n                self.granularity_hrs = 1\n        if isinstance(self.granularity_hrs, (float, int)):\n            self.granularity_hrs = pd.Series(self.granularity_hrs, index=self.sent_up.index)\n        self.__check_indices()\n\n    def __check_indices(self):\n        \"\"\"Validate that all input Series have matching indices.\n\n        Raises:\n            ValueError: If any Series has mismatched index with sent_up\n        \"\"\"\n        ref_index = self.sent_up.index\n        to_check = [\n            self.received_up,\n            self.sent_down,\n            self.received_down,\n            self.price_node_from,\n            self.price_node_to\n        ]\n        for v in to_check:\n            if not ref_index.equals(v.index):\n                raise ValueError(f'All indices of provided series must be equal.')\n\n    @property\n    def congestion_rent_up(self) -&gt; pd.Series:\n        \"\"\"Calculate congestion rent for up direction flows.\n\n        Returns:\n            Series with congestion rents in up direction (\u20ac)\n        \"\"\"\n        return self.granularity_hrs * (\n                self.received_up * self.price_node_to -\n                self.sent_up * self.price_node_from\n        )\n\n    @property\n    def congestion_rent_down(self) -&gt; pd.Series:\n        \"\"\"Calculate congestion rent for down direction flows.\n\n        Returns:\n            Series with congestion rents in down direction (\u20ac)\n        \"\"\"\n        return self.granularity_hrs * (\n                self.received_down * self.price_node_from -\n                self.sent_down * self.price_node_to\n        )\n\n    @property\n    def congestion_rent_total(self) -&gt; pd.Series:\n        \"\"\"Calculate total congestion rent (sum of up and down directions).\n\n        Returns:\n            Series with total congestion rents (\u20ac)\n        \"\"\"\n        return self.congestion_rent_up + self.congestion_rent_down\n\n    def calculate(self) -&gt; pd.Series:\n        \"\"\"Calculate total congestion rent (convenience method).\n\n        Returns:\n            Series with total congestion rents in \u20ac (same as congestion_rent_total property)\n        \"\"\"\n        return self.congestion_rent_total\n\n    @classmethod\n    def from_net_flow_without_losses(\n            cls,\n            net_flow: pd.Series,\n            price_node_from: pd.Series,\n            price_node_to: pd.Series,\n            granularity_hrs: float = None\n    ) -&gt; pd.Series:\n        \"\"\"Calculate congestion rent from net flow data assuming no losses.\n\n        Convenience method for cases where transmission losses are negligible\n        or not available. Splits net flow into unidirectional components and\n        assumes sent equals received for each direction.\n\n        Args:\n            net_flow: Net power flow (positive = up direction, negative = down direction) in MW or MWh\n            price_node_from: Price at sending node (\u20ac/MWh)\n            price_node_to: Price at receiving node (\u20ac/MWh)\n            granularity_hrs: Time granularity in hours (auto-detected if None)\n\n        Returns:\n            Series with total congestion rents in \u20ac\n\n        Example:\n\n            &gt;&gt;&gt; # Net flow with price data\n            &gt;&gt;&gt; net_flow = pd.Series([100, -50, 75], index=time_index)  \n            &gt;&gt;&gt; rent = CongestionRentCalculator.from_net_flow_without_losses(\n            ...     net_flow, price_from, price_to\n            ... )\n        \"\"\"\n        sent_up = net_flow.clip(lower=0)\n        sent_down = (-net_flow).clip(lower=0)\n        return cls(\n            sent_up=sent_up,\n            received_up=sent_up,\n            sent_down=sent_down,\n            received_down=sent_down,\n            price_node_from=price_node_from,\n            price_node_to=price_node_to,\n            granularity_hrs=granularity_hrs\n        ).calculate()\n\n    @classmethod\n    def from_up_and_down_flow_without_losses(\n            cls,\n            flow_up: pd.Series,\n            flow_down: pd.Series,\n            price_node_from: pd.Series,\n            price_node_to: pd.Series,\n            granularity_hrs: float = None\n    ) -&gt; pd.Series:\n        \"\"\"Calculate congestion rent from bidirectional flow data assuming no losses.\n\n        Convenience method for cases where flows are already separated into up/down\n        directions and transmission losses are negligible. Assumes sent equals\n        received for each direction.\n\n        Args:\n            flow_up: Power flow in up direction (MW or MWh, non-negative)\n            flow_down: Power flow in down direction (MW or MWh, non-negative)  \n            price_node_from: Price at sending node (\u20ac/MWh)\n            price_node_to: Price at receiving node (\u20ac/MWh)\n            granularity_hrs: Time granularity in hours (auto-detected if None)\n\n        Returns:\n            Series with total congestion rents in \u20ac\n\n        Example:\n\n            &gt;&gt;&gt; # Separate up/down flows\n            &gt;&gt;&gt; flow_up = pd.Series([100, 0, 75], index=time_index)\n            &gt;&gt;&gt; flow_down = pd.Series([0, 50, 0], index=time_index)\n            &gt;&gt;&gt; rent = CongestionRentCalculator.from_up_and_down_flow_without_losses(\n            ...     flow_up, flow_down, price_from, price_to\n            ... )\n        \"\"\"\n        return cls(\n            sent_up=flow_up,\n            received_up=flow_up,\n            sent_down=flow_down,\n            received_down=flow_down,\n            price_node_from=price_node_from,\n            price_node_to=price_node_to,\n            granularity_hrs=granularity_hrs\n        ).calculate()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mescal.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.congestion_rent_up","title":"congestion_rent_up  <code>property</code>","text":"<pre><code>congestion_rent_up: Series\n</code></pre> <p>Calculate congestion rent for up direction flows.</p> <p>Returns:</p> Type Description <code>Series</code> <p>Series with congestion rents in up direction (\u20ac)</p>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mescal.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.congestion_rent_down","title":"congestion_rent_down  <code>property</code>","text":"<pre><code>congestion_rent_down: Series\n</code></pre> <p>Calculate congestion rent for down direction flows.</p> <p>Returns:</p> Type Description <code>Series</code> <p>Series with congestion rents in down direction (\u20ac)</p>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mescal.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.congestion_rent_total","title":"congestion_rent_total  <code>property</code>","text":"<pre><code>congestion_rent_total: Series\n</code></pre> <p>Calculate total congestion rent (sum of up and down directions).</p> <p>Returns:</p> Type Description <code>Series</code> <p>Series with total congestion rents (\u20ac)</p>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mescal.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize granularity and validate input consistency.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/congestion_rent.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize granularity and validate input consistency.\"\"\"\n    if self.granularity_hrs is None:\n        if isinstance(self.sent_up.index, pd.DatetimeIndex):\n            if len(self.sent_up.index) &gt; 0:\n                from mescal.energy_data_handling.granularity_analyzer import TimeSeriesGranularityAnalyzer\n                analyzer = TimeSeriesGranularityAnalyzer(strict_mode=False)\n                self.granularity_hrs = analyzer.get_granularity_as_series_of_hours(self.sent_up.index)\n            else:\n                self.granularity_hrs = 0\n        else:\n            logger.warning(f'Granularity for CongestionRentCalculator is defaulting back to 1 hrs.')\n            self.granularity_hrs = 1\n    if isinstance(self.granularity_hrs, (float, int)):\n        self.granularity_hrs = pd.Series(self.granularity_hrs, index=self.sent_up.index)\n    self.__check_indices()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mescal.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.__check_indices","title":"__check_indices","text":"<pre><code>__check_indices()\n</code></pre> <p>Validate that all input Series have matching indices.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any Series has mismatched index with sent_up</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/congestion_rent.py</code> <pre><code>def __check_indices(self):\n    \"\"\"Validate that all input Series have matching indices.\n\n    Raises:\n        ValueError: If any Series has mismatched index with sent_up\n    \"\"\"\n    ref_index = self.sent_up.index\n    to_check = [\n        self.received_up,\n        self.sent_down,\n        self.received_down,\n        self.price_node_from,\n        self.price_node_to\n    ]\n    for v in to_check:\n        if not ref_index.equals(v.index):\n            raise ValueError(f'All indices of provided series must be equal.')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mescal.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.calculate","title":"calculate","text":"<pre><code>calculate() -&gt; Series\n</code></pre> <p>Calculate total congestion rent (convenience method).</p> <p>Returns:</p> Type Description <code>Series</code> <p>Series with total congestion rents in \u20ac (same as congestion_rent_total property)</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/congestion_rent.py</code> <pre><code>def calculate(self) -&gt; pd.Series:\n    \"\"\"Calculate total congestion rent (convenience method).\n\n    Returns:\n        Series with total congestion rents in \u20ac (same as congestion_rent_total property)\n    \"\"\"\n    return self.congestion_rent_total\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mescal.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.from_net_flow_without_losses","title":"from_net_flow_without_losses  <code>classmethod</code>","text":"<pre><code>from_net_flow_without_losses(net_flow: Series, price_node_from: Series, price_node_to: Series, granularity_hrs: float = None) -&gt; Series\n</code></pre> <p>Calculate congestion rent from net flow data assuming no losses.</p> <p>Convenience method for cases where transmission losses are negligible or not available. Splits net flow into unidirectional components and assumes sent equals received for each direction.</p> <p>Parameters:</p> Name Type Description Default <code>net_flow</code> <code>Series</code> <p>Net power flow (positive = up direction, negative = down direction) in MW or MWh</p> required <code>price_node_from</code> <code>Series</code> <p>Price at sending node (\u20ac/MWh)</p> required <code>price_node_to</code> <code>Series</code> <p>Price at receiving node (\u20ac/MWh)</p> required <code>granularity_hrs</code> <code>float</code> <p>Time granularity in hours (auto-detected if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series with total congestion rents in \u20ac</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Net flow with price data\n&gt;&gt;&gt; net_flow = pd.Series([100, -50, 75], index=time_index)  \n&gt;&gt;&gt; rent = CongestionRentCalculator.from_net_flow_without_losses(\n...     net_flow, price_from, price_to\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/congestion_rent.py</code> <pre><code>@classmethod\ndef from_net_flow_without_losses(\n        cls,\n        net_flow: pd.Series,\n        price_node_from: pd.Series,\n        price_node_to: pd.Series,\n        granularity_hrs: float = None\n) -&gt; pd.Series:\n    \"\"\"Calculate congestion rent from net flow data assuming no losses.\n\n    Convenience method for cases where transmission losses are negligible\n    or not available. Splits net flow into unidirectional components and\n    assumes sent equals received for each direction.\n\n    Args:\n        net_flow: Net power flow (positive = up direction, negative = down direction) in MW or MWh\n        price_node_from: Price at sending node (\u20ac/MWh)\n        price_node_to: Price at receiving node (\u20ac/MWh)\n        granularity_hrs: Time granularity in hours (auto-detected if None)\n\n    Returns:\n        Series with total congestion rents in \u20ac\n\n    Example:\n\n        &gt;&gt;&gt; # Net flow with price data\n        &gt;&gt;&gt; net_flow = pd.Series([100, -50, 75], index=time_index)  \n        &gt;&gt;&gt; rent = CongestionRentCalculator.from_net_flow_without_losses(\n        ...     net_flow, price_from, price_to\n        ... )\n    \"\"\"\n    sent_up = net_flow.clip(lower=0)\n    sent_down = (-net_flow).clip(lower=0)\n    return cls(\n        sent_up=sent_up,\n        received_up=sent_up,\n        sent_down=sent_down,\n        received_down=sent_down,\n        price_node_from=price_node_from,\n        price_node_to=price_node_to,\n        granularity_hrs=granularity_hrs\n    ).calculate()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mescal.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.from_up_and_down_flow_without_losses","title":"from_up_and_down_flow_without_losses  <code>classmethod</code>","text":"<pre><code>from_up_and_down_flow_without_losses(flow_up: Series, flow_down: Series, price_node_from: Series, price_node_to: Series, granularity_hrs: float = None) -&gt; Series\n</code></pre> <p>Calculate congestion rent from bidirectional flow data assuming no losses.</p> <p>Convenience method for cases where flows are already separated into up/down directions and transmission losses are negligible. Assumes sent equals received for each direction.</p> <p>Parameters:</p> Name Type Description Default <code>flow_up</code> <code>Series</code> <p>Power flow in up direction (MW or MWh, non-negative)</p> required <code>flow_down</code> <code>Series</code> <p>Power flow in down direction (MW or MWh, non-negative)  </p> required <code>price_node_from</code> <code>Series</code> <p>Price at sending node (\u20ac/MWh)</p> required <code>price_node_to</code> <code>Series</code> <p>Price at receiving node (\u20ac/MWh)</p> required <code>granularity_hrs</code> <code>float</code> <p>Time granularity in hours (auto-detected if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series with total congestion rents in \u20ac</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Separate up/down flows\n&gt;&gt;&gt; flow_up = pd.Series([100, 0, 75], index=time_index)\n&gt;&gt;&gt; flow_down = pd.Series([0, 50, 0], index=time_index)\n&gt;&gt;&gt; rent = CongestionRentCalculator.from_up_and_down_flow_without_losses(\n...     flow_up, flow_down, price_from, price_to\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/congestion_rent.py</code> <pre><code>@classmethod\ndef from_up_and_down_flow_without_losses(\n        cls,\n        flow_up: pd.Series,\n        flow_down: pd.Series,\n        price_node_from: pd.Series,\n        price_node_to: pd.Series,\n        granularity_hrs: float = None\n) -&gt; pd.Series:\n    \"\"\"Calculate congestion rent from bidirectional flow data assuming no losses.\n\n    Convenience method for cases where flows are already separated into up/down\n    directions and transmission losses are negligible. Assumes sent equals\n    received for each direction.\n\n    Args:\n        flow_up: Power flow in up direction (MW or MWh, non-negative)\n        flow_down: Power flow in down direction (MW or MWh, non-negative)  \n        price_node_from: Price at sending node (\u20ac/MWh)\n        price_node_to: Price at receiving node (\u20ac/MWh)\n        granularity_hrs: Time granularity in hours (auto-detected if None)\n\n    Returns:\n        Series with total congestion rents in \u20ac\n\n    Example:\n\n        &gt;&gt;&gt; # Separate up/down flows\n        &gt;&gt;&gt; flow_up = pd.Series([100, 0, 75], index=time_index)\n        &gt;&gt;&gt; flow_down = pd.Series([0, 50, 0], index=time_index)\n        &gt;&gt;&gt; rent = CongestionRentCalculator.from_up_and_down_flow_without_losses(\n        ...     flow_up, flow_down, price_from, price_to\n        ... )\n    \"\"\"\n    return cls(\n        sent_up=flow_up,\n        received_up=flow_up,\n        sent_down=flow_down,\n        received_down=flow_down,\n        price_node_from=price_node_from,\n        price_node_to=price_node_to,\n        granularity_hrs=granularity_hrs\n    ).calculate()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/direction_aggs/","title":"MESCAL Aggregate Directional (Up- / Down-) to Net-Variables","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/direction_aggs/#mescal.energy_data_handling.variable_utils.aggregate_up_down_directions_to_net_column.UpDownNetAppender","title":"UpDownNetAppender","text":"<p>Computes and appends net and total columns from bidirectional data.</p> <p>This utility class processes DataFrames containing bidirectional energy data (e.g., power flows, trade volumes) and creates derived columns representing net flows (up - down) and total flows (up + down). It automatically identifies column pairs based on configurable directional identifiers.</p> <p>Energy market context: Bidirectional energy data is common in electricity markets where flows, trades, or capacities can occur in both directions between nodes or areas. Net calculations show the overall direction and magnitude of transfers, while total calculations show overall activity levels.</p> <p>Common use cases: - Power flow analysis: Converting line flows to net flows - Trade balance: Net imports/exports from bilateral trade data</p> <p>The class uses CommonBaseKeyFinder to identify related column pairs and supports flexible naming conventions for input and output columns.</p> <p>Parameters:</p> Name Type Description Default <code>up_identifier</code> <code>str</code> <p>String identifier for \"up\" direction columns (default: '_up')</p> <code>'_up'</code> <code>down_identifier</code> <code>str</code> <p>String identifier for \"down\" direction columns (default: '_down')</p> <code>'_down'</code> <code>net_col_suffix</code> <code>str</code> <p>Suffix for generated net columns (default: '_net')</p> <code>'_net'</code> <code>net_col_prefix</code> <code>str</code> <p>Prefix for generated net columns (default: None)</p> <code>None</code> <code>total_col_suffix</code> <code>str</code> <p>Suffix for generated total columns (default: '_total')</p> <code>'_total'</code> <code>total_col_prefix</code> <code>str</code> <p>Prefix for generated total columns (default: None)</p> <code>None</code> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If neither suffix nor prefix provided for net or total columns</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Data with bidirectional flows\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'DE_FR_up': [100, 200, 300],\n...     'DE_FR_down': [30, 50, 100],\n...     'FR_BE_up': [400, 500, 600],\n...     'FR_BE_down': [150, 200, 300]\n... })\n&gt;&gt;&gt; \n&gt;&gt;&gt; appender = UpDownNetAppender()\n&gt;&gt;&gt; result = appender.append_net_columns_from_up_down_columns(data)\n&gt;&gt;&gt; print(result.columns)  # Includes DE_FR_net, FR_BE_net\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/aggregate_up_down_directions_to_net_column.py</code> <pre><code>class UpDownNetAppender:\n    \"\"\"Computes and appends net and total columns from bidirectional data.\n\n    This utility class processes DataFrames containing bidirectional energy data\n    (e.g., power flows, trade volumes) and creates derived columns representing\n    net flows (up - down) and total flows (up + down). It automatically identifies\n    column pairs based on configurable directional identifiers.\n\n    Energy market context:\n    Bidirectional energy data is common in electricity markets where flows,\n    trades, or capacities can occur in both directions between nodes or areas.\n    Net calculations show the overall direction and magnitude of transfers,\n    while total calculations show overall activity levels.\n\n    Common use cases:\n    - Power flow analysis: Converting line flows to net flows\n    - Trade balance: Net imports/exports from bilateral trade data\n\n    The class uses CommonBaseKeyFinder to identify related column pairs and\n    supports flexible naming conventions for input and output columns.\n\n    Args:\n        up_identifier: String identifier for \"up\" direction columns (default: '_up')\n        down_identifier: String identifier for \"down\" direction columns (default: '_down')\n        net_col_suffix: Suffix for generated net columns (default: '_net')\n        net_col_prefix: Prefix for generated net columns (default: None)\n        total_col_suffix: Suffix for generated total columns (default: '_total')\n        total_col_prefix: Prefix for generated total columns (default: None)\n\n    Raises:\n        AttributeError: If neither suffix nor prefix provided for net or total columns\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; # Data with bidirectional flows\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'DE_FR_up': [100, 200, 300],\n        ...     'DE_FR_down': [30, 50, 100],\n        ...     'FR_BE_up': [400, 500, 600],\n        ...     'FR_BE_down': [150, 200, 300]\n        ... })\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; appender = UpDownNetAppender()\n        &gt;&gt;&gt; result = appender.append_net_columns_from_up_down_columns(data)\n        &gt;&gt;&gt; print(result.columns)  # Includes DE_FR_net, FR_BE_net\n    \"\"\"\n\n    def __init__(\n            self,\n            up_identifier: str = '_up',\n            down_identifier: str = '_down',\n            net_col_suffix: str = '_net',\n            net_col_prefix: str = None,\n            total_col_suffix: str = '_total',\n            total_col_prefix: str = None,\n    ):\n        \"\"\"Initialize the UpDownNetAppender with naming conventions.\n\n        Args:\n            up_identifier: String identifier for \"up\" direction columns\n            down_identifier: String identifier for \"down\" direction columns  \n            net_col_suffix: Suffix for generated net columns\n            net_col_prefix: Prefix for generated net columns\n            total_col_suffix: Suffix for generated total columns\n            total_col_prefix: Prefix for generated total columns\n\n        Raises:\n            AttributeError: If neither suffix nor prefix provided for net or total columns\n        \"\"\"\n        self._up_identifier = up_identifier\n        self._down_identifier = down_identifier\n\n        self._net_col_suffix = net_col_suffix or ''\n        self._net_col_prefix = net_col_prefix or ''\n        if not any([self._net_col_suffix, self._net_col_prefix]):\n            raise AttributeError(\"Either net_col_suffix or net_col_prefix must be provided\")\n\n        self._total_col_suffix = total_col_suffix or ''\n        self._total_col_prefix = total_col_prefix or ''\n        if not any([self._total_col_suffix, self._total_col_prefix]):\n            raise AttributeError(\"Either total_col_suffix or total_col_prefix must be provided\")\n\n        self._common_base_key_finder = CommonBaseKeyFinder(up_identifier, down_identifier)\n\n    def append_net_columns_from_up_down_columns(\n            self,\n            ts_df_with_up_down_columns: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Append net columns (up - down) to the DataFrame.\n\n        Identifies all column pairs with up/down directional identifiers and\n        creates corresponding net columns by subtracting down values from up values.\n        Net columns represent the overall direction and magnitude of flows.\n\n        Args:\n            ts_df_with_up_down_columns: DataFrame containing bidirectional columns\n                with up/down identifiers. Can have single or multi-level column index.\n\n        Returns:\n            DataFrame with original columns plus new net columns. Net values are\n            positive when up &gt; down, negative when down &gt; up.\n\n        Example:\n\n            &gt;&gt;&gt; data = pd.DataFrame({\n            ...     'flow_up': [100, 200], 'flow_down': [30, 50]\n            ... })\n            &gt;&gt;&gt; result = appender.append_net_columns_from_up_down_columns(data)\n            &gt;&gt;&gt; print(result['flow_net'])  # [70, 150]\n        \"\"\"\n        return self._append_columns_from_up_down_columns(ts_df_with_up_down_columns, 'net')\n\n    def append_total_columns_from_up_down_columns(\n            self,\n            ts_df_with_up_down_columns: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Append total columns (up + down) to the DataFrame.\n\n        Identifies all column pairs with up/down directional identifiers and\n        creates corresponding total columns by adding up and down values.\n        Total columns represent the overall activity level regardless of direction.\n\n        Args:\n            ts_df_with_up_down_columns: DataFrame containing bidirectional columns\n                with up/down identifiers. Can have single or multi-level column index.\n\n        Returns:\n            DataFrame with original columns plus new total columns. Total values\n            represent the sum of absolute flows in both directions.\n\n        Example:\n\n            &gt;&gt;&gt; data = pd.DataFrame({\n            ...     'flow_up': [100, 200], 'flow_down': [30, 50]  \n            ... })\n            &gt;&gt;&gt; result = appender.append_total_columns_from_up_down_columns(data)\n            &gt;&gt;&gt; print(result['flow_total'])  # [130, 250]\n        \"\"\"\n        return self._append_columns_from_up_down_columns(ts_df_with_up_down_columns, 'total')\n\n    def _append_columns_from_up_down_columns(\n            self,\n            ts_df_with_up_down_columns: pd.DataFrame,\n            which_agg: Literal['net', 'total']\n    ) -&gt; pd.DataFrame:\n        \"\"\"Internal method to append either net or total columns.\n\n        Args:\n            ts_df_with_up_down_columns: DataFrame with bidirectional data\n            which_agg: Type of aggregation to perform ('net' or 'total')\n\n        Returns:\n            DataFrame with new aggregated columns appended\n\n        Raises:\n            NotImplementedError: If which_agg is not 'net' or 'total'\n        \"\"\"\n\n        up_id = self._up_identifier\n        down_id = self._down_identifier\n\n        _col_names = ts_df_with_up_down_columns.columns.get_level_values(0).unique()\n        up_down_columns = self._common_base_key_finder.get_keys_for_which_all_association_tags_appear(_col_names)\n\n        for c in up_down_columns:\n            up_col = f'{c}{up_id}'\n            down_col = f'{c}{down_id}'\n            if which_agg == 'net':\n                new_col = f'{self._net_col_prefix}{c}{self._net_col_suffix}'\n                new_values = ts_df_with_up_down_columns[up_col].subtract(\n                    ts_df_with_up_down_columns[down_col], fill_value=0,\n                )\n            elif which_agg == 'total':\n                new_col = f'{self._total_col_prefix}{c}{self._total_col_suffix}'\n                new_values = ts_df_with_up_down_columns[up_col].add(\n                    ts_df_with_up_down_columns[down_col], fill_value=0,\n                )\n            else:\n                raise NotImplementedError\n            ts_df_with_up_down_columns = set_column(ts_df_with_up_down_columns, new_col, new_values)\n        return ts_df_with_up_down_columns\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/direction_aggs/#mescal.energy_data_handling.variable_utils.aggregate_up_down_directions_to_net_column.UpDownNetAppender.__init__","title":"__init__","text":"<pre><code>__init__(up_identifier: str = '_up', down_identifier: str = '_down', net_col_suffix: str = '_net', net_col_prefix: str = None, total_col_suffix: str = '_total', total_col_prefix: str = None)\n</code></pre> <p>Initialize the UpDownNetAppender with naming conventions.</p> <p>Parameters:</p> Name Type Description Default <code>up_identifier</code> <code>str</code> <p>String identifier for \"up\" direction columns</p> <code>'_up'</code> <code>down_identifier</code> <code>str</code> <p>String identifier for \"down\" direction columns  </p> <code>'_down'</code> <code>net_col_suffix</code> <code>str</code> <p>Suffix for generated net columns</p> <code>'_net'</code> <code>net_col_prefix</code> <code>str</code> <p>Prefix for generated net columns</p> <code>None</code> <code>total_col_suffix</code> <code>str</code> <p>Suffix for generated total columns</p> <code>'_total'</code> <code>total_col_prefix</code> <code>str</code> <p>Prefix for generated total columns</p> <code>None</code> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If neither suffix nor prefix provided for net or total columns</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/aggregate_up_down_directions_to_net_column.py</code> <pre><code>def __init__(\n        self,\n        up_identifier: str = '_up',\n        down_identifier: str = '_down',\n        net_col_suffix: str = '_net',\n        net_col_prefix: str = None,\n        total_col_suffix: str = '_total',\n        total_col_prefix: str = None,\n):\n    \"\"\"Initialize the UpDownNetAppender with naming conventions.\n\n    Args:\n        up_identifier: String identifier for \"up\" direction columns\n        down_identifier: String identifier for \"down\" direction columns  \n        net_col_suffix: Suffix for generated net columns\n        net_col_prefix: Prefix for generated net columns\n        total_col_suffix: Suffix for generated total columns\n        total_col_prefix: Prefix for generated total columns\n\n    Raises:\n        AttributeError: If neither suffix nor prefix provided for net or total columns\n    \"\"\"\n    self._up_identifier = up_identifier\n    self._down_identifier = down_identifier\n\n    self._net_col_suffix = net_col_suffix or ''\n    self._net_col_prefix = net_col_prefix or ''\n    if not any([self._net_col_suffix, self._net_col_prefix]):\n        raise AttributeError(\"Either net_col_suffix or net_col_prefix must be provided\")\n\n    self._total_col_suffix = total_col_suffix or ''\n    self._total_col_prefix = total_col_prefix or ''\n    if not any([self._total_col_suffix, self._total_col_prefix]):\n        raise AttributeError(\"Either total_col_suffix or total_col_prefix must be provided\")\n\n    self._common_base_key_finder = CommonBaseKeyFinder(up_identifier, down_identifier)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/direction_aggs/#mescal.energy_data_handling.variable_utils.aggregate_up_down_directions_to_net_column.UpDownNetAppender.append_net_columns_from_up_down_columns","title":"append_net_columns_from_up_down_columns","text":"<pre><code>append_net_columns_from_up_down_columns(ts_df_with_up_down_columns: DataFrame) -&gt; DataFrame\n</code></pre> <p>Append net columns (up - down) to the DataFrame.</p> <p>Identifies all column pairs with up/down directional identifiers and creates corresponding net columns by subtracting down values from up values. Net columns represent the overall direction and magnitude of flows.</p> <p>Parameters:</p> Name Type Description Default <code>ts_df_with_up_down_columns</code> <code>DataFrame</code> <p>DataFrame containing bidirectional columns with up/down identifiers. Can have single or multi-level column index.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with original columns plus new net columns. Net values are</p> <code>DataFrame</code> <p>positive when up &gt; down, negative when down &gt; up.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; data = pd.DataFrame({\n...     'flow_up': [100, 200], 'flow_down': [30, 50]\n... })\n&gt;&gt;&gt; result = appender.append_net_columns_from_up_down_columns(data)\n&gt;&gt;&gt; print(result['flow_net'])  # [70, 150]\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/aggregate_up_down_directions_to_net_column.py</code> <pre><code>def append_net_columns_from_up_down_columns(\n        self,\n        ts_df_with_up_down_columns: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Append net columns (up - down) to the DataFrame.\n\n    Identifies all column pairs with up/down directional identifiers and\n    creates corresponding net columns by subtracting down values from up values.\n    Net columns represent the overall direction and magnitude of flows.\n\n    Args:\n        ts_df_with_up_down_columns: DataFrame containing bidirectional columns\n            with up/down identifiers. Can have single or multi-level column index.\n\n    Returns:\n        DataFrame with original columns plus new net columns. Net values are\n        positive when up &gt; down, negative when down &gt; up.\n\n    Example:\n\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'flow_up': [100, 200], 'flow_down': [30, 50]\n        ... })\n        &gt;&gt;&gt; result = appender.append_net_columns_from_up_down_columns(data)\n        &gt;&gt;&gt; print(result['flow_net'])  # [70, 150]\n    \"\"\"\n    return self._append_columns_from_up_down_columns(ts_df_with_up_down_columns, 'net')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/direction_aggs/#mescal.energy_data_handling.variable_utils.aggregate_up_down_directions_to_net_column.UpDownNetAppender.append_total_columns_from_up_down_columns","title":"append_total_columns_from_up_down_columns","text":"<pre><code>append_total_columns_from_up_down_columns(ts_df_with_up_down_columns: DataFrame) -&gt; DataFrame\n</code></pre> <p>Append total columns (up + down) to the DataFrame.</p> <p>Identifies all column pairs with up/down directional identifiers and creates corresponding total columns by adding up and down values. Total columns represent the overall activity level regardless of direction.</p> <p>Parameters:</p> Name Type Description Default <code>ts_df_with_up_down_columns</code> <code>DataFrame</code> <p>DataFrame containing bidirectional columns with up/down identifiers. Can have single or multi-level column index.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with original columns plus new total columns. Total values</p> <code>DataFrame</code> <p>represent the sum of absolute flows in both directions.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; data = pd.DataFrame({\n...     'flow_up': [100, 200], 'flow_down': [30, 50]  \n... })\n&gt;&gt;&gt; result = appender.append_total_columns_from_up_down_columns(data)\n&gt;&gt;&gt; print(result['flow_total'])  # [130, 250]\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/aggregate_up_down_directions_to_net_column.py</code> <pre><code>def append_total_columns_from_up_down_columns(\n        self,\n        ts_df_with_up_down_columns: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Append total columns (up + down) to the DataFrame.\n\n    Identifies all column pairs with up/down directional identifiers and\n    creates corresponding total columns by adding up and down values.\n    Total columns represent the overall activity level regardless of direction.\n\n    Args:\n        ts_df_with_up_down_columns: DataFrame containing bidirectional columns\n            with up/down identifiers. Can have single or multi-level column index.\n\n    Returns:\n        DataFrame with original columns plus new total columns. Total values\n        represent the sum of absolute flows in both directions.\n\n    Example:\n\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'flow_up': [100, 200], 'flow_down': [30, 50]  \n        ... })\n        &gt;&gt;&gt; result = appender.append_total_columns_from_up_down_columns(data)\n        &gt;&gt;&gt; print(result['flow_total'])  # [130, 250]\n    \"\"\"\n    return self._append_columns_from_up_down_columns(ts_df_with_up_down_columns, 'total')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/in_common_aggs/","title":"MESCAL Aggregate Columns with Part in Common","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/in_common_aggs/#mescal.energy_data_handling.variable_utils.aggregate_cols_with_part_in_common.AggregatedColumnAppender","title":"AggregatedColumnAppender","text":"<p>Adds aggregated columns to pandas DataFrames by summing columns that share a common identifier.</p> <p>This utility is particularly useful in energy systems modeling where multiple variables of the same type (e.g., different wind generation sources, various demand components, multiple storage units) need to be aggregated into totals for analysis or visualization.</p> <p>The class handles both single-level and multi-level DataFrame columns, making it suitable for MESCAL's energy variable structures that often include hierarchical indexing (time, regions, technologies, etc.).</p> Energy Domain Context <ul> <li>Aggregates extensive quantities (volumes, energy, capacities) by summation</li> <li>Preserves NaN when all quantities in an aggregation are NaNs</li> <li>Suitable for technology groupings, regional aggregations, and fuel type summations</li> </ul> <p>Parameters:</p> Name Type Description Default <code>in_common_part</code> <code>str</code> <p>The common substring to search for in column names. All columns containing this substring will be summed together.</p> required <code>agg_col_name_prefix</code> <code>str</code> <p>Prefix to add to the aggregated column name. Defaults to empty string.</p> <code>None</code> <code>agg_col_name_suffix</code> <code>str</code> <p>Suffix to add to the aggregated column name. Defaults to empty string.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic energy variable aggregation\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'wind_onshore_gen': [100, 200, 300],\n...     'wind_offshore_gen': [50, 70, 100], \n...     'solar_pv_gen': [30, 40, 50]\n... })\n&gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_suffix='_total')\n&gt;&gt;&gt; result = appender.add_aggregated_column(data)\n&gt;&gt;&gt; # Creates 'wind_total' column with sum of wind_onshore_gen and wind_offshore_gen\n\n&gt;&gt;&gt; # Multi-level columns for scenario analysis\n&gt;&gt;&gt; columns = pd.MultiIndex.from_tuples([\n...     ('wind_onshore', 'scenario_1'), ('wind_onshore', 'scenario_2'),\n...     ('wind_offshore', 'scenario_1'), ('wind_offshore', 'scenario_2')\n... ])\n&gt;&gt;&gt; data = pd.DataFrame(np.random.rand(24, 4), columns=columns)\n&gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_prefix='total_')\n&gt;&gt;&gt; result = appender.add_aggregated_column(data)\n&gt;&gt;&gt; # Creates 'total_wind' with aggregated values for each scenario\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/aggregate_cols_with_part_in_common.py</code> <pre><code>class AggregatedColumnAppender:\n    \"\"\"\n    Adds aggregated columns to pandas DataFrames by summing columns that share a common identifier.\n\n    This utility is particularly useful in energy systems modeling where multiple variables of the same\n    type (e.g., different wind generation sources, various demand components, multiple storage units)\n    need to be aggregated into totals for analysis or visualization.\n\n    The class handles both single-level and multi-level DataFrame columns, making it suitable for\n    MESCAL's energy variable structures that often include hierarchical indexing (time,\n    regions, technologies, etc.).\n\n    Energy Domain Context:\n        - Aggregates extensive quantities (volumes, energy, capacities) by summation\n        - Preserves NaN when all quantities in an aggregation are NaNs\n        - Suitable for technology groupings, regional aggregations, and fuel type summations\n\n    Args:\n        in_common_part (str): The common substring to search for in column names. All columns\n            containing this substring will be summed together.\n        agg_col_name_prefix (str, optional): Prefix to add to the aggregated column name.\n            Defaults to empty string.\n        agg_col_name_suffix (str, optional): Suffix to add to the aggregated column name.\n            Defaults to empty string.\n\n    Examples:\n\n        &gt;&gt;&gt; # Basic energy variable aggregation\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'wind_onshore_gen': [100, 200, 300],\n        ...     'wind_offshore_gen': [50, 70, 100], \n        ...     'solar_pv_gen': [30, 40, 50]\n        ... })\n        &gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_suffix='_total')\n        &gt;&gt;&gt; result = appender.add_aggregated_column(data)\n        &gt;&gt;&gt; # Creates 'wind_total' column with sum of wind_onshore_gen and wind_offshore_gen\n\n        &gt;&gt;&gt; # Multi-level columns for scenario analysis\n        &gt;&gt;&gt; columns = pd.MultiIndex.from_tuples([\n        ...     ('wind_onshore', 'scenario_1'), ('wind_onshore', 'scenario_2'),\n        ...     ('wind_offshore', 'scenario_1'), ('wind_offshore', 'scenario_2')\n        ... ])\n        &gt;&gt;&gt; data = pd.DataFrame(np.random.rand(24, 4), columns=columns)\n        &gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_prefix='total_')\n        &gt;&gt;&gt; result = appender.add_aggregated_column(data)\n        &gt;&gt;&gt; # Creates 'total_wind' with aggregated values for each scenario\n    \"\"\"\n\n    def __init__(\n            self,\n            in_common_part: str,\n            agg_col_name_prefix: str = None,\n            agg_col_name_suffix: str = None,\n    ):\n        self._in_common_part = in_common_part\n        self._agg_col_name_prefix = agg_col_name_prefix or ''\n        self._agg_col_name_suffix = agg_col_name_suffix or ''\n\n    def add_aggregated_column(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Add an aggregated column to the DataFrame by summing matching columns.\n\n        This method identifies all columns containing the specified common part and sums them\n        into a new aggregated column. The aggregation preserves the DataFrame's structure\n        and handles both single-level and multi-level column indices.\n\n        For energy data, this is typically used to:\n        - Aggregate different technology types (e.g., all wind sources)\n        - Sum regional contributions (e.g., all demand in a country)\n        - Combine fuel types (e.g., all fossil fuel generators)\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing energy variables. Can have single-level\n                or multi-level column structure. For multi-level columns, aggregation is performed\n                at the appropriate level while preserving the hierarchy.\n\n        Returns:\n            pd.DataFrame: Original DataFrame with added aggregated column. The new column name\n                follows the pattern: {prefix}{in_common_part}{suffix}\n\n        Raises:\n            ValueError: If no columns contain the specified common part.\n            TypeError: If input is not a pandas DataFrame.\n\n        Examples:\n            Single-level columns - technology aggregation:\n            &gt;&gt;&gt; gen_data = pd.DataFrame({\n            ...     'wind_onshore_MW': [120, 150, 180],\n            ...     'wind_offshore_MW': [80, 90, 100],\n            ...     'solar_pv_MW': [200, 250, 300],\n            ...     'demand_MW': [400, 490, 580]\n            ... })\n            &gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_suffix='_total_MW')\n            &gt;&gt;&gt; result = appender.add_aggregated_column(gen_data)\n            &gt;&gt;&gt; result['wind_total_MW']  # [200, 240, 280]\n\n            Multi-level columns - scenario and regional analysis\n            &gt;&gt;&gt; idx = pd.date_range('2024-01-01', periods=3, freq='h')\n            &gt;&gt;&gt; cols = pd.MultiIndex.from_tuples([\n            ...     ('storage_battery', 'DE', 'scenario_1'), \n            ...     ('storage_battery', 'FR', 'scenario_1'),\n            ...     ('storage_pumped', 'DE', 'scenario_1'),\n            ...     ('storage_pumped', 'FR', 'scenario_1')\n            ... ], names=['technology', 'region', 'scenario'])\n            &gt;&gt;&gt; data = pd.DataFrame(np.random.rand(3, 4), index=idx, columns=cols)\n            &gt;&gt;&gt; appender = AggregatedColumnAppender('storage', agg_col_name_prefix='total_')\n            &gt;&gt;&gt; result = appender.add_aggregated_column(data)\n            &gt;&gt;&gt; # Creates 'total_storage' with sums grouped by (region, scenario)\n\n            Handling NaN values in energy time series\n            &gt;&gt;&gt; price_data = pd.DataFrame({\n            ...     'price_day_ahead': [50.0, np.nan, 45.0],\n            ...     'price_intraday': [np.nan, np.nan, 47.0],  \n            ...     'demand_forecast': [1000, 1100, 1200]\n            ... })\n            &gt;&gt;&gt; appender = AggregatedColumnAppender('price', agg_col_name_suffix='_avg')\n            &gt;&gt;&gt; result = appender.add_aggregated_column(price_data)\n            &gt;&gt;&gt; # 'price_avg': [50.0, NaN, 92.0] - preserves NaN when all inputs are NaN\n\n        Note:\n            For multi-level DataFrames, the aggregation respects the hierarchical structure.\n            If columns have levels beyond the first (technology level), the aggregation\n            groups by those additional levels, creating separate totals for each combination.\n\n            NaN handling follows pandas summation rules: NaN values are ignored unless all\n            values in a row are NaN, in which case the result is NaN.\n        \"\"\"\n        cols = df.columns.get_level_values(0).unique()\n        cols_with_common_part = [x for x in cols if self._in_common_part in x]\n\n        df_in_common = df[cols_with_common_part]\n        if df.columns.nlevels == 1:\n            dff = df_in_common.sum(axis=1)\n            dff.loc[df_in_common.isna().all(axis=1)] = np.nan\n        else:\n            _groupby = list(range(1, df.columns.nlevels))\n            dff = df_in_common.T.groupby(level=_groupby).sum().T\n            _all_na = df_in_common.isna().T.groupby(level=_groupby).all().T\n            if _all_na.any().any():\n                for c in _all_na.columns:\n                    dff.loc[_all_na[c], c] = np.nan\n\n        new_col_name = f'{self._agg_col_name_prefix}{self._in_common_part}{self._agg_col_name_suffix}'\n        df = set_column(df, new_col_name, dff)\n        return df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/in_common_aggs/#mescal.energy_data_handling.variable_utils.aggregate_cols_with_part_in_common.AggregatedColumnAppender.add_aggregated_column","title":"add_aggregated_column","text":"<pre><code>add_aggregated_column(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Add an aggregated column to the DataFrame by summing matching columns.</p> <p>This method identifies all columns containing the specified common part and sums them into a new aggregated column. The aggregation preserves the DataFrame's structure and handles both single-level and multi-level column indices.</p> <p>For energy data, this is typically used to: - Aggregate different technology types (e.g., all wind sources) - Sum regional contributions (e.g., all demand in a country) - Combine fuel types (e.g., all fossil fuel generators)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing energy variables. Can have single-level or multi-level column structure. For multi-level columns, aggregation is performed at the appropriate level while preserving the hierarchy.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Original DataFrame with added aggregated column. The new column name follows the pattern: {prefix}{in_common_part}{suffix}</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no columns contain the specified common part.</p> <code>TypeError</code> <p>If input is not a pandas DataFrame.</p> <p>Examples:</p> <p>Single-level columns - technology aggregation:</p> <pre><code>&gt;&gt;&gt; gen_data = pd.DataFrame({\n...     'wind_onshore_MW': [120, 150, 180],\n...     'wind_offshore_MW': [80, 90, 100],\n...     'solar_pv_MW': [200, 250, 300],\n...     'demand_MW': [400, 490, 580]\n... })\n&gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_suffix='_total_MW')\n&gt;&gt;&gt; result = appender.add_aggregated_column(gen_data)\n&gt;&gt;&gt; result['wind_total_MW']  # [200, 240, 280]\n</code></pre> <p>Multi-level columns - scenario and regional analysis</p> <pre><code>&gt;&gt;&gt; idx = pd.date_range('2024-01-01', periods=3, freq='h')\n&gt;&gt;&gt; cols = pd.MultiIndex.from_tuples([\n...     ('storage_battery', 'DE', 'scenario_1'), \n...     ('storage_battery', 'FR', 'scenario_1'),\n...     ('storage_pumped', 'DE', 'scenario_1'),\n...     ('storage_pumped', 'FR', 'scenario_1')\n... ], names=['technology', 'region', 'scenario'])\n&gt;&gt;&gt; data = pd.DataFrame(np.random.rand(3, 4), index=idx, columns=cols)\n&gt;&gt;&gt; appender = AggregatedColumnAppender('storage', agg_col_name_prefix='total_')\n&gt;&gt;&gt; result = appender.add_aggregated_column(data)\n&gt;&gt;&gt; # Creates 'total_storage' with sums grouped by (region, scenario)\n</code></pre> <p>Handling NaN values in energy time series</p> <pre><code>&gt;&gt;&gt; price_data = pd.DataFrame({\n...     'price_day_ahead': [50.0, np.nan, 45.0],\n...     'price_intraday': [np.nan, np.nan, 47.0],  \n...     'demand_forecast': [1000, 1100, 1200]\n... })\n&gt;&gt;&gt; appender = AggregatedColumnAppender('price', agg_col_name_suffix='_avg')\n&gt;&gt;&gt; result = appender.add_aggregated_column(price_data)\n&gt;&gt;&gt; # 'price_avg': [50.0, NaN, 92.0] - preserves NaN when all inputs are NaN\n</code></pre> Note <p>For multi-level DataFrames, the aggregation respects the hierarchical structure. If columns have levels beyond the first (technology level), the aggregation groups by those additional levels, creating separate totals for each combination.</p> <p>NaN handling follows pandas summation rules: NaN values are ignored unless all values in a row are NaN, in which case the result is NaN.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/aggregate_cols_with_part_in_common.py</code> <pre><code>def add_aggregated_column(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Add an aggregated column to the DataFrame by summing matching columns.\n\n    This method identifies all columns containing the specified common part and sums them\n    into a new aggregated column. The aggregation preserves the DataFrame's structure\n    and handles both single-level and multi-level column indices.\n\n    For energy data, this is typically used to:\n    - Aggregate different technology types (e.g., all wind sources)\n    - Sum regional contributions (e.g., all demand in a country)\n    - Combine fuel types (e.g., all fossil fuel generators)\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing energy variables. Can have single-level\n            or multi-level column structure. For multi-level columns, aggregation is performed\n            at the appropriate level while preserving the hierarchy.\n\n    Returns:\n        pd.DataFrame: Original DataFrame with added aggregated column. The new column name\n            follows the pattern: {prefix}{in_common_part}{suffix}\n\n    Raises:\n        ValueError: If no columns contain the specified common part.\n        TypeError: If input is not a pandas DataFrame.\n\n    Examples:\n        Single-level columns - technology aggregation:\n        &gt;&gt;&gt; gen_data = pd.DataFrame({\n        ...     'wind_onshore_MW': [120, 150, 180],\n        ...     'wind_offshore_MW': [80, 90, 100],\n        ...     'solar_pv_MW': [200, 250, 300],\n        ...     'demand_MW': [400, 490, 580]\n        ... })\n        &gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_suffix='_total_MW')\n        &gt;&gt;&gt; result = appender.add_aggregated_column(gen_data)\n        &gt;&gt;&gt; result['wind_total_MW']  # [200, 240, 280]\n\n        Multi-level columns - scenario and regional analysis\n        &gt;&gt;&gt; idx = pd.date_range('2024-01-01', periods=3, freq='h')\n        &gt;&gt;&gt; cols = pd.MultiIndex.from_tuples([\n        ...     ('storage_battery', 'DE', 'scenario_1'), \n        ...     ('storage_battery', 'FR', 'scenario_1'),\n        ...     ('storage_pumped', 'DE', 'scenario_1'),\n        ...     ('storage_pumped', 'FR', 'scenario_1')\n        ... ], names=['technology', 'region', 'scenario'])\n        &gt;&gt;&gt; data = pd.DataFrame(np.random.rand(3, 4), index=idx, columns=cols)\n        &gt;&gt;&gt; appender = AggregatedColumnAppender('storage', agg_col_name_prefix='total_')\n        &gt;&gt;&gt; result = appender.add_aggregated_column(data)\n        &gt;&gt;&gt; # Creates 'total_storage' with sums grouped by (region, scenario)\n\n        Handling NaN values in energy time series\n        &gt;&gt;&gt; price_data = pd.DataFrame({\n        ...     'price_day_ahead': [50.0, np.nan, 45.0],\n        ...     'price_intraday': [np.nan, np.nan, 47.0],  \n        ...     'demand_forecast': [1000, 1100, 1200]\n        ... })\n        &gt;&gt;&gt; appender = AggregatedColumnAppender('price', agg_col_name_suffix='_avg')\n        &gt;&gt;&gt; result = appender.add_aggregated_column(price_data)\n        &gt;&gt;&gt; # 'price_avg': [50.0, NaN, 92.0] - preserves NaN when all inputs are NaN\n\n    Note:\n        For multi-level DataFrames, the aggregation respects the hierarchical structure.\n        If columns have levels beyond the first (technology level), the aggregation\n        groups by those additional levels, creating separate totals for each combination.\n\n        NaN handling follows pandas summation rules: NaN values are ignored unless all\n        values in a row are NaN, in which case the result is NaN.\n    \"\"\"\n    cols = df.columns.get_level_values(0).unique()\n    cols_with_common_part = [x for x in cols if self._in_common_part in x]\n\n    df_in_common = df[cols_with_common_part]\n    if df.columns.nlevels == 1:\n        dff = df_in_common.sum(axis=1)\n        dff.loc[df_in_common.isna().all(axis=1)] = np.nan\n    else:\n        _groupby = list(range(1, df.columns.nlevels))\n        dff = df_in_common.T.groupby(level=_groupby).sum().T\n        _all_na = df_in_common.isna().T.groupby(level=_groupby).all().T\n        if _all_na.any().any():\n            for c in _all_na.columns:\n                dff.loc[_all_na[c], c] = np.nan\n\n    new_col_name = f'{self._agg_col_name_prefix}{self._in_common_part}{self._agg_col_name_suffix}'\n    df = set_column(df, new_col_name, dff)\n    return df\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/regional_trade_balance/","title":"MESCAL Regional Trade Balance Calculator","text":""},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/regional_trade_balance/#mescal.energy_data_handling.variable_utils.regional_trade_balance_calculator.RegionalTradeBalanceCalculator","title":"RegionalTradeBalanceCalculator","text":"<p>Aggregates bidirectional power flows between regions based on node-level flow data.</p> <p>Takes line-level flow data with bidirectional flows (up/down) and losses, and aggregates them to a higher regional level (e.g., countries, market areas) or keeps them at node level. Uses networkx for identifying region/node connections and handles multiple lines between the same region/node pairs.</p> <p>When agg_region_column is None, each node is treated as its own region, allowing for node-to-node trade balance analysis without aggregation.</p> <p>Example for regional aggregation:</p> <pre><code>&gt;&gt;&gt; line_model_df = pd.DataFrame({\n...     \"node_from\": [\"DE1\", \"FR1\"],\n...     \"node_to\": [\"FR1\", \"BE1\"]\n... })\n&gt;&gt;&gt; node_model_df = pd.DataFrame({\n...     \"country\": [\"DE\", \"FR\", \"BE\"]\n... }, index=[\"DE1\", \"FR1\", \"BE1\"])\n&gt;&gt;&gt; aggregator = RegionalTradeBalanceCalculator(\n...     line_model_df=line_model_df,\n...     node_model_df=node_model_df,\n...     agg_region_column=\"country\"\n... )\n</code></pre> <p>Example for node-level flows:</p> <pre><code>&gt;&gt;&gt; aggregator = RegionalTradeBalanceCalculator(\n...     line_model_df=line_model_df,\n...     node_model_df=node_model_df,\n...     agg_region_column=None  # Keep flows at node level\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/regional_trade_balance_calculator.py</code> <pre><code>class RegionalTradeBalanceCalculator:\n    \"\"\"Aggregates bidirectional power flows between regions based on node-level flow data.\n\n    Takes line-level flow data with bidirectional flows (up/down) and losses, and aggregates\n    them to a higher regional level (e.g., countries, market areas) or keeps them at node level.\n    Uses networkx for identifying region/node connections and handles multiple lines between\n    the same region/node pairs.\n\n    When agg_region_column is None, each node is treated as its own region, allowing for\n    node-to-node trade balance analysis without aggregation.\n\n    Example for regional aggregation:\n\n        &gt;&gt;&gt; line_model_df = pd.DataFrame({\n        ...     \"node_from\": [\"DE1\", \"FR1\"],\n        ...     \"node_to\": [\"FR1\", \"BE1\"]\n        ... })\n        &gt;&gt;&gt; node_model_df = pd.DataFrame({\n        ...     \"country\": [\"DE\", \"FR\", \"BE\"]\n        ... }, index=[\"DE1\", \"FR1\", \"BE1\"])\n        &gt;&gt;&gt; aggregator = RegionalTradeBalanceCalculator(\n        ...     line_model_df=line_model_df,\n        ...     node_model_df=node_model_df,\n        ...     agg_region_column=\"country\"\n        ... )\n\n    Example for node-level flows:\n\n        &gt;&gt;&gt; aggregator = RegionalTradeBalanceCalculator(\n        ...     line_model_df=line_model_df,\n        ...     node_model_df=node_model_df,\n        ...     agg_region_column=None  # Keep flows at node level\n        ... )\n    \"\"\"\n    EXP_VAR = \"exp\"\n    IMP_VAR = \"imp\"\n    NET_EXP_VAR = \"net_exp\"\n    ALL_VARS = [EXP_VAR, IMP_VAR, NET_EXP_VAR]\n\n    def __init__(\n            self,\n            line_model_df: pd.DataFrame,\n            node_model_df: pd.DataFrame,\n            agg_region_column: str | None = \"country\",\n            node_from_col: str = \"node_from\",\n            node_to_col: str = \"node_to\"\n    ):\n        self.line_model_df = line_model_df\n        self.node_model_df = node_model_df\n        self.agg_region_column = agg_region_column\n        self.node_from_col = node_from_col\n        self.node_to_col = node_to_col\n        self.node_to_agg_region_map = self._create_node_to_region_map()\n        self.agg_region_graph = self._create_region_graph()\n\n    def _create_node_to_region_map(self) -&gt; dict:\n        if self.agg_region_column is None:\n            return {node: node for node in self.node_model_df.index}\n        return self.node_model_df[self.agg_region_column].to_dict()\n\n    def _create_region_graph(self) -&gt; nx.Graph:\n        graph = nx.Graph()\n\n        for _, line in self.line_model_df.iterrows():\n            region_from = self.node_to_agg_region_map[line[self.node_from_col]]\n            region_to = self.node_to_agg_region_map[line[self.node_to_col]]\n\n            if region_from != region_to:\n                if not graph.has_edge(region_from, region_to):\n                    graph.add_edge(region_from, region_to)\n\n        return graph\n\n    def _get_net_exp_for_couple(self, primary, secondary, flow_data: NetworkLineFlowsData, flow_type: FlowType) -&gt; pd.Series:\n        mask_forward = (\n                (self.line_model_df[self.node_from_col].map(self.node_to_agg_region_map) == primary) &amp;\n                (self.line_model_df[self.node_to_col].map(self.node_to_agg_region_map) == secondary)\n        )\n        mask_backward = (\n                (self.line_model_df[self.node_from_col].map(self.node_to_agg_region_map) == secondary) &amp;\n                (self.line_model_df[self.node_to_col].map(self.node_to_agg_region_map) == primary)\n        )\n\n        lines_forward = self.line_model_df[mask_forward].index\n        lines_backward = self.line_model_df[mask_backward].index\n\n        if flow_type == FlowType.PRE_LOSS:\n            return (\n                    flow_data.sent_up[lines_forward].sum(axis=1) -\n                    flow_data.sent_down[lines_forward].sum(axis=1) +\n                    flow_data.sent_down[lines_backward].sum(axis=1) -\n                    flow_data.sent_up[lines_backward].sum(axis=1)\n            )\n        else:  # POST_LOSS\n            return (\n                    flow_data.sent_up[lines_forward].sum(axis=1) -\n                    flow_data.received_down[lines_forward].sum(axis=1) +\n                    flow_data.sent_down[lines_backward].sum(axis=1) -\n                    flow_data.received_up[lines_backward].sum(axis=1)\n            )\n\n    def get_trade_balance(\n            self,\n            flow_data: NetworkLineFlowsData,\n            flow_type: FlowType = FlowType.POST_LOSS\n    ) -&gt; pd.DataFrame:\n        flows_list = []\n        column_level_names = [self.primary_name, self.partner_name, \"variable\"]\n\n        for primary in self.get_all_regions():\n            for secondary in self.get_region_neighbors(primary):\n                net_exp = self._get_net_exp_for_couple(primary, secondary, flow_data, flow_type)\n                df = pd.concat(\n                    {\n                        (primary, secondary, self.NET_EXP_VAR): net_exp,\n                        (primary, secondary, self.EXP_VAR): net_exp.clip(0),\n                        (primary, secondary, self.IMP_VAR): net_exp.clip(None, 0).abs(),\n                    },\n                    axis=1,\n                    names=column_level_names,\n                )\n                flows_list.append(df)\n\n        if not flows_list:\n            return pd.DataFrame(\n                index=flow_data.sent_up.index,\n                columns=pd.MultiIndex.from_tuples([], names=column_level_names)\n            )\n\n        return pd.concat(flows_list, axis=1)\n\n    def get_region_neighbors(self, region: str) -&gt; set:\n        return set(self.agg_region_graph.neighbors(region))\n\n    def get_all_regions(self) -&gt; set:\n        return set(self.agg_region_graph.nodes())\n\n    @property\n    def primary_name(self) -&gt; str:\n        return f\"primary_{self.agg_region_column}\"\n\n    @property\n    def partner_name(self) -&gt; str:\n        return f\"partner_{self.agg_region_column}\"\n\n    def aggregate_trade_balance_to_primary_level(self, trade_balance_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Reduces three-level trade balance DataFrame to primary region and variable only.\"\"\"\n        if trade_balance_df.columns.nlevels != 3:\n            raise ValueError(\"Input DataFrame must have three column levels\")\n        if trade_balance_df.columns.names != [self.primary_name, self.partner_name, \"variable\"]:\n            raise ValueError(\"Input DataFrame must be in format from aggregate_flows\")\n\n        return trade_balance_df.T.groupby(level=[self.primary_name, \"variable\"]).sum().T\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/energy_data_handling/variable_utils/regional_trade_balance/#mescal.energy_data_handling.variable_utils.regional_trade_balance_calculator.RegionalTradeBalanceCalculator.aggregate_trade_balance_to_primary_level","title":"aggregate_trade_balance_to_primary_level","text":"<pre><code>aggregate_trade_balance_to_primary_level(trade_balance_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Reduces three-level trade balance DataFrame to primary region and variable only.</p> Source code in <code>submodules/mescal/mescal/energy_data_handling/variable_utils/regional_trade_balance_calculator.py</code> <pre><code>def aggregate_trade_balance_to_primary_level(self, trade_balance_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Reduces three-level trade balance DataFrame to primary region and variable only.\"\"\"\n    if trade_balance_df.columns.nlevels != 3:\n        raise ValueError(\"Input DataFrame must have three column levels\")\n    if trade_balance_df.columns.names != [self.primary_name, self.partner_name, \"variable\"]:\n        raise ValueError(\"Input DataFrame must be in format from aggregate_flows\")\n\n    return trade_balance_df.T.groupby(level=[self.primary_name, \"variable\"]).sum().T\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/","title":"MESCAL Pandas Utils","text":""},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/#mescal.utils.pandas_utils","title":"pandas_utils","text":"<p>MESCAL pandas utilities for energy systems data analysis.</p> <p>This package provides specialized pandas utilities designed for energy systems analysis workflows in the MESCAL framework. These utilities are optimized for working with MultiIndex data structures that are common in multi-scenario energy modeling studies.</p> <p>The utilities support MESCAL's core data flow patterns where energy data is typically structured with multiple dimensions (scenarios, time periods, network components, technologies) and requires specialized operations for filtering, transformation, and aggregation.</p> <p>Modules:</p> Name Description <code>prepend_model_prop_levels</code> <p>Add model properties as MultiIndex levels to time-series data</p> <code>filter_by_model_query</code> <p>Filter time-series using model metadata queries</p> <code>flatten_df</code> <p>Convert MultiIndex DataFrames to flat format for visualization</p> <code>sort_multiindex</code> <p>Sort MultiIndex levels with custom ordering</p> <code>xs_df</code> <p>Enhanced cross-section interface for MultiIndex DataFrames</p> <code>merge_multi_index_levels</code> <p>Combine multiple MultiIndex levels into single level</p> <code>add_index_as_column</code> <p>Convert index levels to DataFrame columns</p> <p>Examples:</p> <pre><code>Basic usage for energy systems analysis:\n&gt;&gt;&gt; from mescal.utils.pandas_utils import prepend_model_prop_levels, filter_by_model_query\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load generator model and time-series data\n&gt;&gt;&gt; generators = study.scen.fetch('generators')  # Model metadata\n&gt;&gt;&gt; generation = study.scen.fetch('generators_t.p')  # Time-series data\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Add technology and zone properties to time-series\n&gt;&gt;&gt; gen_with_props = prepend_model_prop_levels(\n...     generation, generators, 'technology', 'zone'\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Filter for renewable generators only\n&gt;&gt;&gt; renewable_gen = filter_by_model_query(\n...     gen_with_props, generators, 'technology.isin([\"solar\", \"wind\"])'\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Aggregate by technology and zone\n&gt;&gt;&gt; tech_zone_totals = renewable_gen.T.groupby(level=['technology', 'zone']).sum().T\n</code></pre> Architecture Integration <p>These utilities integrate seamlessly with MESCAL's three-tier architecture:</p> <ul> <li>General utilities (this package): Platform-agnostic data transformations</li> <li>Platform-specific: Used by platform interfaces (mescal-pypsa, etc.)</li> <li>Study-specific: Extended in individual studies for custom analysis</li> </ul> <p>They preserve MESCAL's MultiIndex data flow patterns while enabling flexible data manipulation and transformation for energy systems analysis workflows.</p> Performance Notes <ul> <li>Operations maintain MultiIndex structures for memory efficiency</li> <li>Bulk operations are preferred over iterative transformations</li> <li>Query-based filtering minimizes data copying</li> <li>Lazy evaluation patterns supported where possible</li> </ul>"},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/combine_df/","title":"MESCAL Pandas Util <code>combine_df</code>","text":""},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/combine_df/#mescal.utils.pandas_utils.combine_df.combine_dfs","title":"combine_dfs","text":"<pre><code>combine_dfs(dfs: Iterable[Series | DataFrame], keep_first: bool = True)\n</code></pre> <p>Combine multiple DataFrames or Series using intelligent merging strategies.</p> <p>This function automatically determines how to combine DataFrames based on their index and column structure: - If DataFrames share indices but not columns: concatenate along columns (axis=1) - If DataFrames share columns but not indices: concatenate along rows (axis=0) - Otherwise: use combine_first() to fill missing values</p> <p>Parameters:</p> Name Type Description Default <code>dfs</code> <code>Iterable[Series | DataFrame]</code> <p>An iterable of pandas DataFrames or Series to combine.</p> required <code>keep_first</code> <code>bool</code> <p>If True, prioritize values from earlier DataFrames when using combine_first(). If False, prioritize values from later DataFrames.</p> <code>True</code> <p>Returns:</p> Type Description <p>A single DataFrame or Series containing the combined data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no DataFrames are provided in the iterable.</p> Energy Domain Context <p>In Energy Systems Analysis, you often deal with fragmented data, stored or imported from different locations. For example:</p> <pre><code>- You have multiple simulation results that you want to concatenate,\n    e.g. one each model covers only one month and you\n    need to merge those into a single df\n- You have a yearly simulation result, but one week must be\n    replaced with another result, because you had to re-run\n    that with a different setting. So only that week should\n    be overwritten.\n- You have a local csv file with static properties that you\n    want to merge with the model_df that is coming from the\n    simulation platform.\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df1 = pd.DataFrame({'A': [1, 2]}, index=['x', 'y'])\n&gt;&gt;&gt; df2 = pd.DataFrame({'B': [3, 4]}, index=['x', 'y'])\n&gt;&gt;&gt; result = combine_dfs([df1, df2])  # Column concat\n&gt;&gt;&gt; print(result)\n       A  B\n    x  1  3\n    y  2  4\n\n&gt;&gt;&gt; df3 = pd.DataFrame({'A': [5, 6]}, index=['z', 'w'])\n&gt;&gt;&gt; result = combine_dfs([df1, df3])  # Row concat\n&gt;&gt;&gt; print(result)\n       A\n    x  1\n    y  2\n    z  5\n    w  6\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/pandas_utils/combine_df.py</code> <pre><code>def combine_dfs(dfs: Iterable[pd.Series | pd.DataFrame], keep_first: bool = True):\n    \"\"\"Combine multiple DataFrames or Series using intelligent merging strategies.\n\n    This function automatically determines how to combine DataFrames based on their\n    index and column structure:\n    - If DataFrames share indices but not columns: concatenate along columns (axis=1)\n    - If DataFrames share columns but not indices: concatenate along rows (axis=0)\n    - Otherwise: use combine_first() to fill missing values\n\n    Args:\n        dfs: An iterable of pandas DataFrames or Series to combine.\n        keep_first: If True, prioritize values from earlier DataFrames when using\n            combine_first(). If False, prioritize values from later DataFrames.\n\n    Returns:\n        A single DataFrame or Series containing the combined data.\n\n    Raises:\n        ValueError: If no DataFrames are provided in the iterable.\n\n    Energy Domain Context:\n        In Energy Systems Analysis, you often deal with fragmented data,\n        stored or imported from different locations. For example:\n\n            - You have multiple simulation results that you want to concatenate,\n                e.g. one each model covers only one month and you\n                need to merge those into a single df\n            - You have a yearly simulation result, but one week must be\n                replaced with another result, because you had to re-run\n                that with a different setting. So only that week should\n                be overwritten.\n            - You have a local csv file with static properties that you\n                want to merge with the model_df that is coming from the\n                simulation platform.\n\n    Examples:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df1 = pd.DataFrame({'A': [1, 2]}, index=['x', 'y'])\n        &gt;&gt;&gt; df2 = pd.DataFrame({'B': [3, 4]}, index=['x', 'y'])\n        &gt;&gt;&gt; result = combine_dfs([df1, df2])  # Column concat\n        &gt;&gt;&gt; print(result)\n               A  B\n            x  1  3\n            y  2  4\n\n        &gt;&gt;&gt; df3 = pd.DataFrame({'A': [5, 6]}, index=['z', 'w'])\n        &gt;&gt;&gt; result = combine_dfs([df1, df3])  # Row concat\n        &gt;&gt;&gt; print(result)\n               A\n            x  1\n            y  2\n            z  5\n            w  6\n    \"\"\"\n    size = sum(1 for _ in dfs)\n    if size == 0:\n        raise ValueError(\"You need to pass at least one DataFrame / Series.\")\n    if size == 1:\n        return [df for df in dfs][0]\n\n    def merge_func(df1, df2):\n        if isinstance(df1, pd.DataFrame) and isinstance(df2, pd.DataFrame):\n            column_intersection = set(df1.columns).intersection(df2.columns)\n            index_intersection = set(df1.index).intersection(df2.index)\n            if (len(column_intersection) == 0) and (len(index_intersection) &gt; 0):\n                return pd.concat([df1, df2], axis=1)\n            elif (len(column_intersection) &gt; 0) and (len(index_intersection) == 0):\n                return pd.concat([df1, df2], axis=0)\n\n        if keep_first:\n            return df1.combine_first(df2)\n        else:\n            return df2.combine_first(df1)\n\n    return reduce(merge_func, dfs)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/filter/","title":"MESCAL Pandas Util <code>filter_by_model_query</code>","text":""},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/filter/#mescal.utils.pandas_utils.filter.filter_by_model_query","title":"filter_by_model_query","text":"<pre><code>filter_by_model_query(df: Series | DataFrame, model_df: DataFrame, query: str = None, match_on_level: int | str = None) -&gt; DataFrame | Series\n</code></pre> <p>Filter DataFrame or Series based on a query applied to a model DataFrame.</p> <p>This function filters data by applying a pandas query to a model DataFrame and using the resulting index to filter the target DataFrame or Series. It handles both simple and MultiIndex cases automatically.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Series | DataFrame</code> <p>The DataFrame or Series to filter.</p> required <code>model_df</code> <code>DataFrame</code> <p>The model DataFrame containing metadata used for filtering. Must have an index that can be matched against df's axis.</p> required <code>query</code> <code>str</code> <p>A pandas query string to apply to model_df. If None or empty, returns df unchanged. Uses pandas query syntax.</p> <code>None</code> <code>match_on_level</code> <code>int | str</code> <p>For MultiIndex cases, specifies which level to match on. Can be an integer (level position) or string (level name).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | Series</code> <p>Filtered DataFrame or Series with the same type as input df.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # You have a generation time-series df\n&gt;&gt;&gt; print(gen_df)  # Original DataFrame\n    generator            GenA  GenB  GenC  SolarA  WindA\n    2024-01-01 00:00:00   100   200   150      50     80\n    2024-01-01 01:00:00   120   180   170      60     90\n    2024-01-01 02:00:00   110   190   160      55     85\n\n&gt;&gt;&gt; # You have a generator model df\n&gt;&gt;&gt; print(model_df)\n              zone technology  is_res\n    generator\n    GenA        DE    nuclear   False\n    GenB        DE       coal   False\n    GenC        FR        gas   False\n    SolarA      DE      solar    True\n    WindA       NL       wind    True\n\n&gt;&gt;&gt; only_de_conv = filter_by_model_query(gen_df, model_df, '(not is_res) and (zone == \"DE\")')\n&gt;&gt;&gt; print(only_de_conv)  # DataFrame with only non-res generators in DE\n    generator            GenA GenB\n    2024-01-01 00:00:00   100  200\n    2024-01-01 01:00:00   120  180\n    2024-01-01 02:00:00   110  190\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/pandas_utils/filter.py</code> <pre><code>def filter_by_model_query(\n        df: pd.Series | pd.DataFrame,\n        model_df: pd.DataFrame,\n        query: str = None,\n        match_on_level: int | str = None,\n) -&gt; pd.DataFrame | pd.Series:\n    \"\"\"Filter DataFrame or Series based on a query applied to a model DataFrame.\n\n    This function filters data by applying a pandas query to a model DataFrame and\n    using the resulting index to filter the target DataFrame or Series. It handles\n    both simple and MultiIndex cases automatically.\n\n    Args:\n        df: The DataFrame or Series to filter.\n        model_df: The model DataFrame containing metadata used for filtering.\n            Must have an index that can be matched against df's axis.\n        query: A pandas query string to apply to model_df. If None or empty,\n            returns df unchanged. Uses pandas query syntax.\n        match_on_level: For MultiIndex cases, specifies which level to match on.\n            Can be an integer (level position) or string (level name).\n\n    Returns:\n        Filtered DataFrame or Series with the same type as input df.\n\n    Example:\n\n        &gt;&gt;&gt; # You have a generation time-series df\n        &gt;&gt;&gt; print(gen_df)  # Original DataFrame\n            generator            GenA  GenB  GenC  SolarA  WindA\n            2024-01-01 00:00:00   100   200   150      50     80\n            2024-01-01 01:00:00   120   180   170      60     90\n            2024-01-01 02:00:00   110   190   160      55     85\n\n        &gt;&gt;&gt; # You have a generator model df\n        &gt;&gt;&gt; print(model_df)\n                      zone technology  is_res\n            generator\n            GenA        DE    nuclear   False\n            GenB        DE       coal   False\n            GenC        FR        gas   False\n            SolarA      DE      solar    True\n            WindA       NL       wind    True\n\n        &gt;&gt;&gt; only_de_conv = filter_by_model_query(gen_df, model_df, '(not is_res) and (zone == \"DE\")')\n        &gt;&gt;&gt; print(only_de_conv)  # DataFrame with only non-res generators in DE\n            generator            GenA GenB\n            2024-01-01 00:00:00   100  200\n            2024-01-01 01:00:00   120  180\n            2024-01-01 02:00:00   110  190\n    \"\"\"\n    if query is None or query == '':\n        return df\n\n    axis, idx_selection_level = get_matching_axis_and_level(df, model_df.index, match_on_level)\n    idx = df.axes[axis]\n\n    selection = model_df.query(query, engine='python').copy(deep=True).index\n    selection = list(set(selection).intersection(idx.get_level_values(idx_selection_level)))\n\n    if isinstance(idx, pd.MultiIndex):\n        selection = [i for i in idx if i[idx_selection_level] in selection]\n\n    if isinstance(df, pd.Series):\n        return df[selection] if axis == 0 else df.loc[selection]\n    else:\n        return df.loc[selection, :] if axis == 0 else df.loc[:, selection]\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/flatten_df/","title":"MESCAL Pandas Util <code>flatten_df</code>","text":""},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/flatten_df/#mescal.utils.pandas_utils.flatten_df.flatten_df","title":"flatten_df","text":"<pre><code>flatten_df(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Transform a time-series DataFrame into a flat format with one value per row.</p> <p>Converts a DataFrame with multi-level columns (objects/variables/properties) and time-based indices into a long-format DataFrame where each row contains a single value with its corresponding metadata.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame with potentially multi-level columns and indices. Typically represents time-series data with multiple variables, objects, or properties.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Flattened DataFrame in long format where:</p> <ul> <li>Each row represents one data point</li> <li>Original index levels become columns</li> <li>Original column levels become the 'variable' column</li> <li>Data values are in the 'value' column</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample multi-level DataFrame\n&gt;&gt;&gt; dt_idx = pd.date_range('2024-01-01', periods=3, freq='h', name='datetime')\n&gt;&gt;&gt; cols = pd.MultiIndex.from_product([['DE', 'FR'], ['price']], names=['zone', 'type'])\n&gt;&gt;&gt; df = pd.DataFrame(np.random.rand(3, 2), index=dt_idx, columns=cols)\n&gt;&gt;&gt; print(df.head())\n    zone                    DE            FR\n    type                 price volume  price volume\n    datetime\n    2024-01-01 00:00:00  37.45  95.07  73.20  59.87\n    2024-01-01 06:00:00  15.60  15.60   5.81  86.62\n    2024-01-01 12:00:00  60.11  70.81   2.06  96.99\n    2024-01-01 18:00:00  83.24  21.23  18.18  18.34\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Flatten the DataFrame\n&gt;&gt;&gt; flat_df = flatten_df(df)\n&gt;&gt;&gt; print(flat_df.head())\n                  datetime zone    type  value\n    0  2024-01-01 00:00:00   DE   price  37.45\n    1  2024-01-01 06:00:00   DE   price  15.60\n    2  2024-01-01 12:00:00   DE   price  60.11\n    3  2024-01-01 18:00:00   DE   price  83.24\n    4  2024-01-01 00:00:00   DE  volume  95.07\n    5  2024-01-01 06:00:00   DE  volume  15.60\n    6  2024-01-01 12:00:00   DE  volume  70.81\n    7  2024-01-01 18:00:00   DE  volume  21.23\n    8  2024-01-01 00:00:00   FR   price  73.20\n    9  2024-01-01 06:00:00   FR   price   5.81\n    10 2024-01-01 12:00:00   FR   price   2.06\n    11 2024-01-01 18:00:00   FR   price  18.18\n    12 2024-01-01 00:00:00   FR  volume  59.87\n    13 2024-01-01 06:00:00   FR  volume  86.62\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/pandas_utils/flatten_df.py</code> <pre><code>def flatten_df(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Transform a time-series DataFrame into a flat format with one value per row.\n\n    Converts a DataFrame with multi-level columns (objects/variables/properties)\n    and time-based indices into a long-format DataFrame where each row contains\n    a single value with its corresponding metadata.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame with potentially multi-level columns\n            and indices. Typically represents time-series data with multiple\n            variables, objects, or properties.\n\n    Returns:\n        pd.DataFrame: Flattened DataFrame in long format where:\n\n            - Each row represents one data point\n            - Original index levels become columns\n            - Original column levels become the 'variable' column\n            - Data values are in the 'value' column\n\n    Examples:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample multi-level DataFrame\n        &gt;&gt;&gt; dt_idx = pd.date_range('2024-01-01', periods=3, freq='h', name='datetime')\n        &gt;&gt;&gt; cols = pd.MultiIndex.from_product([['DE', 'FR'], ['price']], names=['zone', 'type'])\n        &gt;&gt;&gt; df = pd.DataFrame(np.random.rand(3, 2), index=dt_idx, columns=cols)\n        &gt;&gt;&gt; print(df.head())\n            zone                    DE            FR\n            type                 price volume  price volume\n            datetime\n            2024-01-01 00:00:00  37.45  95.07  73.20  59.87\n            2024-01-01 06:00:00  15.60  15.60   5.81  86.62\n            2024-01-01 12:00:00  60.11  70.81   2.06  96.99\n            2024-01-01 18:00:00  83.24  21.23  18.18  18.34\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Flatten the DataFrame\n        &gt;&gt;&gt; flat_df = flatten_df(df)\n        &gt;&gt;&gt; print(flat_df.head())\n                          datetime zone    type  value\n            0  2024-01-01 00:00:00   DE   price  37.45\n            1  2024-01-01 06:00:00   DE   price  15.60\n            2  2024-01-01 12:00:00   DE   price  60.11\n            3  2024-01-01 18:00:00   DE   price  83.24\n            4  2024-01-01 00:00:00   DE  volume  95.07\n            5  2024-01-01 06:00:00   DE  volume  15.60\n            6  2024-01-01 12:00:00   DE  volume  70.81\n            7  2024-01-01 18:00:00   DE  volume  21.23\n            8  2024-01-01 00:00:00   FR   price  73.20\n            9  2024-01-01 06:00:00   FR   price   5.81\n            10 2024-01-01 12:00:00   FR   price   2.06\n            11 2024-01-01 18:00:00   FR   price  18.18\n            12 2024-01-01 00:00:00   FR  volume  59.87\n            13 2024-01-01 06:00:00   FR  volume  86.62\n    \"\"\"\n\n    data = df.copy()\n    if any(i is None for i in data.columns.names):\n        if data.columns.nlevels == 1:\n            data.columns.name = 'columns'\n        else:\n            data.columns.names = [f'column_level_{i}' if name is None else name for i, name in enumerate(df.columns.names)]\n    if any(i is None for i in data.index.names):\n        if data.index.nlevels == 1:\n            data.index.name = 'index'\n        else:\n            data.index.names = [f'index_level_{i}' if name is None else name for i, name in enumerate(df.index.names)]\n\n    depth_cols = data.columns.nlevels\n    idx_names = list(data.index.names)\n    if depth_cols &gt; 1:\n        idx_cols = [(i, ) + tuple('' for _ in range(depth_cols - 1)) for i in idx_names]\n    else:\n        idx_cols = idx_names\n    data = data.reset_index().melt(id_vars=idx_cols)\n    data = data.rename(columns={tup: name for tup, name in zip(idx_cols, idx_names)})\n\n    return data\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/merge_multi_index_levels/","title":"MESCAL Pandas Util <code>merge_multi_index_levels</code>","text":""},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/merge_multi_index_levels/#mescal.utils.pandas_utils.merge_multi_index_levels.merge_multi_index_levels","title":"merge_multi_index_levels","text":"<pre><code>merge_multi_index_levels(multi_index: MultiIndex, levels: list[str], name_of_new_level: str, join_levels_by: str = ' - ', append_new_level_as_last: bool = True) -&gt; MultiIndex\n</code></pre> <p>Merge multiple levels of a MultiIndex into a single new level.</p> <p>Combines specified levels from a pandas MultiIndex by joining their values with a separator string, creating a new level while preserving other levels.</p> <p>Parameters:</p> Name Type Description Default <code>multi_index</code> <code>MultiIndex</code> <p>The MultiIndex to modify.</p> required <code>levels</code> <code>list[str]</code> <p>List of level names to merge together.</p> required <code>name_of_new_level</code> <code>str</code> <p>Name for the newly created merged level.</p> required <code>join_levels_by</code> <code>str</code> <p>String used to join the level values. Defaults to ' - '.</p> <code>' - '</code> <code>append_new_level_as_last</code> <code>bool</code> <p>If True, append new level at the end. If False, prepend at the beginning. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>MultiIndex</code> <p>A new MultiIndex with the specified levels merged into a single level.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; index = pd.MultiIndex.from_tuples([\n...     ('DE', 'solar', '2024'),\n...     ('DE', 'wind', '2024'),\n...     ('FR', 'nuclear', '2024')\n... ], names=['country', 'technology', 'year'])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Merge country and technology levels\n&gt;&gt;&gt; new_index = merge_multi_index_levels(\n...     index,\n...     ['country', 'technology'],\n...     'location_tech',\n...     join_levels_by='_'\n... )\n&gt;&gt;&gt; print(new_index.names)\n    ['year', 'location_tech']\n&gt;&gt;&gt; print(new_index.tolist())\n    [('2024', 'DE_solar'), ('2024', 'DE_wind'), ('2024', 'FR_nuclear')]\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/pandas_utils/merge_multi_index_levels.py</code> <pre><code>def merge_multi_index_levels(\n        multi_index: pd.MultiIndex,\n        levels: list[str],\n        name_of_new_level: str,\n        join_levels_by: str = ' - ',\n        append_new_level_as_last: bool = True\n) -&gt; pd.MultiIndex:\n    \"\"\"Merge multiple levels of a MultiIndex into a single new level.\n\n    Combines specified levels from a pandas MultiIndex by joining their values\n    with a separator string, creating a new level while preserving other levels.\n\n    Args:\n        multi_index: The MultiIndex to modify.\n        levels: List of level names to merge together.\n        name_of_new_level: Name for the newly created merged level.\n        join_levels_by: String used to join the level values. Defaults to ' - '.\n        append_new_level_as_last: If True, append new level at the end. If False,\n            prepend at the beginning. Defaults to True.\n\n    Returns:\n        A new MultiIndex with the specified levels merged into a single level.\n\n    Examples:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; index = pd.MultiIndex.from_tuples([\n        ...     ('DE', 'solar', '2024'),\n        ...     ('DE', 'wind', '2024'),\n        ...     ('FR', 'nuclear', '2024')\n        ... ], names=['country', 'technology', 'year'])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Merge country and technology levels\n        &gt;&gt;&gt; new_index = merge_multi_index_levels(\n        ...     index,\n        ...     ['country', 'technology'],\n        ...     'location_tech',\n        ...     join_levels_by='_'\n        ... )\n        &gt;&gt;&gt; print(new_index.names)\n            ['year', 'location_tech']\n        &gt;&gt;&gt; print(new_index.tolist())\n            [('2024', 'DE_solar'), ('2024', 'DE_wind'), ('2024', 'FR_nuclear')]\n    \"\"\"\n    df_index = multi_index.to_frame()\n    merged_level = df_index[levels].astype(str).agg(join_levels_by.join, axis=1)\n    remaining_levels = [level for level in multi_index.names if level not in levels]\n\n    if append_new_level_as_last:\n        new_level_order = remaining_levels + [name_of_new_level]\n    else:\n        new_level_order = [name_of_new_level] + remaining_levels\n\n    df_index = df_index[remaining_levels]\n    df_index[name_of_new_level] = merged_level\n\n    return pd.MultiIndex.from_frame(df_index[new_level_order])\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/pend_props/","title":"MESCAL Pandas Util <code>prepend_model_prop_levels</code>","text":""},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/pend_props/#mescal.utils.pandas_utils.pend_props.prepend_model_prop_levels","title":"prepend_model_prop_levels","text":"<pre><code>prepend_model_prop_levels(data: Series | DataFrame, model: DataFrame, *properties, prepend_to_top: bool = True, match_on_level: str = None) -&gt; Series | DataFrame\n</code></pre> <p>Prepend model properties as new index levels to data.</p> <p>Searches for an index level in data that matches the model's index, then prepends specified properties from the model as new index levels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series | DataFrame</code> <p>The pandas object to add properties to.</p> required <code>model</code> <code>DataFrame</code> <p>DataFrame containing properties to prepend, with matching index.</p> required <code>*properties</code> <p>Column names from model to use as new index levels.</p> <code>()</code> <code>prepend_to_top</code> <code>bool</code> <p>If True, add properties at the beginning of index levels. If False, add at the end.</p> <code>True</code> <code>match_on_level</code> <code>str</code> <p>Optional level name to constrain matching to specific level. Useful in case the there are multiple index levels in data that match the model's index</p> <code>None</code> <p>Returns:</p> Type Description <code>Series | DataFrame</code> <p>Copy of data with properties prepended as new index levels.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any property is not found in model columns.</p> Energy Domain Context <p>In Energy Systems Analysis, you often have to groupby and aggregate by certain properties. This module makes it easy to include the properties as a new index level before performing the groupby - agg pipeline.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # You have a generation time-series df\n&gt;&gt;&gt; print(gen_df)  # Original DataFrame\n    generator            GenA  GenB  GenC  SolarA  WindA\n    2024-01-01 00:00:00   100   200   150      50     80\n    2024-01-01 01:00:00   120   180   170      60     90\n    2024-01-01 02:00:00   110   190   160      55     85\n\n&gt;&gt;&gt; # You have a generator model df\n&gt;&gt;&gt; print(model_df)\n              zone technology  is_res\n    generator\n    GenA        DE    nuclear   False\n    GenB        DE       coal   False\n    GenC        FR        gas   False\n    SolarA      DE      solar    True\n    WindA       NL       wind    True\n\n&gt;&gt;&gt; gen_with_props = prepend_model_prop_levels(gen_df, model_df, 'zone', 'is_res')\n&gt;&gt;&gt; print(gen_with_props)  # DataFrame with prepended properties\n    is_res              False            True\n    zone                   DE        FR     DE    NL\n    generator            GenA GenB GenC SolarA WindA\n    2024-01-01 00:00:00   100  200  150     50    80\n    2024-01-01 01:00:00   120  180  170     60    90\n    2024-01-01 02:00:00   110  190  160     55    85\n\n&gt;&gt;&gt; gen_by_zone_and_type = gen_with_props.T.groupby(level=['zone', 'is_res']).sum().T\n&gt;&gt;&gt; print(gen_by_zone_and_type)  # grouped and aggregated\n    zone                   DE          FR    NL\n    is_res              False True  False True\n    2024-01-01 00:00:00   300    50   150    80\n    2024-01-01 01:00:00   300    60   170    90\n    2024-01-01 02:00:00   300    55   160    85\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/pandas_utils/pend_props.py</code> <pre><code>def prepend_model_prop_levels(\n        data: pd.Series | pd.DataFrame,\n        model: pd.DataFrame,\n        *properties,\n        prepend_to_top: bool = True,\n        match_on_level: str = None,\n) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"Prepend model properties as new index levels to data.\n\n    Searches for an index level in data that matches the model's index, then\n    prepends specified properties from the model as new index levels.\n\n    Args:\n        data: The pandas object to add properties to.\n        model: DataFrame containing properties to prepend, with matching index.\n        *properties: Column names from model to use as new index levels.\n        prepend_to_top: If True, add properties at the beginning of index levels.\n            If False, add at the end.\n        match_on_level: Optional level name to constrain matching to specific level.\n            Useful in case the there are multiple index levels in data that match\n            the model's index\n\n    Returns:\n        Copy of data with properties prepended as new index levels.\n\n    Raises:\n        ValueError: If any property is not found in model columns.\n\n    Energy Domain Context:\n        In Energy Systems Analysis, you often have to groupby and aggregate\n        by certain properties. This module makes it easy to include the properties\n        as a new index level before performing the groupby - agg pipeline.\n\n    Example:\n\n        &gt;&gt;&gt; # You have a generation time-series df\n        &gt;&gt;&gt; print(gen_df)  # Original DataFrame\n            generator            GenA  GenB  GenC  SolarA  WindA\n            2024-01-01 00:00:00   100   200   150      50     80\n            2024-01-01 01:00:00   120   180   170      60     90\n            2024-01-01 02:00:00   110   190   160      55     85\n\n        &gt;&gt;&gt; # You have a generator model df\n        &gt;&gt;&gt; print(model_df)\n                      zone technology  is_res\n            generator\n            GenA        DE    nuclear   False\n            GenB        DE       coal   False\n            GenC        FR        gas   False\n            SolarA      DE      solar    True\n            WindA       NL       wind    True\n\n        &gt;&gt;&gt; gen_with_props = prepend_model_prop_levels(gen_df, model_df, 'zone', 'is_res')\n        &gt;&gt;&gt; print(gen_with_props)  # DataFrame with prepended properties\n            is_res              False            True\n            zone                   DE        FR     DE    NL\n            generator            GenA GenB GenC SolarA WindA\n            2024-01-01 00:00:00   100  200  150     50    80\n            2024-01-01 01:00:00   120  180  170     60    90\n            2024-01-01 02:00:00   110  190  160     55    85\n\n        &gt;&gt;&gt; gen_by_zone_and_type = gen_with_props.T.groupby(level=['zone', 'is_res']).sum().T\n        &gt;&gt;&gt; print(gen_by_zone_and_type)  # grouped and aggregated\n            zone                   DE          FR    NL\n            is_res              False True  False True\n            2024-01-01 00:00:00   300    50   150    80\n            2024-01-01 01:00:00   300    60   170    90\n            2024-01-01 02:00:00   300    55   160    85\n    \"\"\"\n    tmp = data.copy()\n    properties = [p for p in properties if not ((p is None) or (p == ''))]\n\n    if not properties:\n        return tmp\n\n    for prop in properties:\n        if prop not in model.columns.tolist():\n            raise ValueError(f'Property unavailable: {prop} was not found in your model_df.')\n    axis, level = get_matching_axis_and_level(data, model.index, match_on_level)\n\n    match_keys = tmp.axes[axis].get_level_values(level)\n    new_index = tmp.axes[axis].to_frame(index=False)\n    for prop in properties:\n        if prop not in new_index:\n            loc = 0 if prepend_to_top else len(new_index.columns)\n            new_index.insert(loc, prop, model.loc[match_keys, prop].values)\n    new_index = pd.MultiIndex.from_frame(new_index)\n    if axis == 0:\n        tmp.index = new_index\n    else:\n        tmp.columns = new_index\n\n    if is_series(data):\n        tmp: pd.Series = tmp\n        return tmp\n    elif is_dataframe(data):\n        tmp: pd.DataFrame = tmp\n        return tmp\n    else:\n        return tmp\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/set_new_column/","title":"MESCAL Pandas Util <code>set_new_column</code>","text":""},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/set_new_column/#mescal.utils.pandas_utils.set_new_column.set_column","title":"set_column","text":"<pre><code>set_column(df: DataFrame, new_column_name: Hashable, new_column_values: Series | DataFrame) -&gt; DataFrame\n</code></pre> <p>Set or replace a column in a DataFrame with new values.</p> <p>Adds a new column or replaces an existing column in a DataFrame. Handles both Series and DataFrame inputs, with special logic for MultiIndex columns when using DataFrame inputs.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to modify.</p> required <code>new_column_name</code> <code>Hashable</code> <p>Name/key for the new column.</p> required <code>new_column_values</code> <code>Series | DataFrame</code> <p>Values for the new column. Can be a Series for simple columns or a DataFrame for MultiIndex column structures.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A copy of the DataFrame with the new column added or existing column replaced.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If length of df and new_column_values don't match, or if new_column_values DataFrame has incorrect number of column levels.</p> <code>TypeError</code> <p>If new_column_values is neither Series nor DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Add Series as new column\n&gt;&gt;&gt; new_series = pd.Series([7, 8, 9])\n&gt;&gt;&gt; result = set_column(df, 'C', new_series)\n&gt;&gt;&gt; print(result.columns.tolist())\n    ['A', 'B', 'C']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Replace existing column\n&gt;&gt;&gt; replacement = pd.Series([10, 11, 12])\n&gt;&gt;&gt; result = set_column(df, 'A', replacement)\n&gt;&gt;&gt; print(result['A'].tolist())\n    [10, 11, 12]\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/pandas_utils/set_new_column.py</code> <pre><code>def set_column(\n        df: pd.DataFrame,\n        new_column_name: Hashable,\n        new_column_values: pd.Series | pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Set or replace a column in a DataFrame with new values.\n\n    Adds a new column or replaces an existing column in a DataFrame. Handles both\n    Series and DataFrame inputs, with special logic for MultiIndex columns when\n    using DataFrame inputs.\n\n    Args:\n        df: The DataFrame to modify.\n        new_column_name: Name/key for the new column.\n        new_column_values: Values for the new column. Can be a Series for simple\n            columns or a DataFrame for MultiIndex column structures.\n\n    Returns:\n        A copy of the DataFrame with the new column added or existing column replaced.\n\n    Raises:\n        ValueError: If length of df and new_column_values don't match, or if\n            new_column_values DataFrame has incorrect number of column levels.\n        TypeError: If new_column_values is neither Series nor DataFrame.\n\n    Examples:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Add Series as new column\n        &gt;&gt;&gt; new_series = pd.Series([7, 8, 9])\n        &gt;&gt;&gt; result = set_column(df, 'C', new_series)\n        &gt;&gt;&gt; print(result.columns.tolist())\n            ['A', 'B', 'C']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Replace existing column\n        &gt;&gt;&gt; replacement = pd.Series([10, 11, 12])\n        &gt;&gt;&gt; result = set_column(df, 'A', replacement)\n        &gt;&gt;&gt; print(result['A'].tolist())\n            [10, 11, 12]\n    \"\"\"\n\n    dff = df.copy()\n\n    if not len(dff) == len(new_column_values):\n        raise ValueError('Length of dff and new_column_values must be equal.')\n\n    # TODO optional: check index\n\n    if isinstance(new_column_values, pd.Series):\n        dff[new_column_name] = new_column_values\n        return dff\n\n    if isinstance(new_column_values, pd.DataFrame):\n        if not new_column_values.columns.nlevels == (dff.columns.nlevels - 1):\n            raise ValueError(\n                'Your new_column_values must have n-1 column levels, where n is the number of levels in dff.'\n            )\n\n        if new_column_name in dff.columns:\n            dff = dff.drop(columns=[new_column_name])\n\n        new_column_values = pd.concat({new_column_name: new_column_values}, axis=1, names=[dff.columns.names[0]])\n        dff = pd.concat([dff, new_column_values], axis=1)\n        return dff\n\n    else:\n        raise TypeError('Used new_column_values type not accepted.')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/sort_multiindex/","title":"MESCAL Pandas Util <code>sort_multiindex</code>","text":""},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/sort_multiindex/#mescal.utils.pandas_utils.sort_multiindex.sort_multiindex","title":"sort_multiindex","text":"<pre><code>sort_multiindex(df: DataFrame, custom_order: list[str | int], level: str | int, axis: int = 0) -&gt; DataFrame\n</code></pre> <p>Sort a DataFrame's MultiIndex at a specific level using a custom order.</p> <p>Reorders the specified level according to custom_order while preserving the existing order of all other levels. This allows for sequential sorting operations where each sort maintains previous orderings.</p> <p>Values in the target level that are not included in custom_order will be appended at the end, maintaining their original relative order.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame with MultiIndex to sort.</p> required <code>custom_order</code> <code>list[str | int]</code> <p>List of values defining the desired order for the specified level.</p> required <code>level</code> <code>str | int</code> <p>Level to sort. Can be level name (str) or level number (int).</p> required <code>axis</code> <code>int</code> <p>Axis to sort along. 0 for index (rows), 1 for columns.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with reordered MultiIndex according to the custom order.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If axis is not 0 or 1, or if level is not a valid string or integer.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; idx = pd.MultiIndex.from_arrays([['A', 'A', 'B', 'B'], [1, 2, 1, 2]])\n&gt;&gt;&gt; df = pd.DataFrame({'val': [10, 20, 30, 40]}, index=idx)\n&gt;&gt;&gt; sort_multiindex(df, [2, 1], level=1)  # Sort second level\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/pandas_utils/sort_multiindex.py</code> <pre><code>def sort_multiindex(df: pd.DataFrame, custom_order: list[str | int], level: str | int, axis: int = 0) -&gt; pd.DataFrame:\n    \"\"\"Sort a DataFrame's MultiIndex at a specific level using a custom order.\n\n    Reorders the specified level according to custom_order while preserving\n    the existing order of all other levels. This allows for sequential sorting\n    operations where each sort maintains previous orderings.\n\n    Values in the target level that are not included in custom_order will be\n    appended at the end, maintaining their original relative order.\n\n    Args:\n        df: The DataFrame with MultiIndex to sort.\n        custom_order: List of values defining the desired order for the specified level.\n        level: Level to sort. Can be level name (str) or level number (int).\n        axis: Axis to sort along. 0 for index (rows), 1 for columns.\n\n    Returns:\n        DataFrame with reordered MultiIndex according to the custom order.\n\n    Raises:\n        ValueError: If axis is not 0 or 1, or if level is not a valid string or integer.\n\n    Example:\n\n        &gt;&gt;&gt; idx = pd.MultiIndex.from_arrays([['A', 'A', 'B', 'B'], [1, 2, 1, 2]])\n        &gt;&gt;&gt; df = pd.DataFrame({'val': [10, 20, 30, 40]}, index=idx)\n        &gt;&gt;&gt; sort_multiindex(df, [2, 1], level=1)  # Sort second level\n    \"\"\"\n    if axis not in [0, 1]:\n        raise ValueError(\"axis must be 0 (rows) or 1 (columns)\")\n\n    idx = df.axes[axis]\n    if isinstance(idx, pd.MultiIndex):\n        if isinstance(level, str):\n            level_num = idx.names.index(level)\n        elif isinstance(level, int):\n            level_num = level\n        else:\n            raise ValueError(\"level must be a string (level name) or an integer (level number)\")\n\n        idx_tuples = idx.to_list()\n        remaining_values_in_level_to_sort = [i for i in idx.get_level_values(level_num).unique() if\n                                             i not in custom_order]\n        ordered_tuples = []\n        for value in custom_order + remaining_values_in_level_to_sort:\n            for tuple_item in idx_tuples:\n                if tuple_item[level_num] == value:\n                    ordered_tuples.append(tuple_item)\n        new_index = pd.MultiIndex.from_tuples(ordered_tuples, names=idx.names)\n    else:\n        idx_values = idx.to_list()\n        remaining_values = [i for i in idx.unique() if i not in custom_order]\n        ordered_values = []\n        for value in custom_order + remaining_values:\n            for idx_value in idx_values:\n                if idx_value == value:\n                    ordered_values.append(idx_value)\n        new_index = pd.Index(ordered_values, name=idx.name)\n\n    if axis == 0:\n        return df.reindex(new_index)\n    else:\n        return df.reindex(columns=new_index)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/xs_df/","title":"MESCAL Pandas Util <code>xs_df</code>","text":""},{"location":"mescal-package-documentation/api_reference/utils/pandas_utils/xs_df/#mescal.utils.pandas_utils.xs_df.xs_df","title":"xs_df","text":"<pre><code>xs_df(df: DataFrame, keys: Hashable | list[Hashable], axis: Axis = 0, level: Hashable = None) -&gt; DataFrame\n</code></pre> <p>Extract cross-section from MultiIndex DataFrame with support for multiple keys.</p> <p>This function provides a flexible interface to pandas .xs() method with enhanced functionality for MESCAL's MultiIndex data structures. It supports both single and multiple key selection, making it particularly useful for energy systems analysis where data often has complex hierarchical structures.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame with MultiIndex (either on index or columns).</p> required <code>keys</code> <code>Hashable | list[Hashable]</code> <p>Single key or list of keys to select from the specified level. For single keys, uses pandas .xs() method with drop_level=True. For multiple keys, uses .isin() for efficient selection.</p> required <code>axis</code> <code>Axis</code> <p>Axis to operate on. Can be 0/'index'/'rows' for index operations or 1/'columns' for column operations. Defaults to 0.</p> <code>0</code> <code>level</code> <code>Hashable</code> <p>Name or position of the MultiIndex level to select from. Must be specified for MultiIndex operations.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with cross-section data. For single keys, the specified level</p> <code>DataFrame</code> <p>is dropped. For multiple keys, the level is preserved.</p> <p>Examples:</p> <p>Single dataset selection from MESCAL multi-scenario data:</p> <pre><code>&gt;&gt;&gt; multi_scenario_prices = study.scen.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; base_prices = xs_df(multi_scenario_prices, 'base', level='dataset')\n</code></pre> <p>Multiple scenario selection:</p> <pre><code>&gt;&gt;&gt; scenarios = ['base', 'high_renewable', 'low_cost']\n&gt;&gt;&gt; selected_data = xs_df(multi_scenario_prices, scenarios, level='dataset')\n</code></pre> <p>Column-wise selection for specific buses:</p> <pre><code>&gt;&gt;&gt; bus_names = ['Bus_1', 'Bus_2', 'Bus_3']\n&gt;&gt;&gt; selected_buses = xs_df(price_data, bus_names, axis='columns', level='Bus')\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/pandas_utils/xs_df.py</code> <pre><code>def xs_df(\n        df: pd.DataFrame,\n        keys: Hashable | list[Hashable],\n        axis: Axis = 0,\n        level: Hashable = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Extract cross-section from MultiIndex DataFrame with support for multiple keys.\n\n    This function provides a flexible interface to pandas .xs() method with enhanced\n    functionality for MESCAL's MultiIndex data structures. It supports both single\n    and multiple key selection, making it particularly useful for energy systems\n    analysis where data often has complex hierarchical structures.\n\n    Args:\n        df: Input DataFrame with MultiIndex (either on index or columns).\n        keys: Single key or list of keys to select from the specified level.\n            For single keys, uses pandas .xs() method with drop_level=True.\n            For multiple keys, uses .isin() for efficient selection.\n        axis: Axis to operate on. Can be 0/'index'/'rows' for index operations\n            or 1/'columns' for column operations. Defaults to 0.\n        level: Name or position of the MultiIndex level to select from.\n            Must be specified for MultiIndex operations.\n\n    Returns:\n        DataFrame with cross-section data. For single keys, the specified level\n        is dropped. For multiple keys, the level is preserved.\n\n    Examples:\n        Single dataset selection from MESCAL multi-scenario data:\n        &gt;&gt;&gt; multi_scenario_prices = study.scen.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; base_prices = xs_df(multi_scenario_prices, 'base', level='dataset')\n\n        Multiple scenario selection:\n        &gt;&gt;&gt; scenarios = ['base', 'high_renewable', 'low_cost']\n        &gt;&gt;&gt; selected_data = xs_df(multi_scenario_prices, scenarios, level='dataset')\n\n        Column-wise selection for specific buses:\n        &gt;&gt;&gt; bus_names = ['Bus_1', 'Bus_2', 'Bus_3']\n        &gt;&gt;&gt; selected_buses = xs_df(price_data, bus_names, axis='columns', level='Bus')\n    \"\"\"\n    if isinstance(keys, list):\n        if axis in [0, 'index', 'rows']:\n            return df.iloc[df.index.get_level_values(level).isin(keys)]\n        return df.iloc[:, df.columns.get_level_values(level).isin(keys)]\n    return df.xs(keys, level=level, axis=axis, drop_level=True)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/","title":"MESCAL Plotly Utils","text":""},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/#mescal.utils.plotly_utils","title":"plotly_utils","text":"<p>Plotly utilities for enhanced figure styling and subplot manipulation.</p> <p>This package provides utilities for working with Plotly figures in the MESCAL framework, including:</p> <ul> <li>plotly_theme: Custom themes and color palettes (template) for consistent visualization styling</li> <li>figure_utils: Common figure modifications like titles, annotations, and axis controls</li> <li>px_category_utils: Tools for working with categorical data in faceted Plotly Express plots</li> </ul> <p>The utilities enable precise control over figure appearance and facilitate adding elements to specific subplots in complex multi-panel visualizations.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from mescal.utils.plotly_utils import PlotlyTheme, figure_utils\n&gt;&gt;&gt; from mescal.utils.plotly_utils.plotly_theme import colors\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply consistent theming\n&gt;&gt;&gt; theme = PlotlyTheme(default_colorway=colors.qualitative.default)\n&gt;&gt;&gt; theme.apply()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Style figures\n&gt;&gt;&gt; figure_utils.set_title(fig, \"Energy Analysis Dashboard\")\n&gt;&gt;&gt; figure_utils.make_annotations_bold(fig)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/figure_utils/","title":"MESCAL Plotly Util <code>figure_utils</code>","text":""},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mescal.utils.plotly_utils.figure_utils","title":"figure_utils","text":"<p>Utility functions for styling and modifying Plotly figures.</p> <p>This module provides convenience functions for common figure modifications such as title formatting, annotation styling, axis configuration, and adding interactive controls to Plotly figures.</p>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mescal.utils.plotly_utils.figure_utils.set_title","title":"set_title","text":"<pre><code>set_title(fig: Figure, title: str)\n</code></pre> <p>Set a centered, bold title for the figure.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to modify.</p> required <code>title</code> <code>str</code> <p>Title text to display.</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; set_title(fig, \"Sales Performance Dashboard\")\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/figure_utils.py</code> <pre><code>def set_title(fig: go.Figure, title: str):\n    \"\"\"Set a centered, bold title for the figure.\n\n    Args:\n        fig: Plotly figure object to modify.\n        title: Title text to display.\n\n    Example:\n\n        &gt;&gt;&gt; set_title(fig, \"Sales Performance Dashboard\")\n    \"\"\"\n    title = f'&lt;b&gt;{title}&lt;/b&gt;'\n    fig.update_layout(title_text=title, title_x=0.5)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mescal.utils.plotly_utils.figure_utils.remove_category_in_annotations","title":"remove_category_in_annotations","text":"<pre><code>remove_category_in_annotations(fig: Figure)\n</code></pre> <p>Remove category names from subplot annotations, keeping only values.</p> <p>Modifies annotation text to show only the part after '=' for cleaner subplot labels (e.g., 'sex=Male' becomes 'Male').</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object with annotations to modify.</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; remove_category_in_annotations(fig)  # 'smoker=Yes' \u2192 'Yes'\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/figure_utils.py</code> <pre><code>def remove_category_in_annotations(fig: go.Figure):\n    \"\"\"Remove category names from subplot annotations, keeping only values.\n\n    Modifies annotation text to show only the part after '=' for cleaner\n    subplot labels (e.g., 'sex=Male' becomes 'Male').\n\n    Args:\n        fig: Plotly figure object with annotations to modify.\n\n    Example:\n\n        &gt;&gt;&gt; remove_category_in_annotations(fig)  # 'smoker=Yes' \u2192 'Yes'\n    \"\"\"\n    fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mescal.utils.plotly_utils.figure_utils.make_annotations_bold","title":"make_annotations_bold","text":"<pre><code>make_annotations_bold(fig: Figure)\n</code></pre> <p>Apply bold formatting to all figure annotations.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object with annotations to modify.</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; make_annotations_bold(fig)  # Makes all subplot labels bold\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/figure_utils.py</code> <pre><code>def make_annotations_bold(fig: go.Figure):\n    \"\"\"Apply bold formatting to all figure annotations.\n\n    Args:\n        fig: Plotly figure object with annotations to modify.\n\n    Example:\n\n        &gt;&gt;&gt; make_annotations_bold(fig)  # Makes all subplot labels bold\n    \"\"\"\n    fig.for_each_annotation(lambda a: a.update(text='&lt;b&gt;' + a.text + '&lt;/b&gt;'))\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mescal.utils.plotly_utils.figure_utils.unmatch_xaxes","title":"unmatch_xaxes","text":"<pre><code>unmatch_xaxes(fig: Figure)\n</code></pre> <p>Remove x-axis matching across subplots.</p> <p>Allows each subplot to have independent x-axis ranges and scaling.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to modify.</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; unmatch_xaxes(fig)  # Each subplot can have different x-ranges\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/figure_utils.py</code> <pre><code>def unmatch_xaxes(fig: go.Figure):\n    \"\"\"Remove x-axis matching across subplots.\n\n    Allows each subplot to have independent x-axis ranges and scaling.\n\n    Args:\n        fig: Plotly figure object to modify.\n\n    Example:\n\n        &gt;&gt;&gt; unmatch_xaxes(fig)  # Each subplot can have different x-ranges\n    \"\"\"\n    fig.update_xaxes(matches=None)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mescal.utils.plotly_utils.figure_utils.unmatch_yaxes","title":"unmatch_yaxes","text":"<pre><code>unmatch_yaxes(fig: Figure)\n</code></pre> <p>Remove y-axis matching across subplots.</p> <p>Allows each subplot to have independent y-axis ranges and scaling.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to modify.</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; unmatch_yaxes(fig)  # Each subplot can have different y-ranges\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/figure_utils.py</code> <pre><code>def unmatch_yaxes(fig: go.Figure):\n    \"\"\"Remove y-axis matching across subplots.\n\n    Allows each subplot to have independent y-axis ranges and scaling.\n\n    Args:\n        fig: Plotly figure object to modify.\n\n    Example:\n\n        &gt;&gt;&gt; unmatch_yaxes(fig)  # Each subplot can have different y-ranges\n    \"\"\"\n    fig.update_yaxes(matches=None)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mescal.utils.plotly_utils.figure_utils.reverse_legend_traceorder","title":"reverse_legend_traceorder","text":"<pre><code>reverse_legend_traceorder(fig: Figure)\n</code></pre> <p>Reverse the order of legend entries.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to modify.</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; reverse_legend_traceorder(fig)  # Last trace appears first in legend\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/figure_utils.py</code> <pre><code>def reverse_legend_traceorder(fig: go.Figure):\n    \"\"\"Reverse the order of legend entries.\n\n    Args:\n        fig: Plotly figure object to modify.\n\n    Example:\n\n        &gt;&gt;&gt; reverse_legend_traceorder(fig)  # Last trace appears first in legend\n    \"\"\"\n    fig.update_layout(legend_traceorder=\"reversed\")\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mescal.utils.plotly_utils.figure_utils.add_datetime_rangeslider","title":"add_datetime_rangeslider","text":"<pre><code>add_datetime_rangeslider(fig: Figure)\n</code></pre> <p>Add an interactive datetime range slider and selector to the figure.</p> <p>Adds a range slider below the plot and time period selector buttons for easy navigation of time series data.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to modify.</p> required <p>Returns:</p> Type Description <p>Modified figure object with range controls.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; fig = add_datetime_rangeslider(fig)\n&gt;&gt;&gt; fig.show()  # Now includes 1d, 1w, 1m, 6m, YTD, 1y buttons\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/figure_utils.py</code> <pre><code>def add_datetime_rangeslider(fig: go.Figure):\n    \"\"\"Add an interactive datetime range slider and selector to the figure.\n\n    Adds a range slider below the plot and time period selector buttons\n    for easy navigation of time series data.\n\n    Args:\n        fig: Plotly figure object to modify.\n\n    Returns:\n        Modified figure object with range controls.\n\n    Example:\n\n        &gt;&gt;&gt; fig = add_datetime_rangeslider(fig)\n        &gt;&gt;&gt; fig.show()  # Now includes 1d, 1w, 1m, 6m, YTD, 1y buttons\n    \"\"\"\n    fig.update_xaxes(\n        rangeslider_visible=True,\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label=\"1d\", step=\"day\", stepmode=\"backward\"),\n                dict(count=7, label=\"1w\", step=\"day\", stepmode=\"backward\"),\n                dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n                dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n                dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n                dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n                dict(step=\"all\")\n            ])\n        )\n    )\n    fig.update_layout(yaxis_fixedrange=False)\n    return fig\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/","title":"MESCAL Plotly Util <code>PlotlyTheme</code>","text":""},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme","title":"plotly_theme","text":"<p>Custom Plotly theme and color palette definitions for consistent visualization styling.</p> <p>This module provides a comprehensive theming system for Plotly figures, including predefined color palettes, theme configuration, and template application. It enables consistent styling across all visualizations in the MESCAL framework.</p>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.colors","title":"colors  <code>module-attribute</code>","text":"<pre><code>colors = ColorPalette\n</code></pre> <p>Global color palette instance for convenient access to all color schemes.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; colors.primary.blue\n'#0984e3'\n&gt;&gt;&gt; colors.sequential.default\n['#00b894', '#0984e3', '#d63031']\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.ConstantsIterable","title":"ConstantsIterable","text":"<p>Base class for creating iterable constant collections.</p> <p>Provides dictionary-like interface methods (items, values, keys) for accessing class attributes as constants.</p> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/plotly_theme.py</code> <pre><code>class ConstantsIterable:\n    \"\"\"Base class for creating iterable constant collections.\n\n    Provides dictionary-like interface methods (items, values, keys) for\n    accessing class attributes as constants.\n    \"\"\"\n    @classmethod\n    def items(cls):\n        \"\"\"Yield (name, value) pairs for all non-private, non-callable attributes.\n\n        Yields:\n            Tuple of (attribute_name, attribute_value) for class constants.\n        \"\"\"\n        for attr_name in dir(cls):\n            if not attr_name.startswith('__') and not callable(getattr(cls, attr_name)):\n                yield attr_name, getattr(cls, attr_name)\n\n    @classmethod\n    def values(cls):\n        \"\"\"Yield values for all non-private, non-callable attributes.\n\n        Yields:\n            Attribute values for class constants.\n        \"\"\"\n        for attr_name in dir(cls):\n            if not attr_name.startswith('__') and not callable(getattr(cls, attr_name)):\n                yield getattr(cls, attr_name)\n\n    @classmethod\n    def keys(cls):\n        \"\"\"Yield names for all non-private, non-callable attributes.\n\n        Yields:\n            Attribute names for class constants.\n        \"\"\"\n        for attr_name in dir(cls):\n            if not attr_name.startswith('__') and not callable(getattr(cls, attr_name)):\n                yield attr_name\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.ConstantsIterable.items","title":"items  <code>classmethod</code>","text":"<pre><code>items()\n</code></pre> <p>Yield (name, value) pairs for all non-private, non-callable attributes.</p> <p>Yields:</p> Type Description <p>Tuple of (attribute_name, attribute_value) for class constants.</p> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/plotly_theme.py</code> <pre><code>@classmethod\ndef items(cls):\n    \"\"\"Yield (name, value) pairs for all non-private, non-callable attributes.\n\n    Yields:\n        Tuple of (attribute_name, attribute_value) for class constants.\n    \"\"\"\n    for attr_name in dir(cls):\n        if not attr_name.startswith('__') and not callable(getattr(cls, attr_name)):\n            yield attr_name, getattr(cls, attr_name)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.ConstantsIterable.values","title":"values  <code>classmethod</code>","text":"<pre><code>values()\n</code></pre> <p>Yield values for all non-private, non-callable attributes.</p> <p>Yields:</p> Type Description <p>Attribute values for class constants.</p> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/plotly_theme.py</code> <pre><code>@classmethod\ndef values(cls):\n    \"\"\"Yield values for all non-private, non-callable attributes.\n\n    Yields:\n        Attribute values for class constants.\n    \"\"\"\n    for attr_name in dir(cls):\n        if not attr_name.startswith('__') and not callable(getattr(cls, attr_name)):\n            yield getattr(cls, attr_name)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.ConstantsIterable.keys","title":"keys  <code>classmethod</code>","text":"<pre><code>keys()\n</code></pre> <p>Yield names for all non-private, non-callable attributes.</p> <p>Yields:</p> Type Description <p>Attribute names for class constants.</p> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/plotly_theme.py</code> <pre><code>@classmethod\ndef keys(cls):\n    \"\"\"Yield names for all non-private, non-callable attributes.\n\n    Yields:\n        Attribute names for class constants.\n    \"\"\"\n    for attr_name in dir(cls):\n        if not attr_name.startswith('__') and not callable(getattr(cls, attr_name)):\n            yield attr_name\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.PrimaryColors","title":"PrimaryColors","text":"<p>               Bases: <code>ConstantsIterable</code></p> <p>Primary color palette with vibrant, distinct colors.</p> <p>Provides a curated set of primary colors suitable for categorical data visualization and general plotting needs.</p> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/plotly_theme.py</code> <pre><code>class PrimaryColors(ConstantsIterable):\n    \"\"\"Primary color palette with vibrant, distinct colors.\n\n    Provides a curated set of primary colors suitable for categorical data\n    visualization and general plotting needs.\n    \"\"\"\n    mint = '#00b894'\n    cyan = '#00cec9'\n    blue = '#0984e3'\n    red = '#d63031'\n    pink = '#e84393'\n    green_light = '#badc58'\n    green_bold = '#6ab04c'\n    orange_light = '#fdcb6e'\n    orange_bold = '#e17055'\n    purple_light = '#a29bfe'\n    purple_bold = '#6c5ce7'\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.SequentialColors","title":"SequentialColors","text":"<p>               Bases: <code>ConstantsIterable</code></p> <p>Sequential color palettes for ordered data visualization.</p> <p>Contains multi-hue and single-hue sequential palettes appropriate for displaying ordered data such as numerical ranges or intensity maps.</p> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/plotly_theme.py</code> <pre><code>class SequentialColors(ConstantsIterable):\n    \"\"\"Sequential color palettes for ordered data visualization.\n\n    Contains multi-hue and single-hue sequential palettes appropriate for\n    displaying ordered data such as numerical ranges or intensity maps.\n    \"\"\"\n    mint_blue_red = ['#00b894', '#0984e3', '#d63031']\n    blue_cyan_pink = ['#0984e3', '#00cec9', '#e84393']\n    shades_of_mint = ['#e6fff7', '#55efc4', '#00b894', '#009677', '#006b54']\n    shades_of_cyan = ['#e6ffff', '#8ee8e7', '#00cec9', '#00a29a', '#00756e']\n    shades_of_blue = ['#e6f4ff', '#74b9ff', '#0984e3', '#0063b1', '#004680']\n    shades_of_red = ['#ffe6e6', '#ff7675', '#d63031', '#b02525', '#801b1b']\n    shades_of_pink = ['#ffe6f3', '#fd79a8', '#e84393', '#c13584', '#962264']\n    default = mint_blue_red\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.DivergingColors","title":"DivergingColors","text":"<p>               Bases: <code>ConstantsIterable</code></p> <p>Diverging color palettes for data with meaningful midpoint.</p> <p>Provides color schemes that emphasize deviations from a central value, suitable for correlation matrices, anomaly detection, and comparative analysis.</p> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/plotly_theme.py</code> <pre><code>class DivergingColors(ConstantsIterable):\n    \"\"\"Diverging color palettes for data with meaningful midpoint.\n\n    Provides color schemes that emphasize deviations from a central value,\n    suitable for correlation matrices, anomaly detection, and comparative analysis.\n    \"\"\"\n    blue_mint = SequentialColors.shades_of_blue[::-1] + SequentialColors.shades_of_mint\n    red_mint = SequentialColors.shades_of_red[::-1] + SequentialColors.shades_of_mint\n    pink_blue = SequentialColors.shades_of_pink[::-1] + SequentialColors.shades_of_blue\n    pink_cyan = SequentialColors.shades_of_pink[::-1] + SequentialColors.shades_of_cyan\n    default = blue_mint\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.CyclicalColors","title":"CyclicalColors","text":"<p>               Bases: <code>ConstantsIterable</code></p> <p>Cyclical color palettes for periodic data.</p> <p>Reserved for future implementation of color schemes appropriate for cyclical data such as seasonal patterns or angular measurements.</p> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/plotly_theme.py</code> <pre><code>class CyclicalColors(ConstantsIterable):\n    \"\"\"Cyclical color palettes for periodic data.\n\n    Reserved for future implementation of color schemes appropriate for\n    cyclical data such as seasonal patterns or angular measurements.\n    \"\"\"\n    default = None\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.QualitativeColors","title":"QualitativeColors","text":"<p>               Bases: <code>ConstantsIterable</code></p> <p>Qualitative color palettes for categorical data.</p> <p>Provides distinct, visually separable colors for categorical variables without inherent ordering.</p> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/plotly_theme.py</code> <pre><code>class QualitativeColors(ConstantsIterable):\n    \"\"\"Qualitative color palettes for categorical data.\n\n    Provides distinct, visually separable colors for categorical variables\n    without inherent ordering.\n    \"\"\"\n    default = [\n        PrimaryColors.blue,\n        PrimaryColors.mint,\n        PrimaryColors.cyan,\n        PrimaryColors.red,\n        PrimaryColors.pink,\n        PrimaryColors.green_light,\n        PrimaryColors.green_bold,\n        PrimaryColors.orange_light,\n        PrimaryColors.orange_bold,\n        PrimaryColors.purple_light,\n        PrimaryColors.purple_bold,\n    ]\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.ColorPalette","title":"ColorPalette","text":"<p>Central access point for all color palette categories.</p> <p>Organizes color palettes by type (primary, sequential, diverging, etc.) for easy access and consistent usage across the framework.</p> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/plotly_theme.py</code> <pre><code>class ColorPalette:\n    \"\"\"Central access point for all color palette categories.\n\n    Organizes color palettes by type (primary, sequential, diverging, etc.)\n    for easy access and consistent usage across the framework.\n    \"\"\"\n    primary = PrimaryColors\n    sequential = SequentialColors\n    diverging = DivergingColors\n    cyclical = CyclicalColors\n    qualitative = QualitativeColors\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.PlotlyTheme","title":"PlotlyTheme  <code>dataclass</code>","text":"<p>Configurable Plotly theme for consistent figure styling.</p> <p>This dataclass encapsulates all theme settings and provides methods to apply them to Plotly's global template system. It supports customization of colors, fonts, backgrounds, axis styling, and optional watermarking.</p> <p>Attributes:</p> Name Type Description <code>default_colorway</code> <code>list[str]</code> <p>Default color sequence for traces.</p> <code>font</code> <code>dict</code> <p>Font configuration dictionary.</p> <code>paper_color</code> <code>str</code> <p>Background color outside the plot area.</p> <code>background_color</code> <code>str</code> <p>Background color of the plot area.</p> <code>xaxis</code> <code>dict</code> <p>X-axis styling configuration.</p> <code>yaxis</code> <code>dict</code> <p>Y-axis styling configuration.</p> <code>legend</code> <code>dict</code> <p>Legend styling configuration.</p> <code>watermark_text</code> <code>str</code> <p>Optional watermark text to display.</p> <code>watermark_position</code> <code>tuple[float, float]</code> <p>(x, y) position for watermark (paper coordinates).</p> <code>watermark_opacity</code> <code>float</code> <p>Opacity level for watermark (0.0 to 1.0).</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; theme = PlotlyTheme(\n...     default_colorway=colors.qualitative.default,\n...     watermark_text=\"MESCAL\"\n... )\n&gt;&gt;&gt; theme.apply()\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/plotly_theme.py</code> <pre><code>@dataclass\nclass PlotlyTheme:\n    \"\"\"Configurable Plotly theme for consistent figure styling.\n\n    This dataclass encapsulates all theme settings and provides methods to\n    apply them to Plotly's global template system. It supports customization\n    of colors, fonts, backgrounds, axis styling, and optional watermarking.\n\n    Attributes:\n        default_colorway: Default color sequence for traces.\n        font: Font configuration dictionary.\n        paper_color: Background color outside the plot area.\n        background_color: Background color of the plot area.\n        xaxis: X-axis styling configuration.\n        yaxis: Y-axis styling configuration.\n        legend: Legend styling configuration.\n        watermark_text: Optional watermark text to display.\n        watermark_position: (x, y) position for watermark (paper coordinates).\n        watermark_opacity: Opacity level for watermark (0.0 to 1.0).\n\n    Example:\n\n        &gt;&gt;&gt; theme = PlotlyTheme(\n        ...     default_colorway=colors.qualitative.default,\n        ...     watermark_text=\"MESCAL\"\n        ... )\n        &gt;&gt;&gt; theme.apply()\n    \"\"\"\n    default_colorway: list[str] = field(default_factory=list)\n    font: dict = field(default_factory=dict)\n    paper_color: str = '#ffffff'\n    background_color: str = '#F2F2F2'\n    xaxis: dict = field(default_factory=dict)\n    yaxis: dict = field(default_factory=dict)\n    legend: dict = field(default_factory=dict)\n    watermark_text: str = None\n    watermark_position: tuple[float, float] = (0.99, 0.01)\n    watermark_opacity: float = 0.1\n\n    def apply(self) -&gt; None:\n        \"\"\"Apply the theme settings to Plotly's global template system.\n\n        Creates a custom template with all specified settings and sets it as\n        the default template for all subsequent figure creation. The template\n        includes styling for layout, axes, legends, and optionally watermarks.\n\n        Note:\n            This method modifies Plotly's global state and affects all figures\n            created after calling this method.\n\n        Example:\n\n            &gt;&gt;&gt; theme = PlotlyTheme(watermark_text=\"My Project\")\n            &gt;&gt;&gt; theme.apply()\n            &gt;&gt;&gt; fig = go.Figure()  # Will use the custom theme\n        \"\"\"\n        template = go.layout.Template()\n\n        template.layout.colorway = self.default_colorway\n        template.layout.font = self.font\n        template.layout.paper_bgcolor = self.paper_color\n        template.layout.plot_bgcolor = self.background_color\n        template.layout.xaxis.update(self.xaxis)\n        template.layout.yaxis.update(self.yaxis)\n        template.layout.legend.update(self.legend)\n\n        template.layout.title = dict(x=0.5)\n\n        template.data.bar = [\n            go.Bar(marker=dict(line=dict(width=0)))\n        ]\n\n        if self.watermark_text:\n            template.layout.annotations = [\n                dict(\n                    name='watermark',\n                    text=self.watermark_text,\n                    xref=\"paper\",\n                    yref=\"paper\",\n                    x=self.watermark_position[0],\n                    y=self.watermark_position[1],\n                    showarrow=False,\n                    font=dict(size=50, color=\"black\"),\n                    opacity=self.watermark_opacity,\n                    textangle=0,\n                )\n            ]\n\n        pio.templates[\"custom\"] = template\n        pio.templates.default = \"plotly+custom\"\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mescal.utils.plotly_utils.plotly_theme.PlotlyTheme.apply","title":"apply","text":"<pre><code>apply() -&gt; None\n</code></pre> <p>Apply the theme settings to Plotly's global template system.</p> <p>Creates a custom template with all specified settings and sets it as the default template for all subsequent figure creation. The template includes styling for layout, axes, legends, and optionally watermarks.</p> Note <p>This method modifies Plotly's global state and affects all figures created after calling this method.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; theme = PlotlyTheme(watermark_text=\"My Project\")\n&gt;&gt;&gt; theme.apply()\n&gt;&gt;&gt; fig = go.Figure()  # Will use the custom theme\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/plotly_theme.py</code> <pre><code>def apply(self) -&gt; None:\n    \"\"\"Apply the theme settings to Plotly's global template system.\n\n    Creates a custom template with all specified settings and sets it as\n    the default template for all subsequent figure creation. The template\n    includes styling for layout, axes, legends, and optionally watermarks.\n\n    Note:\n        This method modifies Plotly's global state and affects all figures\n        created after calling this method.\n\n    Example:\n\n        &gt;&gt;&gt; theme = PlotlyTheme(watermark_text=\"My Project\")\n        &gt;&gt;&gt; theme.apply()\n        &gt;&gt;&gt; fig = go.Figure()  # Will use the custom theme\n    \"\"\"\n    template = go.layout.Template()\n\n    template.layout.colorway = self.default_colorway\n    template.layout.font = self.font\n    template.layout.paper_bgcolor = self.paper_color\n    template.layout.plot_bgcolor = self.background_color\n    template.layout.xaxis.update(self.xaxis)\n    template.layout.yaxis.update(self.yaxis)\n    template.layout.legend.update(self.legend)\n\n    template.layout.title = dict(x=0.5)\n\n    template.data.bar = [\n        go.Bar(marker=dict(line=dict(width=0)))\n    ]\n\n    if self.watermark_text:\n        template.layout.annotations = [\n            dict(\n                name='watermark',\n                text=self.watermark_text,\n                xref=\"paper\",\n                yref=\"paper\",\n                x=self.watermark_position[0],\n                y=self.watermark_position[1],\n                showarrow=False,\n                font=dict(size=50, color=\"black\"),\n                opacity=self.watermark_opacity,\n                textangle=0,\n            )\n        ]\n\n    pio.templates[\"custom\"] = template\n    pio.templates.default = \"plotly+custom\"\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/px_category_utils/","title":"MESCAL Plotly Util <code>px_category_utils</code>","text":""},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mescal.utils.plotly_utils.px_category_utils","title":"px_category_utils","text":"<p>Utilities for working with categorical data in Plotly Express subplots and faceted figures.</p> <p>This module provides functions to navigate and manipulate subplot structures created by Plotly Express, particularly when dealing with faceted plots and categorical data. It enables precise positioning of additional elements on specific subplot axes.</p>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mescal.utils.plotly_utils.px_category_utils.get_x_y_axis_for_category","title":"get_x_y_axis_for_category","text":"<pre><code>get_x_y_axis_for_category(fig: Figure, category_args: dict[str, str]) -&gt; tuple[str, str]\n</code></pre> <p>Find the x and y axis names for a subplot matching specific category values.</p> <p>Searches through figure traces to find one whose hovertemplate contains all specified category key-value pairs, then returns the corresponding axis names.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object containing subplot traces.</p> required <code>category_args</code> <code>dict[str, str]</code> <p>Dictionary mapping category names to their values (e.g., {'sex': 'Male', 'smoker': 'Yes'}).</p> required <p>Returns:</p> Type Description <code>str</code> <p>Tuple containing the x-axis and y-axis names (e.g., ('x', 'y') or</p> <code>str</code> <p>('x2', 'y3')).</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no trace contains all the specified category key-value pairs in its hovertemplate.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; category_args = {'sex': 'Male', 'smoker': 'Yes'}\n&gt;&gt;&gt; x_axis, y_axis = get_x_y_axis_for_category(fig, category_args)\n&gt;&gt;&gt; print(f\"Found axes: {x_axis}, {y_axis}\")\n    Found axes: x2, y3\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/px_category_utils.py</code> <pre><code>def get_x_y_axis_for_category(fig: go.Figure, category_args: dict[str, str]) -&gt; tuple[str, str]:\n    \"\"\"Find the x and y axis names for a subplot matching specific category values.\n\n    Searches through figure traces to find one whose hovertemplate contains all\n    specified category key-value pairs, then returns the corresponding axis names.\n\n    Args:\n        fig: Plotly figure object containing subplot traces.\n        category_args: Dictionary mapping category names to their values (e.g.,\n            {'sex': 'Male', 'smoker': 'Yes'}).\n\n    Returns:\n        Tuple containing the x-axis and y-axis names (e.g., ('x', 'y') or\n        ('x2', 'y3')).\n\n    Raises:\n        KeyError: If no trace contains all the specified category key-value pairs\n            in its hovertemplate.\n\n    Example:\n\n        &gt;&gt;&gt; category_args = {'sex': 'Male', 'smoker': 'Yes'}\n        &gt;&gt;&gt; x_axis, y_axis = get_x_y_axis_for_category(fig, category_args)\n        &gt;&gt;&gt; print(f\"Found axes: {x_axis}, {y_axis}\")\n            Found axes: x2, y3\n    \"\"\"\n    keys = [f'{k}={i}' for k, i in category_args.items()]\n    for trace in fig.data:\n        if all(k in trace.hovertemplate for k in keys):\n            x_axis = trace.xaxis if 'xaxis' in trace else 'x'\n            y_axis = trace.yaxis if 'yaxis' in trace else 'y'\n            return x_axis, y_axis\n    raise KeyError(f'No trace with matching key: value pairs {keys} found in any hovertemplate.')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mescal.utils.plotly_utils.px_category_utils.get_all_x_axis_names","title":"get_all_x_axis_names","text":"<pre><code>get_all_x_axis_names(fig: Figure) -&gt; list[str]\n</code></pre> <p>Get all x-axis names from a Plotly figure layout.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of x-axis attribute names found in the figure layout (e.g.,</p> <code>list[str]</code> <p>['xaxis', 'xaxis2', 'xaxis3']).</p> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/px_category_utils.py</code> <pre><code>def get_all_x_axis_names(fig: go.Figure) -&gt; list[str]:\n    \"\"\"Get all x-axis names from a Plotly figure layout.\n\n    Args:\n        fig: Plotly figure object.\n\n    Returns:\n        List of x-axis attribute names found in the figure layout (e.g.,\n        ['xaxis', 'xaxis2', 'xaxis3']).\n    \"\"\"\n    return [attr for attr in fig.layout if attr.startswith('xaxis')]\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mescal.utils.plotly_utils.px_category_utils.get_all_y_axis_names","title":"get_all_y_axis_names","text":"<pre><code>get_all_y_axis_names(fig: Figure) -&gt; list[str]\n</code></pre> <p>Get all y-axis names from a Plotly figure layout.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of y-axis attribute names found in the figure layout (e.g.,</p> <code>list[str]</code> <p>['yaxis', 'yaxis2', 'yaxis3']).</p> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/px_category_utils.py</code> <pre><code>def get_all_y_axis_names(fig: go.Figure) -&gt; list[str]:\n    \"\"\"Get all y-axis names from a Plotly figure layout.\n\n    Args:\n        fig: Plotly figure object.\n\n    Returns:\n        List of y-axis attribute names found in the figure layout (e.g.,\n        ['yaxis', 'yaxis2', 'yaxis3']).\n    \"\"\"\n    return [attr for attr in fig.layout if attr.startswith('yaxis')]\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mescal.utils.plotly_utils.px_category_utils.get_row_col_for_x_y_axis","title":"get_row_col_for_x_y_axis","text":"<pre><code>get_row_col_for_x_y_axis(fig: Figure, x_axis: str, y_axis: str) -&gt; tuple[int, int]\n</code></pre> <p>Convert axis names to subplot row and column indices.</p> <p>Determines the subplot grid position by analyzing axis domains within the figure layout. This is useful for adding elements to specific subplots.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object containing subplots.</p> required <code>x_axis</code> <code>str</code> <p>X-axis name (e.g., 'x', 'x2', 'xaxis', 'xaxis2').</p> required <code>y_axis</code> <code>str</code> <p>Y-axis name (e.g., 'y', 'y2', 'yaxis', 'yaxis2').</p> required <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Tuple of (row, col) indices for the subplot (1-indexed).</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the specified axis names are not found in the figure layout.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; row, col = get_row_col_for_x_y_axis(fig, 'x2', 'y3')\n&gt;&gt;&gt; print(f\"Subplot at row {row}, column {col}\")\n    Subplot at row 2, column 3\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/px_category_utils.py</code> <pre><code>def get_row_col_for_x_y_axis(fig: go.Figure, x_axis: str, y_axis: str) -&gt; tuple[int, int]:\n    \"\"\"Convert axis names to subplot row and column indices.\n\n    Determines the subplot grid position by analyzing axis domains within the\n    figure layout. This is useful for adding elements to specific subplots.\n\n    Args:\n        fig: Plotly figure object containing subplots.\n        x_axis: X-axis name (e.g., 'x', 'x2', 'xaxis', 'xaxis2').\n        y_axis: Y-axis name (e.g., 'y', 'y2', 'yaxis', 'yaxis2').\n\n    Returns:\n        Tuple of (row, col) indices for the subplot (1-indexed).\n\n    Raises:\n        KeyError: If the specified axis names are not found in the figure layout.\n\n    Example:\n\n        &gt;&gt;&gt; row, col = get_row_col_for_x_y_axis(fig, 'x2', 'y3')\n        &gt;&gt;&gt; print(f\"Subplot at row {row}, column {col}\")\n            Subplot at row 2, column 3\n    \"\"\"\n    all_x_axis_names = get_all_x_axis_names(fig)\n    all_y_axis_names = get_all_y_axis_names(fig)\n\n    x_domains = list(sorted(set([tuple(fig.layout[x].domain) for x in all_x_axis_names])))\n    y_domains = list(sorted(set([tuple(fig.layout[y].domain) for y in all_y_axis_names])))\n\n    if 'axis' not in x_axis:\n        x_axis = f'xaxis{x_axis[1:]}'\n    if 'axis' not in y_axis:\n        y_axis = f'yaxis{y_axis[1:]}'\n\n    if x_axis not in all_x_axis_names:\n        raise KeyError(f'No matching axis found for {x_axis}.')\n    if y_axis not in all_y_axis_names:\n        raise KeyError(f'No matching axis found for {y_axis}.')\n\n    col = x_domains.index(tuple(fig.layout[x_axis].domain)) + 1\n    row = y_domains.index(tuple(fig.layout[y_axis].domain)) + 1\n    return row, col\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mescal.utils.plotly_utils.px_category_utils.get_subplot_row_and_col_for_category","title":"get_subplot_row_and_col_for_category","text":"<pre><code>get_subplot_row_and_col_for_category(fig: Figure, category_args: dict[str, str]) -&gt; tuple[int, int]\n</code></pre> <p>Get subplot row and column for a specific category combination.</p> <p>Combines axis lookup and position conversion to directly find the subplot location for given category values. This is the main convenience function for adding elements to category-specific subplots.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object containing faceted subplots.</p> required <code>category_args</code> <code>dict[str, str]</code> <p>Dictionary mapping category names to their values.</p> required <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Tuple of (row, col) indices for the subplot (1-indexed).</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no subplot matches the specified category combination or if axis names cannot be found.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; category_args = {'sex': 'Female', 'smoker': 'No'}\n&gt;&gt;&gt; row, col = get_subplot_row_and_col_for_category(fig, category_args)\n&gt;&gt;&gt; fig.add_trace(trace, row=row, col=col)\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/px_category_utils.py</code> <pre><code>def get_subplot_row_and_col_for_category(fig: go.Figure, category_args: dict[str, str]) -&gt; tuple[int, int]:\n    \"\"\"Get subplot row and column for a specific category combination.\n\n    Combines axis lookup and position conversion to directly find the subplot\n    location for given category values. This is the main convenience function\n    for adding elements to category-specific subplots.\n\n    Args:\n        fig: Plotly figure object containing faceted subplots.\n        category_args: Dictionary mapping category names to their values.\n\n    Returns:\n        Tuple of (row, col) indices for the subplot (1-indexed).\n\n    Raises:\n        KeyError: If no subplot matches the specified category combination or\n            if axis names cannot be found.\n\n    Example:\n\n        &gt;&gt;&gt; category_args = {'sex': 'Female', 'smoker': 'No'}\n        &gt;&gt;&gt; row, col = get_subplot_row_and_col_for_category(fig, category_args)\n        &gt;&gt;&gt; fig.add_trace(trace, row=row, col=col)\n    \"\"\"\n    x_axis, y_axis = get_x_y_axis_for_category(fig, category_args)\n    row, col = get_row_col_for_x_y_axis(fig, x_axis, y_axis)\n    return row, col\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mescal.utils.plotly_utils.px_category_utils.get_index_for_category_on_axis","title":"get_index_for_category_on_axis","text":"<pre><code>get_index_for_category_on_axis(fig: Figure, axis: str, category_value: str) -&gt; int\n</code></pre> <p>Get the numerical index of a categorical value on a specific axis.</p> <p>Converts a categorical string value to its corresponding numerical position on the specified axis. Useful for precise positioning of annotations or additional traces on categorical axes.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object.</p> required <code>axis</code> <code>str</code> <p>Axis name (e.g., 'x', 'x2', 'y', 'y2', 'xaxis', 'yaxis2').</p> required <code>category_value</code> <code>str</code> <p>String value of the category to find.</p> required <p>Returns:</p> Type Description <code>int</code> <p>1-indexed numerical position of the category on the axis.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If category_value is not a string.</p> <code>KeyError</code> <p>If the axis name is invalid or the category value is not found in the axis categoryarray.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; index = get_index_for_category_on_axis(fig, 'x', 'Dinner')\n&gt;&gt;&gt; print(f\"'Dinner' is at position {index}\")\n    'Dinner' is at position 2\n</code></pre> Source code in <code>submodules/mescal/mescal/utils/plotly_utils/px_category_utils.py</code> <pre><code>def get_index_for_category_on_axis(fig: go.Figure, axis: str, category_value: str) -&gt; int:\n    \"\"\"Get the numerical index of a categorical value on a specific axis.\n\n    Converts a categorical string value to its corresponding numerical position\n    on the specified axis. Useful for precise positioning of annotations or\n    additional traces on categorical axes.\n\n    Args:\n        fig: Plotly figure object.\n        axis: Axis name (e.g., 'x', 'x2', 'y', 'y2', 'xaxis', 'yaxis2').\n        category_value: String value of the category to find.\n\n    Returns:\n        1-indexed numerical position of the category on the axis.\n\n    Raises:\n        TypeError: If category_value is not a string.\n        KeyError: If the axis name is invalid or the category value is not\n            found in the axis categoryarray.\n\n    Example:\n\n        &gt;&gt;&gt; index = get_index_for_category_on_axis(fig, 'x', 'Dinner')\n        &gt;&gt;&gt; print(f\"'Dinner' is at position {index}\")\n            'Dinner' is at position 2\n    \"\"\"\n    if not isinstance(category_value, str):\n        raise TypeError(\n            'Method only works with string categories. '\n            'Sure you need this? In case you already have an int / float, just use the value as an index directly.'\n        )\n    if 'axis' not in axis:\n        if axis.startswith('x'):\n            axis = f'xaxis{axis[1:]}'\n        elif axis.startswith('y'):\n            axis = f'yaxis{axis[1:]}'\n        else:\n            raise KeyError(f'Unknown axis {axis}')\n    cat_array = fig.layout[axis].categoryarray\n    if category_value not in cat_array:\n        raise KeyError(f'Unknown category {category_value}. Recognised categories for axis: {cat_array}')\n    index = cat_array.index(category_value) + 1\n    return index\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/","title":"MESCAL Visualization Modules","text":""},{"location":"mescal-package-documentation/api_reference/visualization/#mescal.visualizations","title":"visualizations","text":"<p>MESCAL Visualizations Package.</p> <p>This package provides comprehensive visualization capabilities for energy systems analysis, including interactive maps, time series dashboards, and data export functionality.</p> <p>The visualizations package supports multiple output formats and interactive components designed specifically for multi-scenario energy modeling analysis and comparison.</p> <p>Modules:</p> Name Description <code>- **folium_legend_system</code> <p>** Legend creation and management for Folium maps</p> <code>- **folium_viz_system</code> <p>** Interactive map visualization system using Folium</p> <code>- **value_mapping_system</code> <p>** Data value mapping and color scaling utilities</p> <p>Classes:</p> Name Description <code>- **TimeSeriesDashboardGenerator</code> <p>** Creates interactive Plotly time series dashboards</p> <code>- **HTMLDashboard</code> <p>** Generates comprehensive HTML analysis dashboards</p> <code>- **HTMLTable</code> <p>** Creates formatted HTML tables for data presentation</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from mescal.visualizations import HTMLDashboard, TimeSeriesDashboardGenerator\n&gt;&gt;&gt; dashboard = HTMLDashboard()\n&gt;&gt;&gt; ts_gen = TimeSeriesDashboardGenerator()\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_dashboard/","title":"MESCAL HTML Dashboard","text":""},{"location":"mescal-package-documentation/api_reference/visualization/html_dashboard/#mescal.visualizations.html_dashboard.HTMLDashboard","title":"HTMLDashboard","text":"<p>A dashboard builder for creating HTML reports with multiple visualizations.</p> <p>This class provides a flexible way to combine Plotly figures, Folium maps, HTML tables, custom HTML content, and section dividers into a single HTML dashboard file. Elements are stored with unique names and can be ordered when saving the final dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The dashboard title. Defaults to 'HTML Dashboard'.</p> <code>None</code> <code>font_family</code> <code>str</code> <p>CSS font family specification for the dashboard. Defaults to \"Arial, sans-serif\".</p> <code>'Arial, sans-serif'</code> <p>Attributes:</p> Name Type Description <code>name</code> <p>The dashboard title.</p> <code>content</code> <code>Dict[str, HTMLDashboardElement]</code> <p>Dictionary mapping element names to HTMLDashboardElement objects.</p> <code>font_family</code> <p>The CSS font family specification.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import plotly.express as px\n&gt;&gt;&gt; dashboard = HTMLDashboard(name=\"My Analysis\")\n&gt;&gt;&gt; fig = px.scatter(px.data.iris(), x=\"sepal_width\", y=\"sepal_length\")\n&gt;&gt;&gt; dashboard.add_plotly_figure(fig, name=\"iris_scatter\")\n&gt;&gt;&gt; dashboard.save(\"analysis.html\")\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/html_dashboard.py</code> <pre><code>class HTMLDashboard:\n    \"\"\"A dashboard builder for creating HTML reports with multiple visualizations.\n\n    This class provides a flexible way to combine Plotly figures, Folium maps,\n    HTML tables, custom HTML content, and section dividers into a single HTML\n    dashboard file. Elements are stored with unique names and can be ordered\n    when saving the final dashboard.\n\n    Args:\n        name: The dashboard title. Defaults to 'HTML Dashboard'.\n        font_family: CSS font family specification for the dashboard.\n            Defaults to \"Arial, sans-serif\".\n\n    Attributes:\n        name: The dashboard title.\n        content: Dictionary mapping element names to HTMLDashboardElement objects.\n        font_family: The CSS font family specification.\n\n    Example:\n\n        &gt;&gt;&gt; import plotly.express as px\n        &gt;&gt;&gt; dashboard = HTMLDashboard(name=\"My Analysis\")\n        &gt;&gt;&gt; fig = px.scatter(px.data.iris(), x=\"sepal_width\", y=\"sepal_length\")\n        &gt;&gt;&gt; dashboard.add_plotly_figure(fig, name=\"iris_scatter\")\n        &gt;&gt;&gt; dashboard.save(\"analysis.html\")\n    \"\"\"\n    def __init__(self, name: str = None, font_family: str = \"Arial, sans-serif\"):\n        self.name = name if name else 'HTML Dashboard'\n        self.content: Dict[str, HTMLDashboardElement] = dict()\n        self.font_family = font_family\n\n    def add_plotly_figure(self, fig: go.Figure, height: str = '100%', name: str = None):\n        \"\"\"Add a Plotly figure to the dashboard.\n\n        Args:\n            fig: The Plotly figure to add.\n            height: CSS height specification for the figure. Defaults to '100%'.\n            name: Unique identifier for the figure. If None, auto-generates.\n\n        Example:\n\n            &gt;&gt;&gt; import plotly.express as px\n            &gt;&gt;&gt; fig = px.bar(x=[\"A\", \"B\", \"C\"], y=[1, 3, 2])\n            &gt;&gt;&gt; dashboard.add_plotly_figure(fig, height=\"400px\", name=\"my_bar_chart\")\n        \"\"\"\n        element = HTMLDashboardElement(fig, height, name)\n        self.content[element.name] = element\n\n    def add_html(self, html_string: str, name: str = None):\n        \"\"\"Add custom HTML content to the dashboard.\n\n        Args:\n            html_string: The HTML content to add.\n            name: Unique identifier for the HTML content. If None, auto-generates.\n\n        Example:\n\n            &gt;&gt;&gt; html = \"&lt;div&gt;&lt;h2&gt;Custom Section&lt;/h2&gt;&lt;p&gt;Some content here.&lt;/p&gt;&lt;/div&gt;\"\n            &gt;&gt;&gt; dashboard.add_html(html, name=\"custom_section\")\n        \"\"\"\n        element = HTMLDashboardElement(html_string, name=name)\n        self.content[element.name] = element\n\n    def add_folium_map(\n            self,\n            folium_map: 'folium.Map',\n            name: str = None,\n    ):\n        \"\"\"Add a Folium map to the dashboard.\n\n        Args:\n            folium_map: The Folium map object to add.\n            name: Unique identifier for the map. If None, auto-generates\n                as \"folium_map_{index}\".\n\n        Returns:\n            str: The name assigned to the map element.\n\n        Example:\n\n            &gt;&gt;&gt; import folium\n            &gt;&gt;&gt; m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)\n            &gt;&gt;&gt; map_name = dashboard.add_folium_map(m, name=\"portland_map\")\n        \"\"\"\n        map_html = folium_map._repr_html_()\n\n        if name is None:\n            name = f\"folium_map_{len([k for k in self.content.keys() if 'folium_map' in k])}\"\n\n        wrapped_map_html = f'&lt;div&gt;{map_html}&lt;/div&gt;'\n\n        self.add_html(wrapped_map_html, name=name)\n\n        return name\n\n    def add_table(\n            self,\n            table: 'HTMLTable',\n            name: str = None,\n            include_dependencies: bool = True\n    ) -&gt; str:\n        \"\"\"Add an HTML table to the dashboard.\n\n        Args:\n            table: The HTMLTable object to add.\n            name: Unique identifier for the table. If None, derives from table title\n                or uses table_id.\n            include_dependencies: Whether to include CSS/JS dependencies in the\n                table HTML. Defaults to True.\n\n        Returns:\n            str: The name assigned to the table element.\n\n        Raises:\n            ValueError: If the table cannot be converted to HTML.\n\n        Example:\n\n            &gt;&gt;&gt; from mescal.visualizations.html_table import HTMLTable\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n            &gt;&gt;&gt; table = HTMLTable(df, title=\"Sample Data\")\n            &gt;&gt;&gt; table_name = dashboard.add_table(table, name=\"sample_table\")\n        \"\"\"\n        try:\n            table_html = table.get_html(include_dependencies=include_dependencies)\n\n            if name is None:\n                name = f\"table_{table.title.lower().replace(' ', '_')}\" if table.title else table.table_id\n\n            self.add_html(table_html, name=name)\n            return name\n\n        except Exception as e:\n            raise ValueError(f\"Failed to add table to dashboard: {str(e)}\") from e\n\n    def add_section_divider(\n            self,\n            title: str,\n            subtitle: str = None,\n            name: str = None,\n            background_color: str = \"#f9f9f9\",\n            title_color: str = \"#333\",\n            subtitle_color: str = \"#666\",\n            padding: str = \"20px\",\n            margin: str = \"20px 0\",\n            text_align: str = \"center\",\n            title_font_size: str = \"24px\",\n            subtitle_font_size: str = \"16px\",\n            border_radius: str = \"0px\",\n            border: str = \"none\",\n            **kwargs\n    ) -&gt; str:\n        \"\"\"Add a styled section divider with title and optional subtitle.\n\n        Creates a formatted section header that can be used to organize dashboard\n        content into logical groups. Supports extensive CSS customization through\n        parameters and keyword arguments.\n\n        Args:\n            title: The main section title.\n            subtitle: Optional subtitle text.\n            name: Unique identifier for the divider. If None, derives from title.\n            background_color: CSS background color. Defaults to \"#f9f9f9\".\n            title_color: CSS color for the title text. Defaults to \"#333\".\n            subtitle_color: CSS color for the subtitle text. Defaults to \"#666\".\n            padding: CSS padding specification. Defaults to \"20px\".\n            margin: CSS margin specification. Defaults to \"20px 0\".\n            text_align: CSS text alignment. Defaults to \"center\".\n            title_font_size: CSS font size for title. Defaults to \"24px\".\n            subtitle_font_size: CSS font size for subtitle. Defaults to \"16px\".\n            border_radius: CSS border radius. Defaults to \"0px\".\n            border: CSS border specification. Defaults to \"none\".\n            **kwargs: Additional CSS properties. Underscores in keys are converted\n                to camelCase (e.g., box_shadow becomes boxShadow).\n\n        Returns:\n            str: The name assigned to the section divider element.\n\n        Example:\n\n            &gt;&gt;&gt; divider_name = dashboard.add_section_divider(\n            ...     title=\"Data Analysis Results\",\n            ...     subtitle=\"Generated on 2024-01-01\",\n            ...     background_color=\"#e3f2fd\",\n            ...     border=\"1px solid #2196f3\"\n            ... )\n        \"\"\"\n        base_style = f\"background-color: {background_color}; padding: {padding}; margin: {margin}; text-align: {text_align}; border-radius: {border_radius}; border: {border};\"\n\n        for key, value in kwargs.items():\n            css_key = ''.join(word.capitalize() if i &gt; 0 else word for i, word in enumerate(key.split('_')))\n            base_style += f\" {css_key}: {value};\"\n\n        html = f'&lt;div style=\"{base_style}\"&gt;\\n'\n        html += f'    &lt;h2 style=\"color: {title_color}; font-size: {title_font_size};\"&gt;{title}&lt;/h2&gt;\\n'\n\n        if subtitle:\n            html += f'    &lt;p style=\"color: {subtitle_color}; font-size: {subtitle_font_size};\"&gt;{subtitle}&lt;/p&gt;\\n'\n\n        html += '&lt;/div&gt;'\n\n        if name is None:\n            name = f\"section_{title.lower().replace(' ', '_')}\"\n\n        self.add_html(html, name=name)\n\n        return name\n\n    def save(self, save_to_path, content_order=None):\n        \"\"\"Save the dashboard as an HTML file.\n\n        Generates a complete HTML document containing all dashboard elements.\n        Automatically handles Plotly.js inclusion (only includes once for efficiency)\n        and creates output directories as needed.\n\n        Args:\n            save_to_path: File path where the HTML dashboard will be saved.\n            content_order: Optional list specifying the order of elements in the\n                dashboard. If None, uses the order elements were added. Must\n                contain only valid element names.\n\n        Raises:\n            KeyError: If content_order contains names not found in the dashboard.\n            TypeError: If an element has an unexpected type (internal error).\n\n        Example:\n\n            &gt;&gt;&gt; dashboard.save(\"my_dashboard.html\")\n            &gt;&gt;&gt; # Custom ordering\n            &gt;&gt;&gt; dashboard.save(\"ordered_dashboard.html\",\n            ...               content_order=[\"intro_section\", \"chart1\", \"table1\"])\n        \"\"\"\n        if not os.path.exists(os.path.dirname(save_to_path)):\n            os.makedirs(os.path.dirname(save_to_path))\n\n        if content_order is None:\n            content_order = list(self.content.keys())\n        else:\n            unrecognized = [k for k in content_order if k not in self.content.keys()]\n            if unrecognized:\n                raise KeyError(f'Unrecognized content names: {unrecognized}. Allowed: {self.content.keys()}')\n\n        content = []\n        plotly_js_included = False\n        for key in content_order:\n            v = self.content[key]\n            if isinstance(v.element, go.Figure):\n                html_text = v.element.to_html(\n                    include_plotlyjs=True if not plotly_js_included else False,\n                    full_html=False,\n                    default_height=v.height\n                )\n                content.append(html_text)\n                plotly_js_included = True\n            elif isinstance(v.element, str):\n                content.append(v.element)\n            else:\n                TypeError(f'Unexpected element type: {type(v.element)}')\n\n        with open(save_to_path, 'w', encoding='utf-8') as dashboard:\n            dashboard.write(\"&lt;html&gt;&lt;head&gt;\\n\")\n            dashboard.write(\"&lt;meta charset='UTF-8'&gt;\\n\")\n            dashboard.write(f\"&lt;title&gt;{self.name}&lt;/title&gt;\\n\")\n            dashboard.write(f\"&lt;style&gt;\\n  body, * {{ font-family: {self.font_family}; }}\\n&lt;/style&gt;\\n\")\n            dashboard.write(\"&lt;/head&gt;&lt;body&gt;\\n\")\n\n            for item in content:\n                dashboard.write(item + \"\\n\")\n\n            dashboard.write(\"&lt;/body&gt;&lt;/html&gt;\\n\")\n\n    def show(self, width: str = \"100%\", height: str = \"600\"):\n        \"\"\"Display the dashboard inline in a Jupyter notebook.\n\n        Creates a temporary HTML file and displays it using an IPython IFrame.\n        This method is designed for use within Jupyter notebooks to provide\n        inline dashboard previews.\n\n        Args:\n            width: CSS width specification for the iframe. Defaults to \"100%\".\n            height: CSS height specification for the iframe. Defaults to \"600\".\n\n        Note:\n            This method requires IPython and is intended for Jupyter notebook use.\n            The temporary file is created in the system temp directory.\n\n        Example:\n\n            &gt;&gt;&gt; # In a Jupyter notebook cell\n            &gt;&gt;&gt; dashboard.show(width=\"100%\", height=\"800px\")\n        \"\"\"\n        import tempfile\n        from pathlib import Path\n        from IPython.display import IFrame, display\n\n        tmp_dir = Path(tempfile.mkdtemp())\n        html_path = tmp_dir / \"dashboard.html\"\n        self.save(html_path)\n        display(IFrame(src=str(html_path.resolve()), width=width, height=height))\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_dashboard/#mescal.visualizations.html_dashboard.HTMLDashboard.add_plotly_figure","title":"add_plotly_figure","text":"<pre><code>add_plotly_figure(fig: Figure, height: str = '100%', name: str = None)\n</code></pre> <p>Add a Plotly figure to the dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The Plotly figure to add.</p> required <code>height</code> <code>str</code> <p>CSS height specification for the figure. Defaults to '100%'.</p> <code>'100%'</code> <code>name</code> <code>str</code> <p>Unique identifier for the figure. If None, auto-generates.</p> <code>None</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; import plotly.express as px\n&gt;&gt;&gt; fig = px.bar(x=[\"A\", \"B\", \"C\"], y=[1, 3, 2])\n&gt;&gt;&gt; dashboard.add_plotly_figure(fig, height=\"400px\", name=\"my_bar_chart\")\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/html_dashboard.py</code> <pre><code>def add_plotly_figure(self, fig: go.Figure, height: str = '100%', name: str = None):\n    \"\"\"Add a Plotly figure to the dashboard.\n\n    Args:\n        fig: The Plotly figure to add.\n        height: CSS height specification for the figure. Defaults to '100%'.\n        name: Unique identifier for the figure. If None, auto-generates.\n\n    Example:\n\n        &gt;&gt;&gt; import plotly.express as px\n        &gt;&gt;&gt; fig = px.bar(x=[\"A\", \"B\", \"C\"], y=[1, 3, 2])\n        &gt;&gt;&gt; dashboard.add_plotly_figure(fig, height=\"400px\", name=\"my_bar_chart\")\n    \"\"\"\n    element = HTMLDashboardElement(fig, height, name)\n    self.content[element.name] = element\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_dashboard/#mescal.visualizations.html_dashboard.HTMLDashboard.add_html","title":"add_html","text":"<pre><code>add_html(html_string: str, name: str = None)\n</code></pre> <p>Add custom HTML content to the dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>html_string</code> <code>str</code> <p>The HTML content to add.</p> required <code>name</code> <code>str</code> <p>Unique identifier for the HTML content. If None, auto-generates.</p> <code>None</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; html = \"&lt;div&gt;&lt;h2&gt;Custom Section&lt;/h2&gt;&lt;p&gt;Some content here.&lt;/p&gt;&lt;/div&gt;\"\n&gt;&gt;&gt; dashboard.add_html(html, name=\"custom_section\")\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/html_dashboard.py</code> <pre><code>def add_html(self, html_string: str, name: str = None):\n    \"\"\"Add custom HTML content to the dashboard.\n\n    Args:\n        html_string: The HTML content to add.\n        name: Unique identifier for the HTML content. If None, auto-generates.\n\n    Example:\n\n        &gt;&gt;&gt; html = \"&lt;div&gt;&lt;h2&gt;Custom Section&lt;/h2&gt;&lt;p&gt;Some content here.&lt;/p&gt;&lt;/div&gt;\"\n        &gt;&gt;&gt; dashboard.add_html(html, name=\"custom_section\")\n    \"\"\"\n    element = HTMLDashboardElement(html_string, name=name)\n    self.content[element.name] = element\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_dashboard/#mescal.visualizations.html_dashboard.HTMLDashboard.add_folium_map","title":"add_folium_map","text":"<pre><code>add_folium_map(folium_map: Map, name: str = None)\n</code></pre> <p>Add a Folium map to the dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>folium_map</code> <code>Map</code> <p>The Folium map object to add.</p> required <code>name</code> <code>str</code> <p>Unique identifier for the map. If None, auto-generates as \"folium_map_{index}\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The name assigned to the map element.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import folium\n&gt;&gt;&gt; m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)\n&gt;&gt;&gt; map_name = dashboard.add_folium_map(m, name=\"portland_map\")\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/html_dashboard.py</code> <pre><code>def add_folium_map(\n        self,\n        folium_map: 'folium.Map',\n        name: str = None,\n):\n    \"\"\"Add a Folium map to the dashboard.\n\n    Args:\n        folium_map: The Folium map object to add.\n        name: Unique identifier for the map. If None, auto-generates\n            as \"folium_map_{index}\".\n\n    Returns:\n        str: The name assigned to the map element.\n\n    Example:\n\n        &gt;&gt;&gt; import folium\n        &gt;&gt;&gt; m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)\n        &gt;&gt;&gt; map_name = dashboard.add_folium_map(m, name=\"portland_map\")\n    \"\"\"\n    map_html = folium_map._repr_html_()\n\n    if name is None:\n        name = f\"folium_map_{len([k for k in self.content.keys() if 'folium_map' in k])}\"\n\n    wrapped_map_html = f'&lt;div&gt;{map_html}&lt;/div&gt;'\n\n    self.add_html(wrapped_map_html, name=name)\n\n    return name\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_dashboard/#mescal.visualizations.html_dashboard.HTMLDashboard.add_table","title":"add_table","text":"<pre><code>add_table(table: HTMLTable, name: str = None, include_dependencies: bool = True) -&gt; str\n</code></pre> <p>Add an HTML table to the dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>HTMLTable</code> <p>The HTMLTable object to add.</p> required <code>name</code> <code>str</code> <p>Unique identifier for the table. If None, derives from table title or uses table_id.</p> <code>None</code> <code>include_dependencies</code> <code>bool</code> <p>Whether to include CSS/JS dependencies in the table HTML. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name assigned to the table element.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the table cannot be converted to HTML.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from mescal.visualizations.html_table import HTMLTable\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n&gt;&gt;&gt; table = HTMLTable(df, title=\"Sample Data\")\n&gt;&gt;&gt; table_name = dashboard.add_table(table, name=\"sample_table\")\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/html_dashboard.py</code> <pre><code>def add_table(\n        self,\n        table: 'HTMLTable',\n        name: str = None,\n        include_dependencies: bool = True\n) -&gt; str:\n    \"\"\"Add an HTML table to the dashboard.\n\n    Args:\n        table: The HTMLTable object to add.\n        name: Unique identifier for the table. If None, derives from table title\n            or uses table_id.\n        include_dependencies: Whether to include CSS/JS dependencies in the\n            table HTML. Defaults to True.\n\n    Returns:\n        str: The name assigned to the table element.\n\n    Raises:\n        ValueError: If the table cannot be converted to HTML.\n\n    Example:\n\n        &gt;&gt;&gt; from mescal.visualizations.html_table import HTMLTable\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n        &gt;&gt;&gt; table = HTMLTable(df, title=\"Sample Data\")\n        &gt;&gt;&gt; table_name = dashboard.add_table(table, name=\"sample_table\")\n    \"\"\"\n    try:\n        table_html = table.get_html(include_dependencies=include_dependencies)\n\n        if name is None:\n            name = f\"table_{table.title.lower().replace(' ', '_')}\" if table.title else table.table_id\n\n        self.add_html(table_html, name=name)\n        return name\n\n    except Exception as e:\n        raise ValueError(f\"Failed to add table to dashboard: {str(e)}\") from e\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_dashboard/#mescal.visualizations.html_dashboard.HTMLDashboard.add_section_divider","title":"add_section_divider","text":"<pre><code>add_section_divider(title: str, subtitle: str = None, name: str = None, background_color: str = '#f9f9f9', title_color: str = '#333', subtitle_color: str = '#666', padding: str = '20px', margin: str = '20px 0', text_align: str = 'center', title_font_size: str = '24px', subtitle_font_size: str = '16px', border_radius: str = '0px', border: str = 'none', **kwargs) -&gt; str\n</code></pre> <p>Add a styled section divider with title and optional subtitle.</p> <p>Creates a formatted section header that can be used to organize dashboard content into logical groups. Supports extensive CSS customization through parameters and keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>The main section title.</p> required <code>subtitle</code> <code>str</code> <p>Optional subtitle text.</p> <code>None</code> <code>name</code> <code>str</code> <p>Unique identifier for the divider. If None, derives from title.</p> <code>None</code> <code>background_color</code> <code>str</code> <p>CSS background color. Defaults to \"#f9f9f9\".</p> <code>'#f9f9f9'</code> <code>title_color</code> <code>str</code> <p>CSS color for the title text. Defaults to \"#333\".</p> <code>'#333'</code> <code>subtitle_color</code> <code>str</code> <p>CSS color for the subtitle text. Defaults to \"#666\".</p> <code>'#666'</code> <code>padding</code> <code>str</code> <p>CSS padding specification. Defaults to \"20px\".</p> <code>'20px'</code> <code>margin</code> <code>str</code> <p>CSS margin specification. Defaults to \"20px 0\".</p> <code>'20px 0'</code> <code>text_align</code> <code>str</code> <p>CSS text alignment. Defaults to \"center\".</p> <code>'center'</code> <code>title_font_size</code> <code>str</code> <p>CSS font size for title. Defaults to \"24px\".</p> <code>'24px'</code> <code>subtitle_font_size</code> <code>str</code> <p>CSS font size for subtitle. Defaults to \"16px\".</p> <code>'16px'</code> <code>border_radius</code> <code>str</code> <p>CSS border radius. Defaults to \"0px\".</p> <code>'0px'</code> <code>border</code> <code>str</code> <p>CSS border specification. Defaults to \"none\".</p> <code>'none'</code> <code>**kwargs</code> <p>Additional CSS properties. Underscores in keys are converted to camelCase (e.g., box_shadow becomes boxShadow).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name assigned to the section divider element.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; divider_name = dashboard.add_section_divider(\n...     title=\"Data Analysis Results\",\n...     subtitle=\"Generated on 2024-01-01\",\n...     background_color=\"#e3f2fd\",\n...     border=\"1px solid #2196f3\"\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/html_dashboard.py</code> <pre><code>def add_section_divider(\n        self,\n        title: str,\n        subtitle: str = None,\n        name: str = None,\n        background_color: str = \"#f9f9f9\",\n        title_color: str = \"#333\",\n        subtitle_color: str = \"#666\",\n        padding: str = \"20px\",\n        margin: str = \"20px 0\",\n        text_align: str = \"center\",\n        title_font_size: str = \"24px\",\n        subtitle_font_size: str = \"16px\",\n        border_radius: str = \"0px\",\n        border: str = \"none\",\n        **kwargs\n) -&gt; str:\n    \"\"\"Add a styled section divider with title and optional subtitle.\n\n    Creates a formatted section header that can be used to organize dashboard\n    content into logical groups. Supports extensive CSS customization through\n    parameters and keyword arguments.\n\n    Args:\n        title: The main section title.\n        subtitle: Optional subtitle text.\n        name: Unique identifier for the divider. If None, derives from title.\n        background_color: CSS background color. Defaults to \"#f9f9f9\".\n        title_color: CSS color for the title text. Defaults to \"#333\".\n        subtitle_color: CSS color for the subtitle text. Defaults to \"#666\".\n        padding: CSS padding specification. Defaults to \"20px\".\n        margin: CSS margin specification. Defaults to \"20px 0\".\n        text_align: CSS text alignment. Defaults to \"center\".\n        title_font_size: CSS font size for title. Defaults to \"24px\".\n        subtitle_font_size: CSS font size for subtitle. Defaults to \"16px\".\n        border_radius: CSS border radius. Defaults to \"0px\".\n        border: CSS border specification. Defaults to \"none\".\n        **kwargs: Additional CSS properties. Underscores in keys are converted\n            to camelCase (e.g., box_shadow becomes boxShadow).\n\n    Returns:\n        str: The name assigned to the section divider element.\n\n    Example:\n\n        &gt;&gt;&gt; divider_name = dashboard.add_section_divider(\n        ...     title=\"Data Analysis Results\",\n        ...     subtitle=\"Generated on 2024-01-01\",\n        ...     background_color=\"#e3f2fd\",\n        ...     border=\"1px solid #2196f3\"\n        ... )\n    \"\"\"\n    base_style = f\"background-color: {background_color}; padding: {padding}; margin: {margin}; text-align: {text_align}; border-radius: {border_radius}; border: {border};\"\n\n    for key, value in kwargs.items():\n        css_key = ''.join(word.capitalize() if i &gt; 0 else word for i, word in enumerate(key.split('_')))\n        base_style += f\" {css_key}: {value};\"\n\n    html = f'&lt;div style=\"{base_style}\"&gt;\\n'\n    html += f'    &lt;h2 style=\"color: {title_color}; font-size: {title_font_size};\"&gt;{title}&lt;/h2&gt;\\n'\n\n    if subtitle:\n        html += f'    &lt;p style=\"color: {subtitle_color}; font-size: {subtitle_font_size};\"&gt;{subtitle}&lt;/p&gt;\\n'\n\n    html += '&lt;/div&gt;'\n\n    if name is None:\n        name = f\"section_{title.lower().replace(' ', '_')}\"\n\n    self.add_html(html, name=name)\n\n    return name\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_dashboard/#mescal.visualizations.html_dashboard.HTMLDashboard.save","title":"save","text":"<pre><code>save(save_to_path, content_order=None)\n</code></pre> <p>Save the dashboard as an HTML file.</p> <p>Generates a complete HTML document containing all dashboard elements. Automatically handles Plotly.js inclusion (only includes once for efficiency) and creates output directories as needed.</p> <p>Parameters:</p> Name Type Description Default <code>save_to_path</code> <p>File path where the HTML dashboard will be saved.</p> required <code>content_order</code> <p>Optional list specifying the order of elements in the dashboard. If None, uses the order elements were added. Must contain only valid element names.</p> <code>None</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If content_order contains names not found in the dashboard.</p> <code>TypeError</code> <p>If an element has an unexpected type (internal error).</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; dashboard.save(\"my_dashboard.html\")\n&gt;&gt;&gt; # Custom ordering\n&gt;&gt;&gt; dashboard.save(\"ordered_dashboard.html\",\n...               content_order=[\"intro_section\", \"chart1\", \"table1\"])\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/html_dashboard.py</code> <pre><code>def save(self, save_to_path, content_order=None):\n    \"\"\"Save the dashboard as an HTML file.\n\n    Generates a complete HTML document containing all dashboard elements.\n    Automatically handles Plotly.js inclusion (only includes once for efficiency)\n    and creates output directories as needed.\n\n    Args:\n        save_to_path: File path where the HTML dashboard will be saved.\n        content_order: Optional list specifying the order of elements in the\n            dashboard. If None, uses the order elements were added. Must\n            contain only valid element names.\n\n    Raises:\n        KeyError: If content_order contains names not found in the dashboard.\n        TypeError: If an element has an unexpected type (internal error).\n\n    Example:\n\n        &gt;&gt;&gt; dashboard.save(\"my_dashboard.html\")\n        &gt;&gt;&gt; # Custom ordering\n        &gt;&gt;&gt; dashboard.save(\"ordered_dashboard.html\",\n        ...               content_order=[\"intro_section\", \"chart1\", \"table1\"])\n    \"\"\"\n    if not os.path.exists(os.path.dirname(save_to_path)):\n        os.makedirs(os.path.dirname(save_to_path))\n\n    if content_order is None:\n        content_order = list(self.content.keys())\n    else:\n        unrecognized = [k for k in content_order if k not in self.content.keys()]\n        if unrecognized:\n            raise KeyError(f'Unrecognized content names: {unrecognized}. Allowed: {self.content.keys()}')\n\n    content = []\n    plotly_js_included = False\n    for key in content_order:\n        v = self.content[key]\n        if isinstance(v.element, go.Figure):\n            html_text = v.element.to_html(\n                include_plotlyjs=True if not plotly_js_included else False,\n                full_html=False,\n                default_height=v.height\n            )\n            content.append(html_text)\n            plotly_js_included = True\n        elif isinstance(v.element, str):\n            content.append(v.element)\n        else:\n            TypeError(f'Unexpected element type: {type(v.element)}')\n\n    with open(save_to_path, 'w', encoding='utf-8') as dashboard:\n        dashboard.write(\"&lt;html&gt;&lt;head&gt;\\n\")\n        dashboard.write(\"&lt;meta charset='UTF-8'&gt;\\n\")\n        dashboard.write(f\"&lt;title&gt;{self.name}&lt;/title&gt;\\n\")\n        dashboard.write(f\"&lt;style&gt;\\n  body, * {{ font-family: {self.font_family}; }}\\n&lt;/style&gt;\\n\")\n        dashboard.write(\"&lt;/head&gt;&lt;body&gt;\\n\")\n\n        for item in content:\n            dashboard.write(item + \"\\n\")\n\n        dashboard.write(\"&lt;/body&gt;&lt;/html&gt;\\n\")\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_dashboard/#mescal.visualizations.html_dashboard.HTMLDashboard.show","title":"show","text":"<pre><code>show(width: str = '100%', height: str = '600')\n</code></pre> <p>Display the dashboard inline in a Jupyter notebook.</p> <p>Creates a temporary HTML file and displays it using an IPython IFrame. This method is designed for use within Jupyter notebooks to provide inline dashboard previews.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>str</code> <p>CSS width specification for the iframe. Defaults to \"100%\".</p> <code>'100%'</code> <code>height</code> <code>str</code> <p>CSS height specification for the iframe. Defaults to \"600\".</p> <code>'600'</code> Note <p>This method requires IPython and is intended for Jupyter notebook use. The temporary file is created in the system temp directory.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # In a Jupyter notebook cell\n&gt;&gt;&gt; dashboard.show(width=\"100%\", height=\"800px\")\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/html_dashboard.py</code> <pre><code>def show(self, width: str = \"100%\", height: str = \"600\"):\n    \"\"\"Display the dashboard inline in a Jupyter notebook.\n\n    Creates a temporary HTML file and displays it using an IPython IFrame.\n    This method is designed for use within Jupyter notebooks to provide\n    inline dashboard previews.\n\n    Args:\n        width: CSS width specification for the iframe. Defaults to \"100%\".\n        height: CSS height specification for the iframe. Defaults to \"600\".\n\n    Note:\n        This method requires IPython and is intended for Jupyter notebook use.\n        The temporary file is created in the system temp directory.\n\n    Example:\n\n        &gt;&gt;&gt; # In a Jupyter notebook cell\n        &gt;&gt;&gt; dashboard.show(width=\"100%\", height=\"800px\")\n    \"\"\"\n    import tempfile\n    from pathlib import Path\n    from IPython.display import IFrame, display\n\n    tmp_dir = Path(tempfile.mkdtemp())\n    html_path = tmp_dir / \"dashboard.html\"\n    self.save(html_path)\n    display(IFrame(src=str(html_path.resolve()), width=width, height=height))\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_table/","title":"MESCAL HTML Table","text":""},{"location":"mescal-package-documentation/api_reference/visualization/html_table/#mescal.visualizations.html_table.HTMLTable","title":"HTMLTable","text":"<p>A class to create interactive Tabulator tables from pandas DataFrames.</p> <p>Tabulator (http://tabulator.info/) is a feature-rich interactive table library that provides sorting, filtering, formatting, and editing capabilities.</p> <p>This class automatically detects column types and applies appropriate formatters, sorters, and filters. It supports customization through column configuration and provides methods to display tables in Jupyter notebooks or save as HTML files.</p> <p>Attributes:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame to display.</p> <code>title</code> <code>Optional[str]</code> <p>Optional title for the table.</p> <code>height</code> <code>str</code> <p>Height of the table container.</p> <code>layout</code> <code>str</code> <p>Tabulator layout mode.</p> <code>theme</code> <code>str</code> <p>Visual theme for the table.</p> <code>pagination</code> <code>Union[str, bool]</code> <p>Pagination settings.</p> <code>page_size</code> <code>int</code> <p>Number of rows per page when pagination is enabled.</p> <code>movable_columns</code> <code>bool</code> <p>Whether columns can be moved.</p> <code>resizable_columns</code> <code>bool</code> <p>Whether columns can be resized.</p> <code>column_config</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>Custom column configurations.</p> <code>responsive</code> <code>bool</code> <p>Whether table should be responsive.</p> <code>selectable</code> <code>bool</code> <p>Whether rows can be selected.</p> <code>table_id</code> <code>Optional[str]</code> <p>Unique identifier for the table.</p> <code>container_style</code> <code>Optional[Dict[str, str]]</code> <p>Custom CSS styles for container.</p> <code>default_float_precision</code> <code>int | None</code> <p>Default decimal places for float columns.</p> Source code in <code>submodules/mescal/mescal/visualizations/html_table.py</code> <pre><code>class HTMLTable:\n    \"\"\"A class to create interactive Tabulator tables from pandas DataFrames.\n\n    Tabulator (http://tabulator.info/) is a feature-rich interactive table library\n    that provides sorting, filtering, formatting, and editing capabilities.\n\n    This class automatically detects column types and applies appropriate formatters,\n    sorters, and filters. It supports customization through column configuration\n    and provides methods to display tables in Jupyter notebooks or save as HTML files.\n\n    Attributes:\n        df (pd.DataFrame): The pandas DataFrame to display.\n        title (Optional[str]): Optional title for the table.\n        height (str): Height of the table container.\n        layout (str): Tabulator layout mode.\n        theme (str): Visual theme for the table.\n        pagination (Union[str, bool]): Pagination settings.\n        page_size (int): Number of rows per page when pagination is enabled.\n        movable_columns (bool): Whether columns can be moved.\n        resizable_columns (bool): Whether columns can be resized.\n        column_config (Optional[Dict[str, Dict[str, Any]]]): Custom column configurations.\n        responsive (bool): Whether table should be responsive.\n        selectable (bool): Whether rows can be selected.\n        table_id (Optional[str]): Unique identifier for the table.\n        container_style (Optional[Dict[str, str]]): Custom CSS styles for container.\n        default_float_precision (int | None): Default decimal places for float columns.\n    \"\"\"\n\n    def __init__(\n            self,\n            df: pd.DataFrame,\n            title: Optional[str] = None,\n            height: str = \"400px\",\n            layout: str = \"fitColumns\",  # fitColumns, fitData, fitDataFill\n            theme: str = \"simple\",  # simple, bootstrap, midnight, modern, etc.\n            pagination: Union[str, bool] = \"local\",  # local, remote, or False\n            page_size: int = 10,\n            movable_columns: bool = True,\n            resizable_columns: bool = True,\n            column_config: Optional[Dict[str, Dict[str, Any]]] = None,\n            responsive: bool = True,\n            selectable: bool = False,\n            table_id: Optional[str] = None,\n            container_style: Optional[Dict[str, str]] = None,\n            default_float_precision: int | None = 2,\n    ):\n        \"\"\"Initialize the HTMLTable with configuration options.\n\n        Args:\n            df: The pandas DataFrame to display in the table.\n            title: Optional title to display above the table.\n            height: CSS height value for the table container (default: \"400px\").\n            layout: Tabulator layout mode - \"fitColumns\", \"fitData\", or \"fitDataFill\".\n            theme: Visual theme - \"simple\", \"bootstrap\", \"midnight\", \"modern\", etc.\n            pagination: Pagination mode - \"local\", \"remote\", or False to disable.\n            page_size: Number of rows to display per page when pagination is enabled.\n            movable_columns: Whether users can drag columns to reorder them.\n            resizable_columns: Whether users can resize column widths.\n            column_config: Dictionary mapping column names to Tabulator column definitions\n                for custom formatting, validation, or behavior.\n            responsive: Whether the table should adapt to different screen sizes.\n            selectable: Whether users can select table rows.\n            table_id: Custom HTML ID for the table element. If None, generates unique ID.\n            container_style: CSS styles to apply to the table container as key-value pairs.\n            default_float_precision: Number of decimal places for float columns.\n                Set to None to disable automatic formatting.\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; df = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Score': [95.5, 87.2]})\n            &gt;&gt;&gt; table = HTMLTable(df, title=\"Student Scores\", height=\"300px\")\n        \"\"\"\n        self.df = df\n        self.title = title\n        self.height = height\n        self.layout = layout\n        self.theme = theme\n        self.pagination = pagination\n        self.page_size = page_size\n        self.movable_columns = movable_columns\n        self.resizable_columns = resizable_columns\n        self.column_config = column_config or {}\n        self.responsive = responsive\n        self.selectable = selectable\n        self.table_id = table_id or f\"tabulator_{str(uuid.uuid4()).replace('-', '')[:8]}\"\n        self.container_style = container_style or {}\n        self.default_float_precision = default_float_precision\n\n    def _get_column_definitions(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Generate column definitions for Tabulator based on DataFrame columns and types.\n\n        Automatically detects column data types and applies appropriate Tabulator\n        configurations including sorters, formatters, and filters. Custom configurations\n        from column_config override automatic detection.\n\n        Returns:\n            List of column definition dictionaries compatible with Tabulator.\n        \"\"\"\n        column_defs = []\n\n        for col in self.df.columns:\n            # Start with default configuration\n            col_def = {\n                \"title\": str(col),\n                \"field\": str(col),\n                \"headerFilter\": \"input\",\n            }\n\n            if col in self.column_config:\n                col_def.update(self.column_config[col])\n            else:\n                sample_data = self.df[col].dropna().iloc[0] if not self.df[col].dropna().empty else None\n\n                if pd.api.types.is_numeric_dtype(self.df[col]):\n                    col_def[\"sorter\"] = \"number\"\n                    col_def[\"headerFilter\"] = \"number\"\n                    col_def[\"hozAlign\"] = \"right\"\n\n                    if pd.api.types.is_float_dtype(self.df[col]) and (self.default_float_precision is not None):\n                        if col not in self.column_config or \"formatter\" not in self.column_config[col]:\n                            col_def[\"formatter\"] = \"money\"\n                            col_def[\"formatterParams\"] = {\n                                \"symbol\": \"\",\n                                \"precision\": str(self.default_float_precision),\n                                \"thousand\": \",\",\n                                \"decimal\": \".\"\n                            }\n                elif pd.api.types.is_datetime64_dtype(self.df[col]):\n                    col_def[\"sorter\"] = \"date\"\n                    col_def[\"headerFilter\"] = \"input\"\n                    col_def[\"formatter\"] = \"datetime\"\n                    col_def[\"formatterParams\"] = {\"outputFormat\": \"YYYY-MM-DD HH:mm:ss\"}\n                elif isinstance(sample_data, bool) or pd.api.types.is_bool_dtype(self.df[col]):\n                    col_def[\"sorter\"] = \"boolean\"\n                    col_def[\"formatter\"] = \"tickCross\"\n                    col_def[\"headerFilter\"] = \"tickCross\"\n                    col_def[\"hozAlign\"] = \"center\"\n                else:\n                    col_def[\"sorter\"] = \"string\"\n\n            column_defs.append(col_def)\n\n        return column_defs\n\n    def _process_dataframe(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Process DataFrame to handle NaN values and convert to JSON records.\n\n        Converts pandas DataFrame to a format suitable for Tabulator by handling\n        NaN values, ensuring numeric columns are properly typed, and converting\n        to JSON-serializable records.\n\n        Returns:\n            List of dictionaries representing table rows.\n        \"\"\"\n        processed_df = self.df.copy()\n\n        for col in processed_df.columns:\n            if pd.api.types.is_numeric_dtype(processed_df[col]):\n                processed_df[col] = processed_df[col].astype(float)\n            processed_df[col] = processed_df[col].where(~pd.isna(processed_df[col]), None)\n\n        records = processed_df.to_dict(orient='records')\n        return records\n\n    def _get_container_style_string(self) -&gt; str:\n        \"\"\"Convert container style dictionary to CSS string.\n\n        Transforms Python-style CSS property names (snake_case) to CSS format\n        (camelCase) and combines with base styling.\n\n        Returns:\n            CSS style string for the table container.\n        \"\"\"\n        base_style = \"padding: 10px; margin: 15px 0;\"\n\n        for key, value in self.container_style.items():\n            css_key = ''.join(word.capitalize() if i &gt; 0 else word\n                              for i, word in enumerate(key.split('_')))\n            base_style += f\" {css_key}: {value};\"\n\n        return base_style\n\n    def get_html(self, include_dependencies: bool = True) -&gt; str:\n        \"\"\"Generate HTML representation of the interactive table.\n\n        Creates the complete HTML markup including JavaScript initialization\n        for the Tabulator table with all configured options.\n\n        Args:\n            include_dependencies: Whether to include Tabulator CSS/JS dependencies\n                in the output. Set to False when embedding in documents that already\n                include these dependencies.\n\n        Returns:\n            Complete HTML string ready for display or embedding.\n\n        Example:\n\n            &gt;&gt;&gt; table = HTMLTable(df, title=\"My Data\")\n            &gt;&gt;&gt; html_output = table.get_html()\n            &gt;&gt;&gt; # Save to file or display in web application\n        \"\"\"\n        processed_data = self._process_dataframe()\n        column_defs = self._get_column_definitions()\n        container_style = self._get_container_style_string()\n\n        html_parts = []\n\n        if include_dependencies:\n            html_parts.append(\n                \"\"\"\n                &lt;link href=\"https://unpkg.com/tabulator-tables@5.4.4/dist/css/tabulator.min.css\" rel=\"stylesheet\"&gt;\n                &lt;script type=\"text/javascript\" src=\"https://unpkg.com/tabulator-tables@5.4.4/dist/js/tabulator.min.js\"&gt;&lt;/script&gt;\n                \"\"\"\n            )\n\n        html_parts.append(f'&lt;div style=\"{container_style}\"&gt;')\n\n        if self.title:\n            html_parts.append(f'&lt;h3 style=\"margin-bottom: 10px;\"&gt;{self.title}&lt;/h3&gt;')\n\n        html_parts.append(f'&lt;div id=\"{self.table_id}\" style=\"height: {self.height};\"&gt;&lt;/div&gt;')\n\n        html_parts.append(\n            f\"\"\"\n            &lt;script type=\"text/javascript\"&gt;\n                document.addEventListener('DOMContentLoaded', function() {{\n                    var tabledata = {json.dumps(processed_data)};\n\n                    var table = new Tabulator(\"#{self.table_id}\", {{\n                        data: tabledata,\n                        columns: {json.dumps(column_defs)},\n                        layout: \"{self.layout}\",\n                        responsiveLayout: {json.dumps(self.responsive)},\n                        movableColumns: {json.dumps(self.movable_columns)},\n                        resizableColumns: {json.dumps(self.resizable_columns)},\n                        selectable: {json.dumps(self.selectable)},\n                    \"\"\"\n        )\n\n        if self.pagination:\n            html_parts.append(f\"\"\"\n            pagination: \"{self.pagination}\",\n            paginationSize: {self.page_size},\n            paginationSizeSelector: [10, 25, 50, 100, true],\n            \"\"\")\n\n        if self.theme:\n            html_parts.append(f'    theme: \"{self.theme}\",')\n\n        html_parts.append(\n            \"\"\"\n                    });\n                });\n            &lt;/script&gt;\n            \"\"\"\n        )\n        html_parts.append(\"&lt;/div&gt;\")\n\n        return \"\\n\".join(html_parts)\n\n    def save_html(self, filepath: str, title: str = \"Tabulator Table\") -&gt; str:\n        \"\"\"Save the table as a standalone HTML file.\n\n        Creates a complete HTML document with embedded Tabulator dependencies\n        and saves it to the specified path. The resulting file can be opened\n        directly in any web browser.\n\n        Args:\n            filepath: Path where to save the HTML file. Parent directories\n                will be created if they don't exist.\n            title: Title for the HTML document head section.\n\n        Returns:\n            The filepath of the saved file (same as input parameter).\n\n        Example:\n\n            &gt;&gt;&gt; table = HTMLTable(df, title=\"Sales Report\")\n            &gt;&gt;&gt; saved_path = table.save_html(\"reports/sales_table.html\")\n            &gt;&gt;&gt; print(f\"Table saved to: {saved_path}\")\n        \"\"\"\n        os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n\n        html = f\"\"\"&lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;meta charset=\"UTF-8\"&gt;\n            &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n            &lt;title&gt;{title}&lt;/title&gt;\n            &lt;link href=\"https://unpkg.com/tabulator-tables@5.4.4/dist/css/tabulator.min.css\" rel=\"stylesheet\"&gt;\n            &lt;script type=\"text/javascript\" src=\"https://unpkg.com/tabulator-tables@5.4.4/dist/js/tabulator.min.js\"&gt;&lt;/script&gt;\n            &lt;style&gt;\n                body {{\n                    font-family: Arial, sans-serif;\n                    margin: 20px;\n                    background-color: #f5f5f5;\n                }}\n                .container {{\n                    background-color: white;\n                    padding: 20px;\n                    border-radius: 5px;\n                    box-shadow: 0 2px 10px rgba(0,0,0,0.05);\n                }}\n            &lt;/style&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;div class=\"container\"&gt;\n                {self.get_html(include_dependencies=False)}\n            &lt;/div&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n        \"\"\"\n\n        with open(filepath, 'w', encoding='utf-8') as f:\n            f.write(html)\n\n        return filepath\n\n    def show(self, width: str = \"100%\", height: str = \"600\"):\n        \"\"\"Display the interactive Tabulator table inline in a Jupyter notebook.\n\n        Creates a temporary HTML file and displays it using an IPython IFrame.\n        This method is designed for use within Jupyter notebook environments.\n\n        Args:\n            width: Width of the display frame (CSS units or percentage).\n            height: Height of the display frame in pixels (as string).\n\n        Note:\n            This method requires IPython to be available and will only work\n            in Jupyter notebook environments.\n\n        Example:\n\n            &gt;&gt;&gt; # In a Jupyter notebook cell:\n            &gt;&gt;&gt; table = HTMLTable(df, title=\"Interactive Data\")\n            &gt;&gt;&gt; table.show(width=\"800px\", height=\"400\")\n        \"\"\"\n        import tempfile\n        from pathlib import Path\n        from IPython.display import IFrame, display\n\n        tmp_dir = Path(tempfile.mkdtemp())\n        html_path = tmp_dir / f\"{self.table_id}.html\"\n        self.save_html(str(html_path))\n        display(IFrame(src=str(html_path.resolve()), width=width, height=height))\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_table/#mescal.visualizations.html_table.HTMLTable.__init__","title":"__init__","text":"<pre><code>__init__(df: DataFrame, title: Optional[str] = None, height: str = '400px', layout: str = 'fitColumns', theme: str = 'simple', pagination: Union[str, bool] = 'local', page_size: int = 10, movable_columns: bool = True, resizable_columns: bool = True, column_config: Optional[Dict[str, Dict[str, Any]]] = None, responsive: bool = True, selectable: bool = False, table_id: Optional[str] = None, container_style: Optional[Dict[str, str]] = None, default_float_precision: int | None = 2)\n</code></pre> <p>Initialize the HTMLTable with configuration options.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame to display in the table.</p> required <code>title</code> <code>Optional[str]</code> <p>Optional title to display above the table.</p> <code>None</code> <code>height</code> <code>str</code> <p>CSS height value for the table container (default: \"400px\").</p> <code>'400px'</code> <code>layout</code> <code>str</code> <p>Tabulator layout mode - \"fitColumns\", \"fitData\", or \"fitDataFill\".</p> <code>'fitColumns'</code> <code>theme</code> <code>str</code> <p>Visual theme - \"simple\", \"bootstrap\", \"midnight\", \"modern\", etc.</p> <code>'simple'</code> <code>pagination</code> <code>Union[str, bool]</code> <p>Pagination mode - \"local\", \"remote\", or False to disable.</p> <code>'local'</code> <code>page_size</code> <code>int</code> <p>Number of rows to display per page when pagination is enabled.</p> <code>10</code> <code>movable_columns</code> <code>bool</code> <p>Whether users can drag columns to reorder them.</p> <code>True</code> <code>resizable_columns</code> <code>bool</code> <p>Whether users can resize column widths.</p> <code>True</code> <code>column_config</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>Dictionary mapping column names to Tabulator column definitions for custom formatting, validation, or behavior.</p> <code>None</code> <code>responsive</code> <code>bool</code> <p>Whether the table should adapt to different screen sizes.</p> <code>True</code> <code>selectable</code> <code>bool</code> <p>Whether users can select table rows.</p> <code>False</code> <code>table_id</code> <code>Optional[str]</code> <p>Custom HTML ID for the table element. If None, generates unique ID.</p> <code>None</code> <code>container_style</code> <code>Optional[Dict[str, str]]</code> <p>CSS styles to apply to the table container as key-value pairs.</p> <code>None</code> <code>default_float_precision</code> <code>int | None</code> <p>Number of decimal places for float columns. Set to None to disable automatic formatting.</p> <code>2</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Score': [95.5, 87.2]})\n&gt;&gt;&gt; table = HTMLTable(df, title=\"Student Scores\", height=\"300px\")\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/html_table.py</code> <pre><code>def __init__(\n        self,\n        df: pd.DataFrame,\n        title: Optional[str] = None,\n        height: str = \"400px\",\n        layout: str = \"fitColumns\",  # fitColumns, fitData, fitDataFill\n        theme: str = \"simple\",  # simple, bootstrap, midnight, modern, etc.\n        pagination: Union[str, bool] = \"local\",  # local, remote, or False\n        page_size: int = 10,\n        movable_columns: bool = True,\n        resizable_columns: bool = True,\n        column_config: Optional[Dict[str, Dict[str, Any]]] = None,\n        responsive: bool = True,\n        selectable: bool = False,\n        table_id: Optional[str] = None,\n        container_style: Optional[Dict[str, str]] = None,\n        default_float_precision: int | None = 2,\n):\n    \"\"\"Initialize the HTMLTable with configuration options.\n\n    Args:\n        df: The pandas DataFrame to display in the table.\n        title: Optional title to display above the table.\n        height: CSS height value for the table container (default: \"400px\").\n        layout: Tabulator layout mode - \"fitColumns\", \"fitData\", or \"fitDataFill\".\n        theme: Visual theme - \"simple\", \"bootstrap\", \"midnight\", \"modern\", etc.\n        pagination: Pagination mode - \"local\", \"remote\", or False to disable.\n        page_size: Number of rows to display per page when pagination is enabled.\n        movable_columns: Whether users can drag columns to reorder them.\n        resizable_columns: Whether users can resize column widths.\n        column_config: Dictionary mapping column names to Tabulator column definitions\n            for custom formatting, validation, or behavior.\n        responsive: Whether the table should adapt to different screen sizes.\n        selectable: Whether users can select table rows.\n        table_id: Custom HTML ID for the table element. If None, generates unique ID.\n        container_style: CSS styles to apply to the table container as key-value pairs.\n        default_float_precision: Number of decimal places for float columns.\n            Set to None to disable automatic formatting.\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Score': [95.5, 87.2]})\n        &gt;&gt;&gt; table = HTMLTable(df, title=\"Student Scores\", height=\"300px\")\n    \"\"\"\n    self.df = df\n    self.title = title\n    self.height = height\n    self.layout = layout\n    self.theme = theme\n    self.pagination = pagination\n    self.page_size = page_size\n    self.movable_columns = movable_columns\n    self.resizable_columns = resizable_columns\n    self.column_config = column_config or {}\n    self.responsive = responsive\n    self.selectable = selectable\n    self.table_id = table_id or f\"tabulator_{str(uuid.uuid4()).replace('-', '')[:8]}\"\n    self.container_style = container_style or {}\n    self.default_float_precision = default_float_precision\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_table/#mescal.visualizations.html_table.HTMLTable.get_html","title":"get_html","text":"<pre><code>get_html(include_dependencies: bool = True) -&gt; str\n</code></pre> <p>Generate HTML representation of the interactive table.</p> <p>Creates the complete HTML markup including JavaScript initialization for the Tabulator table with all configured options.</p> <p>Parameters:</p> Name Type Description Default <code>include_dependencies</code> <code>bool</code> <p>Whether to include Tabulator CSS/JS dependencies in the output. Set to False when embedding in documents that already include these dependencies.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Complete HTML string ready for display or embedding.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; table = HTMLTable(df, title=\"My Data\")\n&gt;&gt;&gt; html_output = table.get_html()\n&gt;&gt;&gt; # Save to file or display in web application\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/html_table.py</code> <pre><code>def get_html(self, include_dependencies: bool = True) -&gt; str:\n    \"\"\"Generate HTML representation of the interactive table.\n\n    Creates the complete HTML markup including JavaScript initialization\n    for the Tabulator table with all configured options.\n\n    Args:\n        include_dependencies: Whether to include Tabulator CSS/JS dependencies\n            in the output. Set to False when embedding in documents that already\n            include these dependencies.\n\n    Returns:\n        Complete HTML string ready for display or embedding.\n\n    Example:\n\n        &gt;&gt;&gt; table = HTMLTable(df, title=\"My Data\")\n        &gt;&gt;&gt; html_output = table.get_html()\n        &gt;&gt;&gt; # Save to file or display in web application\n    \"\"\"\n    processed_data = self._process_dataframe()\n    column_defs = self._get_column_definitions()\n    container_style = self._get_container_style_string()\n\n    html_parts = []\n\n    if include_dependencies:\n        html_parts.append(\n            \"\"\"\n            &lt;link href=\"https://unpkg.com/tabulator-tables@5.4.4/dist/css/tabulator.min.css\" rel=\"stylesheet\"&gt;\n            &lt;script type=\"text/javascript\" src=\"https://unpkg.com/tabulator-tables@5.4.4/dist/js/tabulator.min.js\"&gt;&lt;/script&gt;\n            \"\"\"\n        )\n\n    html_parts.append(f'&lt;div style=\"{container_style}\"&gt;')\n\n    if self.title:\n        html_parts.append(f'&lt;h3 style=\"margin-bottom: 10px;\"&gt;{self.title}&lt;/h3&gt;')\n\n    html_parts.append(f'&lt;div id=\"{self.table_id}\" style=\"height: {self.height};\"&gt;&lt;/div&gt;')\n\n    html_parts.append(\n        f\"\"\"\n        &lt;script type=\"text/javascript\"&gt;\n            document.addEventListener('DOMContentLoaded', function() {{\n                var tabledata = {json.dumps(processed_data)};\n\n                var table = new Tabulator(\"#{self.table_id}\", {{\n                    data: tabledata,\n                    columns: {json.dumps(column_defs)},\n                    layout: \"{self.layout}\",\n                    responsiveLayout: {json.dumps(self.responsive)},\n                    movableColumns: {json.dumps(self.movable_columns)},\n                    resizableColumns: {json.dumps(self.resizable_columns)},\n                    selectable: {json.dumps(self.selectable)},\n                \"\"\"\n    )\n\n    if self.pagination:\n        html_parts.append(f\"\"\"\n        pagination: \"{self.pagination}\",\n        paginationSize: {self.page_size},\n        paginationSizeSelector: [10, 25, 50, 100, true],\n        \"\"\")\n\n    if self.theme:\n        html_parts.append(f'    theme: \"{self.theme}\",')\n\n    html_parts.append(\n        \"\"\"\n                });\n            });\n        &lt;/script&gt;\n        \"\"\"\n    )\n    html_parts.append(\"&lt;/div&gt;\")\n\n    return \"\\n\".join(html_parts)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_table/#mescal.visualizations.html_table.HTMLTable.save_html","title":"save_html","text":"<pre><code>save_html(filepath: str, title: str = 'Tabulator Table') -&gt; str\n</code></pre> <p>Save the table as a standalone HTML file.</p> <p>Creates a complete HTML document with embedded Tabulator dependencies and saves it to the specified path. The resulting file can be opened directly in any web browser.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path where to save the HTML file. Parent directories will be created if they don't exist.</p> required <code>title</code> <code>str</code> <p>Title for the HTML document head section.</p> <code>'Tabulator Table'</code> <p>Returns:</p> Type Description <code>str</code> <p>The filepath of the saved file (same as input parameter).</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; table = HTMLTable(df, title=\"Sales Report\")\n&gt;&gt;&gt; saved_path = table.save_html(\"reports/sales_table.html\")\n&gt;&gt;&gt; print(f\"Table saved to: {saved_path}\")\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/html_table.py</code> <pre><code>def save_html(self, filepath: str, title: str = \"Tabulator Table\") -&gt; str:\n    \"\"\"Save the table as a standalone HTML file.\n\n    Creates a complete HTML document with embedded Tabulator dependencies\n    and saves it to the specified path. The resulting file can be opened\n    directly in any web browser.\n\n    Args:\n        filepath: Path where to save the HTML file. Parent directories\n            will be created if they don't exist.\n        title: Title for the HTML document head section.\n\n    Returns:\n        The filepath of the saved file (same as input parameter).\n\n    Example:\n\n        &gt;&gt;&gt; table = HTMLTable(df, title=\"Sales Report\")\n        &gt;&gt;&gt; saved_path = table.save_html(\"reports/sales_table.html\")\n        &gt;&gt;&gt; print(f\"Table saved to: {saved_path}\")\n    \"\"\"\n    os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n\n    html = f\"\"\"&lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;meta charset=\"UTF-8\"&gt;\n        &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n        &lt;title&gt;{title}&lt;/title&gt;\n        &lt;link href=\"https://unpkg.com/tabulator-tables@5.4.4/dist/css/tabulator.min.css\" rel=\"stylesheet\"&gt;\n        &lt;script type=\"text/javascript\" src=\"https://unpkg.com/tabulator-tables@5.4.4/dist/js/tabulator.min.js\"&gt;&lt;/script&gt;\n        &lt;style&gt;\n            body {{\n                font-family: Arial, sans-serif;\n                margin: 20px;\n                background-color: #f5f5f5;\n            }}\n            .container {{\n                background-color: white;\n                padding: 20px;\n                border-radius: 5px;\n                box-shadow: 0 2px 10px rgba(0,0,0,0.05);\n            }}\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"container\"&gt;\n            {self.get_html(include_dependencies=False)}\n        &lt;/div&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n\n    with open(filepath, 'w', encoding='utf-8') as f:\n        f.write(html)\n\n    return filepath\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/html_table/#mescal.visualizations.html_table.HTMLTable.show","title":"show","text":"<pre><code>show(width: str = '100%', height: str = '600')\n</code></pre> <p>Display the interactive Tabulator table inline in a Jupyter notebook.</p> <p>Creates a temporary HTML file and displays it using an IPython IFrame. This method is designed for use within Jupyter notebook environments.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>str</code> <p>Width of the display frame (CSS units or percentage).</p> <code>'100%'</code> <code>height</code> <code>str</code> <p>Height of the display frame in pixels (as string).</p> <code>'600'</code> Note <p>This method requires IPython to be available and will only work in Jupyter notebook environments.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # In a Jupyter notebook cell:\n&gt;&gt;&gt; table = HTMLTable(df, title=\"Interactive Data\")\n&gt;&gt;&gt; table.show(width=\"800px\", height=\"400\")\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/html_table.py</code> <pre><code>def show(self, width: str = \"100%\", height: str = \"600\"):\n    \"\"\"Display the interactive Tabulator table inline in a Jupyter notebook.\n\n    Creates a temporary HTML file and displays it using an IPython IFrame.\n    This method is designed for use within Jupyter notebook environments.\n\n    Args:\n        width: Width of the display frame (CSS units or percentage).\n        height: Height of the display frame in pixels (as string).\n\n    Note:\n        This method requires IPython to be available and will only work\n        in Jupyter notebook environments.\n\n    Example:\n\n        &gt;&gt;&gt; # In a Jupyter notebook cell:\n        &gt;&gt;&gt; table = HTMLTable(df, title=\"Interactive Data\")\n        &gt;&gt;&gt; table.show(width=\"800px\", height=\"400\")\n    \"\"\"\n    import tempfile\n    from pathlib import Path\n    from IPython.display import IFrame, display\n\n    tmp_dir = Path(tempfile.mkdtemp())\n    html_path = tmp_dir / f\"{self.table_id}.html\"\n    self.save_html(str(html_path))\n    display(IFrame(src=str(html_path.resolve()), width=width, height=height))\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/","title":"TimeSeries Dashboard (Plotly Figure)","text":""},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.TimeSeriesDashboardGenerator","title":"TimeSeriesDashboardGenerator","text":"<p>Main class for generating timeseries heatmap dashboards.</p> <p>Creates comprehensive dashboards that visualize timeseries data as heatmaps with hour-of-day on y-axis and time aggregations on x-axis. Supports faceting by data columns (MultiIndex) or analysis parameters, customizable KPI statistics, and flexible color schemes including per-facet colorscales.</p> <p>The dashboard displays heatmaps alongside statistical summaries and supports various time aggregations (daily, weekly, monthly) with configurable grouping functions (mean, sum, min, max, etc.).</p> The input data must be a pandas DataFrame or Series with: <ul> <li>DateTime index (required for time-based aggregations)</li> <li>For faceting: MultiIndex columns with named levels</li> </ul> <p>Examples:</p> <p>Basic usage with single variable:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from datetime import datetime, timedelta\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Create sample timeseries data\n&gt;&gt;&gt; dates = pd.date_range('2023-01-01', periods=8760, freq='H')\n&gt;&gt;&gt; data = pd.Series(np.random.randn(8760), index=dates, name='power')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Generate basic dashboard\n&gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(x_axis='date')\n&gt;&gt;&gt; fig = generator.get_figure(data)\n&gt;&gt;&gt; fig.show()\n</code></pre> <p>Multi-variable with faceting:</p> <pre><code>&gt;&gt;&gt; # Create multi-column data with proper MultiIndex\n&gt;&gt;&gt; variables = ['solar', 'wind', 'load']\n&gt;&gt;&gt; scenarios = ['base', 'high', 'low']\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Method 1: Using pd.concat to create MultiIndex\n&gt;&gt;&gt; data_dict = {}\n&gt;&gt;&gt; for scenario in scenarios:\n&gt;&gt;&gt;     scenario_data = pd.DataFrame({\n&gt;&gt;&gt;         var: np.random.randn(8760) for var in variables\n&gt;&gt;&gt;     }, index=dates)\n&gt;&gt;&gt;     data_dict[scenario] = scenario_data\n&gt;&gt;&gt; \n&gt;&gt;&gt; data_multi = pd.concat(data_dict, axis=1, names=['scenario', 'variable'])\n&gt;&gt;&gt; print(data_multi)\n    scenario             base  ...   low\n    variable            solar  wind  load  ...  load\n    datetime\n    2023-01-01 00:00:00 -0.95 -1.57  0.89  ...  0.06\n    2023-01-01 01:00:00  1.18  0.88 -0.62  ...  1.18\n    2023-01-01 02:00:00  0.25  0.31  0.12  ...  0.24\n    2023-01-01 03:00:00 -2.02 -0.59 -0.92  ...  0.45\n    2023-01-01 04:00:00  1.13  0.73 -1.04  ... -0.05\n    ...                   ...   ...   ...  ...   ...\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate dashboard with row and column facets\n&gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n&gt;&gt;&gt;     x_axis='date',\n&gt;&gt;&gt;     facet_row='variable',      # First level of MultiIndex\n&gt;&gt;&gt;     facet_col='scenario',      # Second level of MultiIndex\n&gt;&gt;&gt;     facet_row_order=['solar', 'wind', 'load'],\n&gt;&gt;&gt;     facet_col_order=['base', 'high', 'low']\n&gt;&gt;&gt; )\n&gt;&gt;&gt; fig = generator.get_figure(data_multi)\n</code></pre> <p>Custom KPI statistics:</p> <pre><code>&gt;&gt;&gt; # Define custom aggregation functions\n&gt;&gt;&gt; custom_stats = {\n&gt;&gt;&gt;     'Peak': lambda x: x.max(),\n&gt;&gt;&gt;     'Valley': lambda x: x.min(),\n&gt;&gt;&gt;     'Peak-Valley': lambda x: x.max() - x.min(),\n&gt;&gt;&gt;     'Above 50%': lambda x: (x &gt; x.quantile(0.5)).sum() / len(x) * 100,\n&gt;&gt;&gt;     'Volatility': lambda x: x.std() / x.mean() * 100\n&gt;&gt;&gt; }\n&gt;&gt;&gt; \n&gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n&gt;&gt;&gt;     x_axis='week',\n&gt;&gt;&gt;     stat_aggs=custom_stats,\n&gt;&gt;&gt;     facet_row='variable'\n&gt;&gt;&gt; )\n&gt;&gt;&gt; fig = generator.get_figure(data_multi)\n</code></pre> <p>Parameter-based faceting (multiple x-axis or aggregations):</p> <pre><code>&gt;&gt;&gt; # Compare different time aggregations\n&gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n&gt;&gt;&gt;     x_axis=['date', 'week', 'month'],    # Multiple x-axis types\n&gt;&gt;&gt;     facet_col='x_axis',                  # Facet by x_axis parameter\n&gt;&gt;&gt;     facet_row='variable'\n&gt;&gt;&gt; )\n&gt;&gt;&gt; fig = generator.get_figure(data_multi)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Compare different aggregation methods\n&gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n&gt;&gt;&gt;     x_axis='month',\n&gt;&gt;&gt;     groupby_aggregation=['min', 'mean', 'max'],  # Multiple agg methods\n&gt;&gt;&gt;     facet_col='groupby_aggregation',             # Facet by aggregation\n&gt;&gt;&gt;     facet_row='variable'\n&gt;&gt;&gt; )\n&gt;&gt;&gt; fig = generator.get_figure(data_multi)\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>class TimeSeriesDashboardGenerator:\n    \"\"\"Main class for generating timeseries heatmap dashboards.\n\n    Creates comprehensive dashboards that visualize timeseries data as heatmaps\n    with hour-of-day on y-axis and time aggregations on x-axis. Supports faceting\n    by data columns (MultiIndex) or analysis parameters, customizable KPI statistics, and\n    flexible color schemes including per-facet colorscales.\n\n    The dashboard displays heatmaps alongside statistical summaries and supports\n    various time aggregations (daily, weekly, monthly) with configurable grouping\n    functions (mean, sum, min, max, etc.).\n\n    Expected Data Format: The input data must be a pandas DataFrame or Series with:\n        - DateTime index (required for time-based aggregations)\n        - For faceting: MultiIndex columns with named levels\n\n    Examples:\n        Basic usage with single variable:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from datetime import datetime, timedelta\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Create sample timeseries data\n        &gt;&gt;&gt; dates = pd.date_range('2023-01-01', periods=8760, freq='H')\n        &gt;&gt;&gt; data = pd.Series(np.random.randn(8760), index=dates, name='power')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Generate basic dashboard\n        &gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(x_axis='date')\n        &gt;&gt;&gt; fig = generator.get_figure(data)\n        &gt;&gt;&gt; fig.show()\n\n        Multi-variable with faceting:\n\n        &gt;&gt;&gt; # Create multi-column data with proper MultiIndex\n        &gt;&gt;&gt; variables = ['solar', 'wind', 'load']\n        &gt;&gt;&gt; scenarios = ['base', 'high', 'low']\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Method 1: Using pd.concat to create MultiIndex\n        &gt;&gt;&gt; data_dict = {}\n        &gt;&gt;&gt; for scenario in scenarios:\n        &gt;&gt;&gt;     scenario_data = pd.DataFrame({\n        &gt;&gt;&gt;         var: np.random.randn(8760) for var in variables\n        &gt;&gt;&gt;     }, index=dates)\n        &gt;&gt;&gt;     data_dict[scenario] = scenario_data\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; data_multi = pd.concat(data_dict, axis=1, names=['scenario', 'variable'])\n        &gt;&gt;&gt; print(data_multi)\n            scenario             base  ...   low\n            variable            solar  wind  load  ...  load\n            datetime\n            2023-01-01 00:00:00 -0.95 -1.57  0.89  ...  0.06\n            2023-01-01 01:00:00  1.18  0.88 -0.62  ...  1.18\n            2023-01-01 02:00:00  0.25  0.31  0.12  ...  0.24\n            2023-01-01 03:00:00 -2.02 -0.59 -0.92  ...  0.45\n            2023-01-01 04:00:00  1.13  0.73 -1.04  ... -0.05\n            ...                   ...   ...   ...  ...   ...\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Generate dashboard with row and column facets\n        &gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n        &gt;&gt;&gt;     x_axis='date',\n        &gt;&gt;&gt;     facet_row='variable',      # First level of MultiIndex\n        &gt;&gt;&gt;     facet_col='scenario',      # Second level of MultiIndex\n        &gt;&gt;&gt;     facet_row_order=['solar', 'wind', 'load'],\n        &gt;&gt;&gt;     facet_col_order=['base', 'high', 'low']\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; fig = generator.get_figure(data_multi)\n\n        Custom KPI statistics:\n\n        &gt;&gt;&gt; # Define custom aggregation functions\n        &gt;&gt;&gt; custom_stats = {\n        &gt;&gt;&gt;     'Peak': lambda x: x.max(),\n        &gt;&gt;&gt;     'Valley': lambda x: x.min(),\n        &gt;&gt;&gt;     'Peak-Valley': lambda x: x.max() - x.min(),\n        &gt;&gt;&gt;     'Above 50%': lambda x: (x &gt; x.quantile(0.5)).sum() / len(x) * 100,\n        &gt;&gt;&gt;     'Volatility': lambda x: x.std() / x.mean() * 100\n        &gt;&gt;&gt; }\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n        &gt;&gt;&gt;     x_axis='week',\n        &gt;&gt;&gt;     stat_aggs=custom_stats,\n        &gt;&gt;&gt;     facet_row='variable'\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; fig = generator.get_figure(data_multi)\n\n        Parameter-based faceting (multiple x-axis or aggregations):\n\n        &gt;&gt;&gt; # Compare different time aggregations\n        &gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n        &gt;&gt;&gt;     x_axis=['date', 'week', 'month'],    # Multiple x-axis types\n        &gt;&gt;&gt;     facet_col='x_axis',                  # Facet by x_axis parameter\n        &gt;&gt;&gt;     facet_row='variable'\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; fig = generator.get_figure(data_multi)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Compare different aggregation methods\n        &gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n        &gt;&gt;&gt;     x_axis='month',\n        &gt;&gt;&gt;     groupby_aggregation=['min', 'mean', 'max'],  # Multiple agg methods\n        &gt;&gt;&gt;     facet_col='groupby_aggregation',             # Facet by aggregation\n        &gt;&gt;&gt;     facet_row='variable'\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; fig = generator.get_figure(data_multi)\n    \"\"\"\n    def __init__(\n            self,\n            x_axis: X_AXIS_TYPES = 'date',\n            facet_col: str = None,\n            facet_row: str = None,\n            facet_col_wrap: int = None,\n            facet_col_order: list[str] = None,\n            facet_row_order: list[str] = None,\n            ratio_of_stat_col: float = 0.1,\n            stat_aggs: Dict[str, Callable[[pd.Series], float | int]] = None,\n            groupby_aggregation: GROUPBY_AGG_TYPES = 'mean',\n            title: str = None,\n            color_continuous_scale: str | list[str] | list[tuple[float, str]] = None,\n            color_continuous_midpoint: int | float = None,\n            range_color: tuple[float, float] | list[int | float] = None,\n            per_facet_col_colorscale: bool = False,\n            per_facet_row_colorscale: bool = False,\n            facet_row_color_settings: dict = None,\n            facet_col_color_settings: dict = None,\n            subplots_vertical_spacing: float = None,\n            subplots_horizontal_spacing: float = None,\n            time_series_figure_kwargs: dict = None,\n            stat_figure_kwargs: dict = None,\n            universal_figure_kwargs: dict = None,\n            use_string_for_axis: bool = False,\n            config_cls: type[DashboardConfig] = DashboardConfig,\n            data_processor_cls: type[DataProcessor] = DataProcessor,\n            color_manager_cls: type[ColorManager] = ColorManager,\n            trace_generator_cls: type[TraceGenerator] = TraceGenerator,\n            **figure_kwargs\n    ):\n        \"\"\"Initialize the timeseries dashboard generator.\n\n        Args:\n            x_axis: Time aggregation for x-axis or list for faceting.\n            facet_col: Column faceting specification.\n            facet_row: Row faceting specification.\n            facet_col_wrap: Maximum columns per row in faceted layout.\n            facet_col_order: Custom ordering for column facets.\n            facet_row_order: Custom ordering for row facets.\n            ratio_of_stat_col: Width ratio of statistics column to heatmap.\n            stat_aggs: Custom KPI aggregation functions.\n            groupby_aggregation: Data aggregation method or list for faceting.\n            title: Dashboard title.\n            color_continuous_scale: Plotly colorscale specification.\n            color_continuous_midpoint: Midpoint for diverging colorscales.\n            range_color: Fixed color range for heatmaps.\n            per_facet_col_colorscale: Enable separate colorscales per column facet.\n            per_facet_row_colorscale: Enable separate colorscales per row facet.\n            facet_row_color_settings: Custom color settings per row facet.\n            facet_col_color_settings: Custom color settings per column facet.\n            subplots_vertical_spacing: Vertical spacing between subplots.\n            subplots_horizontal_spacing: Horizontal spacing between subplots.\n            time_series_figure_kwargs: Additional heatmap trace parameters.\n            stat_figure_kwargs: Additional statistics trace parameters.\n            universal_figure_kwargs: Parameters applied to all traces.\n            use_string_for_axis: Convert axis values to strings.\n            config_cls: Configuration class for dependency injection.\n            data_processor_cls: Data processor class for dependency injection.\n            color_manager_cls: Color manager class for dependency injection.\n            trace_generator_cls: Trace generator class for dependency injection.\n            **figure_kwargs: Additional figure-level parameters.\n        \"\"\"\n        self.config = config_cls(\n            x_axis=x_axis,\n            facet_col=facet_col,\n            facet_row=facet_row,\n            facet_col_wrap=facet_col_wrap,\n            facet_col_order=facet_col_order,\n            facet_row_order=facet_row_order,\n            ratio_of_stat_col=ratio_of_stat_col,\n            stat_aggs=stat_aggs,\n            groupby_aggregation=groupby_aggregation,\n            title=title,\n            color_continuous_scale=color_continuous_scale,\n            color_continuous_midpoint=color_continuous_midpoint,\n            range_color=range_color,\n            per_facet_col_colorscale=per_facet_col_colorscale,\n            per_facet_row_colorscale=per_facet_row_colorscale,\n            facet_row_color_settings=facet_row_color_settings,\n            facet_col_color_settings=facet_col_color_settings,\n            subplots_vertical_spacing=subplots_vertical_spacing,\n            subplots_horizontal_spacing=subplots_horizontal_spacing,\n            time_series_figure_kwargs=time_series_figure_kwargs,\n            stat_figure_kwargs=stat_figure_kwargs,\n            universal_figure_kwargs=universal_figure_kwargs,\n            use_string_for_axis=use_string_for_axis,\n            ** figure_kwargs\n        )\n        self.data_processor_cls = data_processor_cls\n        self.color_manager_cls = color_manager_cls\n        self.trace_generator_cls = trace_generator_cls\n\n\n    def get_figure(self, data: pd.DataFrame, **kwargs):\n        \"\"\"Generate a complete dashboard figure from timeseries data.\n\n        Creates a plotly figure containing heatmaps with associated statistics,\n        properly formatted axes, and optional faceting. Applies all configured\n        styling, color schemes, and layout settings.\n\n        Args:\n            data: Input timeseries DataFrame or Series with datetime index.\n            **kwargs: Runtime configuration overrides.\n\n        Returns:\n            Plotly Figure object containing the complete dashboard visualization.\n        \"\"\"\n        original_config = copy.deepcopy(self.config)\n\n        for key, value in kwargs.items():\n            if hasattr(self.config, key):\n                setattr(self.config, key, value)\n\n        if not kwargs.get('_skip_validation', False):\n            self.data_processor_cls.validate_input_data_and_config(data, self.config)\n        data = self.data_processor_cls.prepare_dataframe_for_facet(data.copy(), self.config)\n        data = self.data_processor_cls.ensure_df_has_two_column_levels(data, self.config)\n        self.data_processor_cls.update_facet_config(data, self.config)\n\n        fig = self._create_figure_layout_with_subplots(data)\n\n        self._add_heatmap_and_stat_traces_to_figure(data, fig)\n\n        if self.config.per_facet_col_colorscale:\n            self._add_column_colorscales(data, fig)\n            fig.update_traces(showlegend=False)\n        elif self.config.per_facet_row_colorscale:\n            self._add_row_colorscales(data, fig)\n            fig.update_traces(showlegend=False)\n\n        if self.config.title:\n            fig.update_layout(\n                title=f'&lt;b&gt;{self.config.title}&lt;/b&gt;',\n                title_x=0.5,\n            )\n\n        self.config = original_config\n\n        return fig\n\n    def get_figures_chunked(\n            self,\n            data: pd.DataFrame,\n            max_n_rows_per_figure: int = None,\n            n_figures: int = None,\n            chunk_title_suffix: bool = True,\n            **kwargs\n    ) -&gt; list[go.Figure]:\n        \"\"\"Generate multiple figures by splitting facet rows into chunks.\n\n        Useful for handling large datasets with many row facets by creating\n        multiple smaller figures instead of one large figure.\n\n        Args:\n            data: Input timeseries DataFrame with datetime index.\n            max_n_rows_per_figure: Maximum number of row facets per figure.\n            n_figures: Total number of figures to create (alternative to max_n_rows_per_figure).\n            chunk_title_suffix: Whether to add \"(Part X/Y)\" suffix to titles.\n            **kwargs: Runtime configuration overrides.\n\n        Returns:\n            List of plotly Figure objects, each containing a subset of row facets.\n\n        Raises:\n            ValueError: If both or neither of max_n_rows_per_figure and n_figures are provided.\n        \"\"\"\n        original_config = copy.deepcopy(self.config)\n\n        if sum(x is not None for x in [max_n_rows_per_figure, n_figures]) != 1:\n            raise ValueError(\"Provide exactly one of: max_n_rows_per_figure or n_figures\")\n\n        for key, value in kwargs.items():\n            if hasattr(self.config, key):\n                setattr(self.config, key, value)\n\n        data = self.data_processor_cls.prepare_dataframe_for_facet(data, self.config)\n        data = self.data_processor_cls.ensure_df_has_two_column_levels(data, self.config)\n        self.data_processor_cls.update_facet_config(data, self.config)\n\n        if self.config.facet_row is None:\n            return [self.get_figure(data, **kwargs)]\n\n        total_rows = len(self.config.facet_row_order)\n\n        if max_n_rows_per_figure:\n            n_chunks = math.ceil(total_rows / max_n_rows_per_figure)\n            chunk_size = max_n_rows_per_figure\n        else:\n            n_chunks = n_figures\n            chunk_size = math.ceil(total_rows / n_figures)\n\n        figures = []\n        original_title = self.config.title\n\n        for i in range(n_chunks):\n            start_idx = i * chunk_size\n            end_idx = min(start_idx + chunk_size, total_rows)\n            chunk_rows = self.config.facet_row_order[start_idx:end_idx]\n\n            if not chunk_rows:\n                continue\n\n            chunk_kwargs = kwargs.copy()\n            chunk_kwargs['facet_row_order'] = chunk_rows\n\n            if chunk_title_suffix and original_title:\n                chunk_kwargs['title'] = f\"{original_title} (Part {i + 1}/{n_chunks})\"\n            elif chunk_title_suffix:\n                chunk_kwargs['title'] = f\"Part {i + 1}/{n_chunks}\"\n\n            cols_in_chunk = [\n                c for c, facet_row_category in\n                zip(data.columns, data.columns.get_level_values(self.config.facet_row))\n                if facet_row_category in chunk_rows\n            ]\n            data_chunk = data[cols_in_chunk]\n\n            fig = self.get_figure(data_chunk, **chunk_kwargs, _skip_validation=True)\n            figures.append(fig)\n\n        self.config = original_config\n\n        return figures\n\n    def _create_figure_layout_with_subplots(self, data: pd.DataFrame) -&gt; go.Figure:\n        facet_col_wrap = max([1, self.config.facet_col_wrap])\n        ratio_of_stat_col = self.config.ratio_of_stat_col\n\n        has_colorscale_col = self.config.per_facet_row_colorscale\n        has_colorscale_row = self.config.per_facet_col_colorscale\n\n        num_facet_rows = max([1, len(self.config.facet_row_order)])\n        num_facet_cols = max([1, len(self.config.facet_col_order)])\n\n        num_rows = math.ceil(num_facet_cols / facet_col_wrap) * num_facet_rows\n        num_cols = facet_col_wrap * 2  # Each facet gets a heatmap + stats column\n\n        if has_colorscale_col:\n            num_cols += 1\n        if has_colorscale_row:\n            num_rows += 1\n\n        subplot_titles = self._generate_subplot_titles(has_colorscale_col, has_colorscale_row)\n        column_widths = self._get_column_widths(facet_col_wrap, has_colorscale_col, ratio_of_stat_col)\n        row_heights = self._get_row_heights(has_colorscale_row, num_rows)\n        specs = [[{} for _ in range(num_cols)] for _ in range(num_rows)]\n\n        fig = make_subplots(\n            rows=num_rows,\n            cols=num_cols,\n            subplot_titles=subplot_titles,\n            column_widths=column_widths,\n            row_heights=row_heights,\n            specs=specs,\n            vertical_spacing=self.config.subplots_vertical_spacing,\n            horizontal_spacing=self.config.subplots_horizontal_spacing,\n        )\n        fig.update_layout(\n            plot_bgcolor='rgba(0, 0, 0, 0)',\n            margin=dict(t=50, b=50)\n        )\n\n        return fig\n\n    def _get_row_heights(self, has_colorscale_row: bool, num_rows: int) -&gt; list[float]:\n        row_heights = None\n        if has_colorscale_row:\n            regular_height = 1.0\n            colorscale_height = 0.15\n\n            total_regular_rows = num_rows - 1\n            total_height = total_regular_rows * regular_height + colorscale_height\n            norm_regular = regular_height / total_height\n            norm_colorscale = colorscale_height / total_height\n\n            row_heights = [norm_regular] * total_regular_rows + [norm_colorscale]\n        return row_heights\n\n    def _get_column_widths(self, facet_col_wrap, has_colorscale_col, ratio_of_stat_col) -&gt; list[float]:\n        if has_colorscale_col:\n            colorscale_width = 0.03\n            adjusted_width = 1 - colorscale_width\n            column_widths = []\n\n            for _ in range(facet_col_wrap):\n                heatmap_width = (adjusted_width - ratio_of_stat_col) / facet_col_wrap\n                stats_width = ratio_of_stat_col / facet_col_wrap\n                column_widths.extend([heatmap_width, stats_width])\n\n            column_widths.append(colorscale_width)\n        else:\n            column_widths = [(1 - ratio_of_stat_col) / facet_col_wrap, ratio_of_stat_col / facet_col_wrap] * facet_col_wrap\n        return column_widths\n\n    def _generate_subplot_titles(self, has_colorscale_col, has_colorscale_row):\n        subplot_titles = []\n        for row_name in self.config.facet_row_order:\n            for col_name in self.config.facet_col_order:\n                if row_name and col_name:\n                    title = f'{row_name} - {col_name}'\n                else:\n                    title = row_name or col_name\n                subplot_titles.append(title)  # Title for heatmap\n                subplot_titles.append(None)  # Title for stats\n\n            if has_colorscale_col:\n                subplot_titles.append(row_name)\n\n        if has_colorscale_row:\n            for col_name in self.config.facet_col_order:\n                subplot_titles.append(col_name)\n                subplot_titles.append(None)\n        return subplot_titles\n\n    def _add_heatmap_and_stat_traces_to_figure(self, data, fig):\n        facet_col_wrap = self.config.facet_col_wrap\n\n        disable_main_colorbars = self.config.per_facet_col_colorscale or self.config.per_facet_row_colorscale\n        if disable_main_colorbars:\n            self.config.time_series_figure_kwargs['showscale'] = False\n\n        global_color_params = {}\n        if not (self.config.per_facet_col_colorscale or self.config.per_facet_row_colorscale):\n            global_color_params = self.color_manager_cls.compute_color_params(data, self.config)\n\n        current_row = 1\n        row_offset = 0\n\n        for row_idx, row_key in enumerate(self.config.facet_row_order):\n            col_offset = 0\n            for col_idx, col_key in enumerate(self.config.facet_col_order):\n                facet_pos = col_idx % facet_col_wrap\n                if facet_pos == 0 and col_idx &gt; 0:\n                    row_offset += 1\n\n                fig_row = current_row + row_offset\n                fig_col = col_offset + facet_pos * 2 + 1  # +1 because plotly indexing starts at 1\n\n                data_col = facet_key = (row_key, col_key)\n                if data_col not in data.columns:\n                    continue\n                series = data[data_col]\n\n                x_axis = self._get_effective_param_for_data_col('x_axis', data_col)\n                groupby_aggregation = self._get_effective_param_for_data_col('groupby_aggregation', data_col)\n\n                self._set_hovertemplates(x_axis)\n\n                grouped_data = self.data_processor_cls.get_grouped_data(series, x_axis, groupby_aggregation)\n\n                color_params = self._get_color_params_for_facet(data, facet_key, global_color_params)\n\n                show_colorbar = False\n                if not disable_main_colorbars:\n                    show_colorbar = (row_idx == 0 and col_idx == 0)\n\n                heatmap_trace = self.trace_generator_cls.get_heatmap_trace(\n                    grouped_data,\n                    self.config.time_series_figure_kwargs,\n                    color_params,\n                    showscale=show_colorbar,\n                    use_string_for_axis=self.config.use_string_for_axis,\n                )\n\n                fig.add_trace(heatmap_trace, row=fig_row, col=fig_col)\n\n                fig.update_yaxes(\n                    tickvals=[time(hour=h, minute=0) for h in [0, 6, 12, 18]] + [max(grouped_data.index)],\n                    ticktext=['0', '6', '12', '18', '24'],\n                    row=fig_row,\n                    col=fig_col,\n                    autorange='reversed',\n                )\n\n                if x_axis == 'year_week':\n                    fig.update_xaxes(dtick=8, row=fig_row, col=fig_col)\n\n                stats_trace = self.trace_generator_cls.get_stats_trace(\n                    series,\n                    self.config.stat_aggs,\n                    self.config.stat_figure_kwargs,\n                    color_params\n                )\n\n                fig.add_trace(stats_trace, row=fig_row, col=fig_col + 1)\n\n                fig.update_xaxes(showgrid=False, row=fig_row, col=fig_col + 1)\n                fig.update_yaxes(showgrid=False, autorange='reversed', row=fig_row, col=fig_col + 1)\n\n            if col_offset == 0:\n                current_row += math.ceil(len(self.config.facet_col_order) / facet_col_wrap)\n\n    def _get_color_params_for_facet(self, data: pd.DataFrame, facet_key: tuple[str, str], global_color_params: dict) -&gt; dict:\n        if self.config.per_facet_col_colorscale or self.config.per_facet_row_colorscale:\n            color_params = self.color_manager_cls.compute_color_params(data, self.config, facet_key)\n        else:\n            color_params = global_color_params\n        return color_params\n\n    def _add_row_colorscales(self, data, fig):\n        colorscale_col = self.config.facet_col_wrap * 2 + 1  # Column after all heatmaps and stats\n\n        for row_idx, row_key in enumerate(self.config.facet_row_order):\n            row_pos = row_idx * math.ceil(len(self.config.facet_col_order) / self.config.facet_col_wrap) + 1\n\n            facet_key = (row_key, self.config.facet_col_order[0])\n            colorscale, z_max, z_min = self._get_color_settings_for_category(data, facet_key)\n            colorscale_trace = self.trace_generator_cls.create_colorscale_trace(\n                z_min, z_max, colorscale, 'v', row_key\n            )\n\n            fig.add_trace(colorscale_trace, row=row_pos, col=colorscale_col)\n            fig.update_xaxes(showticklabels=False, showgrid=False, row=row_pos, col=colorscale_col)\n            fig.update_yaxes(showticklabels=True, showgrid=False, row=row_pos, col=colorscale_col, side='right')\n\n    def _get_color_settings_for_category(self, data, facet_key):\n        color_params = self.color_manager_cls.compute_color_params(data, self.config, facet_key)\n        colorscale = color_params.get('colorscale', 'viridis')\n        z_min = color_params.get('zmin', 0)\n        z_max = color_params.get('zmax', 1)\n        return colorscale, z_max, z_min\n\n    def _add_column_colorscales(self, data, fig):\n        colorscale_row = math.ceil(len(self.config.facet_col_order) / self.config.facet_col_wrap) * len(\n            self.config.facet_row_order) + 1\n\n        for col_idx, col_key in enumerate(self.config.facet_col_order):\n            col_pos = (col_idx % self.config.facet_col_wrap) * 2 + 1\n\n            facet_key = (self.config.facet_row_order[0], col_key)\n\n            colorscale, z_max, z_min = self._get_color_settings_for_category(data, facet_key)\n\n            colorscale_trace = self.trace_generator_cls.create_colorscale_trace(\n                z_min, z_max, colorscale, 'h', col_key\n            )\n\n            fig.add_trace(colorscale_trace, row=colorscale_row, col=col_pos)\n            fig.update_xaxes(showticklabels=True, showgrid=False, row=colorscale_row, col=col_pos)\n            fig.update_yaxes(showticklabels=False, showgrid=False, row=colorscale_row, col=col_pos)\n\n    def _get_effective_param_for_data_col(self, param_name, data_col):\n        param_value = getattr(self.config, param_name)\n        if not isinstance(param_value, list):\n            return param_value\n        else:\n            return list(set(param_value).intersection(list(data_col)))[0]\n\n    def _set_hovertemplates(self, x_axis):\n        ts_kwargs = self.config.time_series_figure_kwargs\n        stat_kwargs = self.config.stat_figure_kwargs\n\n        ts_kwargs['hovertemplate'] = f\"{x_axis}: %{{x}}&lt;br&gt;Hour of day: %{{y}}&lt;br&gt;Value: %{{z}}&lt;extra&gt;&lt;/extra&gt;\"\n        stat_kwargs['hovertemplate'] = f\"aggregation: %{{y}}&lt;br&gt;Value: %{{z}}&lt;extra&gt;&lt;/extra&gt;\"\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.TimeSeriesDashboardGenerator.__init__","title":"__init__","text":"<pre><code>__init__(x_axis: X_AXIS_TYPES = 'date', facet_col: str = None, facet_row: str = None, facet_col_wrap: int = None, facet_col_order: list[str] = None, facet_row_order: list[str] = None, ratio_of_stat_col: float = 0.1, stat_aggs: Dict[str, Callable[[Series], float | int]] = None, groupby_aggregation: GROUPBY_AGG_TYPES = 'mean', title: str = None, color_continuous_scale: str | list[str] | list[tuple[float, str]] = None, color_continuous_midpoint: int | float = None, range_color: tuple[float, float] | list[int | float] = None, per_facet_col_colorscale: bool = False, per_facet_row_colorscale: bool = False, facet_row_color_settings: dict = None, facet_col_color_settings: dict = None, subplots_vertical_spacing: float = None, subplots_horizontal_spacing: float = None, time_series_figure_kwargs: dict = None, stat_figure_kwargs: dict = None, universal_figure_kwargs: dict = None, use_string_for_axis: bool = False, config_cls: type[DashboardConfig] = DashboardConfig, data_processor_cls: type[DataProcessor] = DataProcessor, color_manager_cls: type[ColorManager] = ColorManager, trace_generator_cls: type[TraceGenerator] = TraceGenerator, **figure_kwargs)\n</code></pre> <p>Initialize the timeseries dashboard generator.</p> <p>Parameters:</p> Name Type Description Default <code>x_axis</code> <code>X_AXIS_TYPES</code> <p>Time aggregation for x-axis or list for faceting.</p> <code>'date'</code> <code>facet_col</code> <code>str</code> <p>Column faceting specification.</p> <code>None</code> <code>facet_row</code> <code>str</code> <p>Row faceting specification.</p> <code>None</code> <code>facet_col_wrap</code> <code>int</code> <p>Maximum columns per row in faceted layout.</p> <code>None</code> <code>facet_col_order</code> <code>list[str]</code> <p>Custom ordering for column facets.</p> <code>None</code> <code>facet_row_order</code> <code>list[str]</code> <p>Custom ordering for row facets.</p> <code>None</code> <code>ratio_of_stat_col</code> <code>float</code> <p>Width ratio of statistics column to heatmap.</p> <code>0.1</code> <code>stat_aggs</code> <code>Dict[str, Callable[[Series], float | int]]</code> <p>Custom KPI aggregation functions.</p> <code>None</code> <code>groupby_aggregation</code> <code>GROUPBY_AGG_TYPES</code> <p>Data aggregation method or list for faceting.</p> <code>'mean'</code> <code>title</code> <code>str</code> <p>Dashboard title.</p> <code>None</code> <code>color_continuous_scale</code> <code>str | list[str] | list[tuple[float, str]]</code> <p>Plotly colorscale specification.</p> <code>None</code> <code>color_continuous_midpoint</code> <code>int | float</code> <p>Midpoint for diverging colorscales.</p> <code>None</code> <code>range_color</code> <code>tuple[float, float] | list[int | float]</code> <p>Fixed color range for heatmaps.</p> <code>None</code> <code>per_facet_col_colorscale</code> <code>bool</code> <p>Enable separate colorscales per column facet.</p> <code>False</code> <code>per_facet_row_colorscale</code> <code>bool</code> <p>Enable separate colorscales per row facet.</p> <code>False</code> <code>facet_row_color_settings</code> <code>dict</code> <p>Custom color settings per row facet.</p> <code>None</code> <code>facet_col_color_settings</code> <code>dict</code> <p>Custom color settings per column facet.</p> <code>None</code> <code>subplots_vertical_spacing</code> <code>float</code> <p>Vertical spacing between subplots.</p> <code>None</code> <code>subplots_horizontal_spacing</code> <code>float</code> <p>Horizontal spacing between subplots.</p> <code>None</code> <code>time_series_figure_kwargs</code> <code>dict</code> <p>Additional heatmap trace parameters.</p> <code>None</code> <code>stat_figure_kwargs</code> <code>dict</code> <p>Additional statistics trace parameters.</p> <code>None</code> <code>universal_figure_kwargs</code> <code>dict</code> <p>Parameters applied to all traces.</p> <code>None</code> <code>use_string_for_axis</code> <code>bool</code> <p>Convert axis values to strings.</p> <code>False</code> <code>config_cls</code> <code>type[DashboardConfig]</code> <p>Configuration class for dependency injection.</p> <code>DashboardConfig</code> <code>data_processor_cls</code> <code>type[DataProcessor]</code> <p>Data processor class for dependency injection.</p> <code>DataProcessor</code> <code>color_manager_cls</code> <code>type[ColorManager]</code> <p>Color manager class for dependency injection.</p> <code>ColorManager</code> <code>trace_generator_cls</code> <code>type[TraceGenerator]</code> <p>Trace generator class for dependency injection.</p> <code>TraceGenerator</code> <code>**figure_kwargs</code> <p>Additional figure-level parameters.</p> <code>{}</code> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>def __init__(\n        self,\n        x_axis: X_AXIS_TYPES = 'date',\n        facet_col: str = None,\n        facet_row: str = None,\n        facet_col_wrap: int = None,\n        facet_col_order: list[str] = None,\n        facet_row_order: list[str] = None,\n        ratio_of_stat_col: float = 0.1,\n        stat_aggs: Dict[str, Callable[[pd.Series], float | int]] = None,\n        groupby_aggregation: GROUPBY_AGG_TYPES = 'mean',\n        title: str = None,\n        color_continuous_scale: str | list[str] | list[tuple[float, str]] = None,\n        color_continuous_midpoint: int | float = None,\n        range_color: tuple[float, float] | list[int | float] = None,\n        per_facet_col_colorscale: bool = False,\n        per_facet_row_colorscale: bool = False,\n        facet_row_color_settings: dict = None,\n        facet_col_color_settings: dict = None,\n        subplots_vertical_spacing: float = None,\n        subplots_horizontal_spacing: float = None,\n        time_series_figure_kwargs: dict = None,\n        stat_figure_kwargs: dict = None,\n        universal_figure_kwargs: dict = None,\n        use_string_for_axis: bool = False,\n        config_cls: type[DashboardConfig] = DashboardConfig,\n        data_processor_cls: type[DataProcessor] = DataProcessor,\n        color_manager_cls: type[ColorManager] = ColorManager,\n        trace_generator_cls: type[TraceGenerator] = TraceGenerator,\n        **figure_kwargs\n):\n    \"\"\"Initialize the timeseries dashboard generator.\n\n    Args:\n        x_axis: Time aggregation for x-axis or list for faceting.\n        facet_col: Column faceting specification.\n        facet_row: Row faceting specification.\n        facet_col_wrap: Maximum columns per row in faceted layout.\n        facet_col_order: Custom ordering for column facets.\n        facet_row_order: Custom ordering for row facets.\n        ratio_of_stat_col: Width ratio of statistics column to heatmap.\n        stat_aggs: Custom KPI aggregation functions.\n        groupby_aggregation: Data aggregation method or list for faceting.\n        title: Dashboard title.\n        color_continuous_scale: Plotly colorscale specification.\n        color_continuous_midpoint: Midpoint for diverging colorscales.\n        range_color: Fixed color range for heatmaps.\n        per_facet_col_colorscale: Enable separate colorscales per column facet.\n        per_facet_row_colorscale: Enable separate colorscales per row facet.\n        facet_row_color_settings: Custom color settings per row facet.\n        facet_col_color_settings: Custom color settings per column facet.\n        subplots_vertical_spacing: Vertical spacing between subplots.\n        subplots_horizontal_spacing: Horizontal spacing between subplots.\n        time_series_figure_kwargs: Additional heatmap trace parameters.\n        stat_figure_kwargs: Additional statistics trace parameters.\n        universal_figure_kwargs: Parameters applied to all traces.\n        use_string_for_axis: Convert axis values to strings.\n        config_cls: Configuration class for dependency injection.\n        data_processor_cls: Data processor class for dependency injection.\n        color_manager_cls: Color manager class for dependency injection.\n        trace_generator_cls: Trace generator class for dependency injection.\n        **figure_kwargs: Additional figure-level parameters.\n    \"\"\"\n    self.config = config_cls(\n        x_axis=x_axis,\n        facet_col=facet_col,\n        facet_row=facet_row,\n        facet_col_wrap=facet_col_wrap,\n        facet_col_order=facet_col_order,\n        facet_row_order=facet_row_order,\n        ratio_of_stat_col=ratio_of_stat_col,\n        stat_aggs=stat_aggs,\n        groupby_aggregation=groupby_aggregation,\n        title=title,\n        color_continuous_scale=color_continuous_scale,\n        color_continuous_midpoint=color_continuous_midpoint,\n        range_color=range_color,\n        per_facet_col_colorscale=per_facet_col_colorscale,\n        per_facet_row_colorscale=per_facet_row_colorscale,\n        facet_row_color_settings=facet_row_color_settings,\n        facet_col_color_settings=facet_col_color_settings,\n        subplots_vertical_spacing=subplots_vertical_spacing,\n        subplots_horizontal_spacing=subplots_horizontal_spacing,\n        time_series_figure_kwargs=time_series_figure_kwargs,\n        stat_figure_kwargs=stat_figure_kwargs,\n        universal_figure_kwargs=universal_figure_kwargs,\n        use_string_for_axis=use_string_for_axis,\n        ** figure_kwargs\n    )\n    self.data_processor_cls = data_processor_cls\n    self.color_manager_cls = color_manager_cls\n    self.trace_generator_cls = trace_generator_cls\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.TimeSeriesDashboardGenerator.get_figure","title":"get_figure","text":"<pre><code>get_figure(data: DataFrame, **kwargs)\n</code></pre> <p>Generate a complete dashboard figure from timeseries data.</p> <p>Creates a plotly figure containing heatmaps with associated statistics, properly formatted axes, and optional faceting. Applies all configured styling, color schemes, and layout settings.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input timeseries DataFrame or Series with datetime index.</p> required <code>**kwargs</code> <p>Runtime configuration overrides.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Plotly Figure object containing the complete dashboard visualization.</p> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>def get_figure(self, data: pd.DataFrame, **kwargs):\n    \"\"\"Generate a complete dashboard figure from timeseries data.\n\n    Creates a plotly figure containing heatmaps with associated statistics,\n    properly formatted axes, and optional faceting. Applies all configured\n    styling, color schemes, and layout settings.\n\n    Args:\n        data: Input timeseries DataFrame or Series with datetime index.\n        **kwargs: Runtime configuration overrides.\n\n    Returns:\n        Plotly Figure object containing the complete dashboard visualization.\n    \"\"\"\n    original_config = copy.deepcopy(self.config)\n\n    for key, value in kwargs.items():\n        if hasattr(self.config, key):\n            setattr(self.config, key, value)\n\n    if not kwargs.get('_skip_validation', False):\n        self.data_processor_cls.validate_input_data_and_config(data, self.config)\n    data = self.data_processor_cls.prepare_dataframe_for_facet(data.copy(), self.config)\n    data = self.data_processor_cls.ensure_df_has_two_column_levels(data, self.config)\n    self.data_processor_cls.update_facet_config(data, self.config)\n\n    fig = self._create_figure_layout_with_subplots(data)\n\n    self._add_heatmap_and_stat_traces_to_figure(data, fig)\n\n    if self.config.per_facet_col_colorscale:\n        self._add_column_colorscales(data, fig)\n        fig.update_traces(showlegend=False)\n    elif self.config.per_facet_row_colorscale:\n        self._add_row_colorscales(data, fig)\n        fig.update_traces(showlegend=False)\n\n    if self.config.title:\n        fig.update_layout(\n            title=f'&lt;b&gt;{self.config.title}&lt;/b&gt;',\n            title_x=0.5,\n        )\n\n    self.config = original_config\n\n    return fig\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.TimeSeriesDashboardGenerator.get_figures_chunked","title":"get_figures_chunked","text":"<pre><code>get_figures_chunked(data: DataFrame, max_n_rows_per_figure: int = None, n_figures: int = None, chunk_title_suffix: bool = True, **kwargs) -&gt; list[Figure]\n</code></pre> <p>Generate multiple figures by splitting facet rows into chunks.</p> <p>Useful for handling large datasets with many row facets by creating multiple smaller figures instead of one large figure.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input timeseries DataFrame with datetime index.</p> required <code>max_n_rows_per_figure</code> <code>int</code> <p>Maximum number of row facets per figure.</p> <code>None</code> <code>n_figures</code> <code>int</code> <p>Total number of figures to create (alternative to max_n_rows_per_figure).</p> <code>None</code> <code>chunk_title_suffix</code> <code>bool</code> <p>Whether to add \"(Part X/Y)\" suffix to titles.</p> <code>True</code> <code>**kwargs</code> <p>Runtime configuration overrides.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Figure]</code> <p>List of plotly Figure objects, each containing a subset of row facets.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both or neither of max_n_rows_per_figure and n_figures are provided.</p> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>def get_figures_chunked(\n        self,\n        data: pd.DataFrame,\n        max_n_rows_per_figure: int = None,\n        n_figures: int = None,\n        chunk_title_suffix: bool = True,\n        **kwargs\n) -&gt; list[go.Figure]:\n    \"\"\"Generate multiple figures by splitting facet rows into chunks.\n\n    Useful for handling large datasets with many row facets by creating\n    multiple smaller figures instead of one large figure.\n\n    Args:\n        data: Input timeseries DataFrame with datetime index.\n        max_n_rows_per_figure: Maximum number of row facets per figure.\n        n_figures: Total number of figures to create (alternative to max_n_rows_per_figure).\n        chunk_title_suffix: Whether to add \"(Part X/Y)\" suffix to titles.\n        **kwargs: Runtime configuration overrides.\n\n    Returns:\n        List of plotly Figure objects, each containing a subset of row facets.\n\n    Raises:\n        ValueError: If both or neither of max_n_rows_per_figure and n_figures are provided.\n    \"\"\"\n    original_config = copy.deepcopy(self.config)\n\n    if sum(x is not None for x in [max_n_rows_per_figure, n_figures]) != 1:\n        raise ValueError(\"Provide exactly one of: max_n_rows_per_figure or n_figures\")\n\n    for key, value in kwargs.items():\n        if hasattr(self.config, key):\n            setattr(self.config, key, value)\n\n    data = self.data_processor_cls.prepare_dataframe_for_facet(data, self.config)\n    data = self.data_processor_cls.ensure_df_has_two_column_levels(data, self.config)\n    self.data_processor_cls.update_facet_config(data, self.config)\n\n    if self.config.facet_row is None:\n        return [self.get_figure(data, **kwargs)]\n\n    total_rows = len(self.config.facet_row_order)\n\n    if max_n_rows_per_figure:\n        n_chunks = math.ceil(total_rows / max_n_rows_per_figure)\n        chunk_size = max_n_rows_per_figure\n    else:\n        n_chunks = n_figures\n        chunk_size = math.ceil(total_rows / n_figures)\n\n    figures = []\n    original_title = self.config.title\n\n    for i in range(n_chunks):\n        start_idx = i * chunk_size\n        end_idx = min(start_idx + chunk_size, total_rows)\n        chunk_rows = self.config.facet_row_order[start_idx:end_idx]\n\n        if not chunk_rows:\n            continue\n\n        chunk_kwargs = kwargs.copy()\n        chunk_kwargs['facet_row_order'] = chunk_rows\n\n        if chunk_title_suffix and original_title:\n            chunk_kwargs['title'] = f\"{original_title} (Part {i + 1}/{n_chunks})\"\n        elif chunk_title_suffix:\n            chunk_kwargs['title'] = f\"Part {i + 1}/{n_chunks}\"\n\n        cols_in_chunk = [\n            c for c, facet_row_category in\n            zip(data.columns, data.columns.get_level_values(self.config.facet_row))\n            if facet_row_category in chunk_rows\n        ]\n        data_chunk = data[cols_in_chunk]\n\n        fig = self.get_figure(data_chunk, **chunk_kwargs, _skip_validation=True)\n        figures.append(fig)\n\n    self.config = original_config\n\n    return figures\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.DashboardConfig","title":"DashboardConfig","text":"<p>Configuration class for timeseries dashboard visualization.</p> <p>Manages all configuration parameters for generating heatmap-based timeseries dashboards with customizable statistics, faceting, and color schemes.</p> Custom KPI Statistics <p>You can define custom KPIs by providing a dictionary of functions that operate on pandas Series. Each function should take a Series and return a single numeric value.</p> <p>KPI Customization Example:</p> <pre><code>&gt;&gt;&gt; custom_kpis = {\n...     'Peak Load': lambda x: x.max(),\n...     'Capacity Factor': lambda x: x.mean() / x.max() * 100,\n...     'Ramp Rate': lambda x: x.diff().abs().max(),\n...     'Hours Above Mean': lambda x: (x &gt; x.mean()).sum(),\n...     'Volatility': lambda x: x.std() / x.mean() * 100 if x.mean() != 0 else 0\n... }\n</code></pre> <p>Available built-in statistics are provided in DEFAULT_STATISTICS and STATISTICS_LIBRARY class attributes.</p> Data Format Requirements <p>Input data must be a pandas DataFrame or Series with: - DatetimeIndex (hourly or sub-hourly recommended) - For faceting: MultiIndex columns with named levels - Column names will be used as facet category labels</p> <p>MultiIndex structure for faceting:</p> <pre><code>&gt;&gt;&gt; # Two-level MultiIndex example\n&gt;&gt;&gt; data.columns = pd.MultiIndex.from_tuples([\n&gt;&gt;&gt;     ('scenario1', 'solar'), ('scenario1', 'wind'),\n&gt;&gt;&gt;     ('scenario2', 'solar'), ('scenario2', 'wind')\n&gt;&gt;&gt; ], names=['scenario', 'technology'])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use column level names for faceting\n&gt;&gt;&gt; config = DashboardConfig(\n...     facet_row='technology',  # Use 'technology' level\n...     facet_col='scenario'     # Use 'scenario' level\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>class DashboardConfig:\n    \"\"\"Configuration class for timeseries dashboard visualization.\n\n    Manages all configuration parameters for generating heatmap-based timeseries\n    dashboards with customizable statistics, faceting, and color schemes.\n\n    Custom KPI Statistics:\n        You can define custom KPIs by providing a dictionary of functions that\n        operate on pandas Series. Each function should take a Series and return\n        a single numeric value.\n\n        KPI Customization Example:\n\n            &gt;&gt;&gt; custom_kpis = {\n            ...     'Peak Load': lambda x: x.max(),\n            ...     'Capacity Factor': lambda x: x.mean() / x.max() * 100,\n            ...     'Ramp Rate': lambda x: x.diff().abs().max(),\n            ...     'Hours Above Mean': lambda x: (x &gt; x.mean()).sum(),\n            ...     'Volatility': lambda x: x.std() / x.mean() * 100 if x.mean() != 0 else 0\n            ... }\n\n        Available built-in statistics are provided in DEFAULT_STATISTICS and\n        STATISTICS_LIBRARY class attributes.\n\n    Data Format Requirements:\n        Input data must be a pandas DataFrame or Series with:\n        - DatetimeIndex (hourly or sub-hourly recommended)\n        - For faceting: MultiIndex columns with named levels\n        - Column names will be used as facet category labels\n\n        MultiIndex structure for faceting:\n\n            &gt;&gt;&gt; # Two-level MultiIndex example\n            &gt;&gt;&gt; data.columns = pd.MultiIndex.from_tuples([\n            &gt;&gt;&gt;     ('scenario1', 'solar'), ('scenario1', 'wind'),\n            &gt;&gt;&gt;     ('scenario2', 'solar'), ('scenario2', 'wind')\n            &gt;&gt;&gt; ], names=['scenario', 'technology'])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Use column level names for faceting\n            &gt;&gt;&gt; config = DashboardConfig(\n            ...     facet_row='technology',  # Use 'technology' level\n            ...     facet_col='scenario'     # Use 'scenario' level\n            ... )\n    \"\"\"\n    DEFAULT_STATISTICS = {\n        'Datums': lambda x: len(x),\n        'Abs max': lambda x: x.abs().max(),\n        'Abs mean': lambda x: x.abs().mean(),\n        'Max': lambda x: x.max(),\n        'Mean': lambda x: x.mean(),\n        'Min': lambda x: x.min(),\n    }\n\n    STATISTICS_LIBRARY = {\n        '# Values': lambda x: (~x.isna()).sum(),\n        '# NaNs': lambda x: x.isna().sum(),\n        '% == 0': lambda x: (x.round(2) == 0).sum() / (~x.isna()).sum() * 100,\n        '% != 0': lambda x: ((x.round(2) != 0) &amp; (~x.isna())).sum() / (~x.isna()).sum() * 100,\n        '% &gt; 0': lambda x: (x.round(2) &gt; 0).sum() / (~x.isna()).sum() * 100,\n        '% &lt; 0': lambda x: (x.round(2) &lt; 0).sum() / (~x.isna()).sum() * 100,\n        'Mean of v&gt;0': lambda x: x.where(x &gt; 0, np.nan).mean(),\n        'Mean of v&lt;0': lambda x: x.where(x &lt; 0, np.nan).mean(),\n        'Median': lambda x: x.median(),\n        'Q0.99': lambda x: x.quantile(0.99),\n        'Q0.95': lambda x: x.quantile(0.95),\n        'Q0.05': lambda x: x.quantile(0.05),\n        'Q0.01': lambda x: x.quantile(0.01),\n        'Std': lambda x: x.std(),\n    }\n\n    def __init__(\n            self,\n            x_axis: X_AXIS_TYPES = 'date',\n            facet_col: str = None,\n            facet_row: str = None,\n            facet_col_wrap: int = None,\n            facet_col_order: list[str] = None,\n            facet_row_order: list[str] = None,\n            ratio_of_stat_col: float = 0.1,\n            stat_aggs: Dict[str, Callable[[pd.Series], float | int]] = None,\n            groupby_aggregation: GROUPBY_AGG_TYPES = 'mean',\n            title: str = None,\n            color_continuous_scale: str | list[str] | list[tuple[float, str]] = 'Turbo',\n            color_continuous_midpoint: int | float = None,\n            range_color: list[int | float] = None,\n            per_facet_col_colorscale: bool = False,\n            per_facet_row_colorscale: bool = False,\n            facet_row_color_settings: dict = None,\n            facet_col_color_settings: dict = None,\n            subplots_vertical_spacing: float = None,\n            subplots_horizontal_spacing: float = None,\n            time_series_figure_kwargs: dict = None,\n            stat_figure_kwargs: dict = None,\n            universal_figure_kwargs: dict = None,\n            use_string_for_axis: bool = False,\n            **figure_kwargs\n    ):\n        \"\"\"Initialize dashboard configuration.\n\n        Args:\n            x_axis: X-axis aggregation type ('date', 'year_month', 'year_week', 'week', 'month', 'year')\n                   or list of aggregation types for faceting.\n            facet_col: Column name to use for column faceting, or 'x_axis'/'groupby_aggregation'\n                      for parameter-based faceting.\n            facet_row: Column name to use for row faceting, or 'x_axis'/'groupby_aggregation'\n                      for parameter-based faceting.\n            facet_col_wrap: Maximum number of columns per row when using column faceting.\n            facet_col_order: Custom ordering for column facets.\n            facet_row_order: Custom ordering for row facets.\n            ratio_of_stat_col: Width ratio of statistics column relative to heatmap column.\n            stat_aggs: Dictionary of statistic names to aggregation functions for KPI calculation.\n            groupby_aggregation: Aggregation method for grouping data ('mean', 'sum', etc.) or list\n                               of methods for faceting.\n            title: Dashboard title.\n            color_continuous_scale: Plotly colorscale name or custom colorscale.\n            color_continuous_midpoint: Midpoint value for diverging colorscales.\n            range_color: Fixed color range [min, max] for heatmaps.\n            per_facet_col_colorscale: Whether to use separate colorscales per column facet.\n            per_facet_row_colorscale: Whether to use separate colorscales per row facet.\n            facet_row_color_settings: Custom color settings per row facet category.\n            facet_col_color_settings: Custom color settings per column facet category.\n            subplots_vertical_spacing: Vertical spacing between subplots.\n            subplots_horizontal_spacing: Horizontal spacing between subplots.\n            time_series_figure_kwargs: Additional kwargs for heatmap traces.\n            stat_figure_kwargs: Additional kwargs for statistics traces.\n            universal_figure_kwargs: kwargs applied to all traces.\n            use_string_for_axis: Whether to convert axis values to strings.\n            **figure_kwargs: Additional figure-level kwargs.\n        \"\"\"\n        self.x_axis = x_axis\n        self.facet_col = facet_col\n        self.facet_row = facet_row\n        self.facet_col_wrap = facet_col_wrap\n        self.facet_col_order = facet_col_order\n        self.facet_row_order = facet_row_order\n        self.ratio_of_stat_col = ratio_of_stat_col\n        self.stat_aggs = stat_aggs or self.DEFAULT_STATISTICS\n        self.groupby_aggregation = groupby_aggregation\n        self.title = title\n\n        self.per_facet_col_colorscale = per_facet_col_colorscale\n        self.per_facet_row_colorscale = per_facet_row_colorscale\n\n        if per_facet_col_colorscale and per_facet_row_colorscale:\n            raise ValueError(\"Cannot use both per_facet_col_colorscale and per_facet_row_colorscale simultaneously\")\n        if facet_row_color_settings and not per_facet_row_colorscale:\n            raise ValueError(\"Set per_facet_row_colorscale to True in order to use facet_row_color_settings.\")\n        if facet_col_color_settings and not per_facet_col_colorscale:\n            raise ValueError(\"Set per_facet_col_colorscale to True in order to use facet_col_color_settings.\")\n\n        self.facet_row_color_settings = facet_row_color_settings or {}\n        self.facet_col_color_settings = facet_col_color_settings or {}\n\n        self.time_series_figure_kwargs = time_series_figure_kwargs or {}\n        self.stat_figure_kwargs = stat_figure_kwargs or {}\n\n        self.subplots_vertical_spacing = subplots_vertical_spacing\n        self.subplots_horizontal_spacing = subplots_horizontal_spacing\n\n        universal_figure_kwargs = universal_figure_kwargs or {}\n\n        self.figure_kwargs = {\n            'color_continuous_scale': color_continuous_scale,\n            'color_continuous_midpoint': color_continuous_midpoint,\n            'range_color': range_color,\n            **universal_figure_kwargs,\n            **figure_kwargs,\n        }\n        self.use_string_for_axis = use_string_for_axis\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.DashboardConfig.__init__","title":"__init__","text":"<pre><code>__init__(x_axis: X_AXIS_TYPES = 'date', facet_col: str = None, facet_row: str = None, facet_col_wrap: int = None, facet_col_order: list[str] = None, facet_row_order: list[str] = None, ratio_of_stat_col: float = 0.1, stat_aggs: Dict[str, Callable[[Series], float | int]] = None, groupby_aggregation: GROUPBY_AGG_TYPES = 'mean', title: str = None, color_continuous_scale: str | list[str] | list[tuple[float, str]] = 'Turbo', color_continuous_midpoint: int | float = None, range_color: list[int | float] = None, per_facet_col_colorscale: bool = False, per_facet_row_colorscale: bool = False, facet_row_color_settings: dict = None, facet_col_color_settings: dict = None, subplots_vertical_spacing: float = None, subplots_horizontal_spacing: float = None, time_series_figure_kwargs: dict = None, stat_figure_kwargs: dict = None, universal_figure_kwargs: dict = None, use_string_for_axis: bool = False, **figure_kwargs)\n</code></pre> <p>Initialize dashboard configuration.</p> <p>Parameters:</p> Name Type Description Default <code>x_axis</code> <code>X_AXIS_TYPES</code> <p>X-axis aggregation type ('date', 'year_month', 'year_week', 'week', 'month', 'year')    or list of aggregation types for faceting.</p> <code>'date'</code> <code>facet_col</code> <code>str</code> <p>Column name to use for column faceting, or 'x_axis'/'groupby_aggregation'       for parameter-based faceting.</p> <code>None</code> <code>facet_row</code> <code>str</code> <p>Column name to use for row faceting, or 'x_axis'/'groupby_aggregation'       for parameter-based faceting.</p> <code>None</code> <code>facet_col_wrap</code> <code>int</code> <p>Maximum number of columns per row when using column faceting.</p> <code>None</code> <code>facet_col_order</code> <code>list[str]</code> <p>Custom ordering for column facets.</p> <code>None</code> <code>facet_row_order</code> <code>list[str]</code> <p>Custom ordering for row facets.</p> <code>None</code> <code>ratio_of_stat_col</code> <code>float</code> <p>Width ratio of statistics column relative to heatmap column.</p> <code>0.1</code> <code>stat_aggs</code> <code>Dict[str, Callable[[Series], float | int]]</code> <p>Dictionary of statistic names to aggregation functions for KPI calculation.</p> <code>None</code> <code>groupby_aggregation</code> <code>GROUPBY_AGG_TYPES</code> <p>Aggregation method for grouping data ('mean', 'sum', etc.) or list                of methods for faceting.</p> <code>'mean'</code> <code>title</code> <code>str</code> <p>Dashboard title.</p> <code>None</code> <code>color_continuous_scale</code> <code>str | list[str] | list[tuple[float, str]]</code> <p>Plotly colorscale name or custom colorscale.</p> <code>'Turbo'</code> <code>color_continuous_midpoint</code> <code>int | float</code> <p>Midpoint value for diverging colorscales.</p> <code>None</code> <code>range_color</code> <code>list[int | float]</code> <p>Fixed color range [min, max] for heatmaps.</p> <code>None</code> <code>per_facet_col_colorscale</code> <code>bool</code> <p>Whether to use separate colorscales per column facet.</p> <code>False</code> <code>per_facet_row_colorscale</code> <code>bool</code> <p>Whether to use separate colorscales per row facet.</p> <code>False</code> <code>facet_row_color_settings</code> <code>dict</code> <p>Custom color settings per row facet category.</p> <code>None</code> <code>facet_col_color_settings</code> <code>dict</code> <p>Custom color settings per column facet category.</p> <code>None</code> <code>subplots_vertical_spacing</code> <code>float</code> <p>Vertical spacing between subplots.</p> <code>None</code> <code>subplots_horizontal_spacing</code> <code>float</code> <p>Horizontal spacing between subplots.</p> <code>None</code> <code>time_series_figure_kwargs</code> <code>dict</code> <p>Additional kwargs for heatmap traces.</p> <code>None</code> <code>stat_figure_kwargs</code> <code>dict</code> <p>Additional kwargs for statistics traces.</p> <code>None</code> <code>universal_figure_kwargs</code> <code>dict</code> <p>kwargs applied to all traces.</p> <code>None</code> <code>use_string_for_axis</code> <code>bool</code> <p>Whether to convert axis values to strings.</p> <code>False</code> <code>**figure_kwargs</code> <p>Additional figure-level kwargs.</p> <code>{}</code> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>def __init__(\n        self,\n        x_axis: X_AXIS_TYPES = 'date',\n        facet_col: str = None,\n        facet_row: str = None,\n        facet_col_wrap: int = None,\n        facet_col_order: list[str] = None,\n        facet_row_order: list[str] = None,\n        ratio_of_stat_col: float = 0.1,\n        stat_aggs: Dict[str, Callable[[pd.Series], float | int]] = None,\n        groupby_aggregation: GROUPBY_AGG_TYPES = 'mean',\n        title: str = None,\n        color_continuous_scale: str | list[str] | list[tuple[float, str]] = 'Turbo',\n        color_continuous_midpoint: int | float = None,\n        range_color: list[int | float] = None,\n        per_facet_col_colorscale: bool = False,\n        per_facet_row_colorscale: bool = False,\n        facet_row_color_settings: dict = None,\n        facet_col_color_settings: dict = None,\n        subplots_vertical_spacing: float = None,\n        subplots_horizontal_spacing: float = None,\n        time_series_figure_kwargs: dict = None,\n        stat_figure_kwargs: dict = None,\n        universal_figure_kwargs: dict = None,\n        use_string_for_axis: bool = False,\n        **figure_kwargs\n):\n    \"\"\"Initialize dashboard configuration.\n\n    Args:\n        x_axis: X-axis aggregation type ('date', 'year_month', 'year_week', 'week', 'month', 'year')\n               or list of aggregation types for faceting.\n        facet_col: Column name to use for column faceting, or 'x_axis'/'groupby_aggregation'\n                  for parameter-based faceting.\n        facet_row: Column name to use for row faceting, or 'x_axis'/'groupby_aggregation'\n                  for parameter-based faceting.\n        facet_col_wrap: Maximum number of columns per row when using column faceting.\n        facet_col_order: Custom ordering for column facets.\n        facet_row_order: Custom ordering for row facets.\n        ratio_of_stat_col: Width ratio of statistics column relative to heatmap column.\n        stat_aggs: Dictionary of statistic names to aggregation functions for KPI calculation.\n        groupby_aggregation: Aggregation method for grouping data ('mean', 'sum', etc.) or list\n                           of methods for faceting.\n        title: Dashboard title.\n        color_continuous_scale: Plotly colorscale name or custom colorscale.\n        color_continuous_midpoint: Midpoint value for diverging colorscales.\n        range_color: Fixed color range [min, max] for heatmaps.\n        per_facet_col_colorscale: Whether to use separate colorscales per column facet.\n        per_facet_row_colorscale: Whether to use separate colorscales per row facet.\n        facet_row_color_settings: Custom color settings per row facet category.\n        facet_col_color_settings: Custom color settings per column facet category.\n        subplots_vertical_spacing: Vertical spacing between subplots.\n        subplots_horizontal_spacing: Horizontal spacing between subplots.\n        time_series_figure_kwargs: Additional kwargs for heatmap traces.\n        stat_figure_kwargs: Additional kwargs for statistics traces.\n        universal_figure_kwargs: kwargs applied to all traces.\n        use_string_for_axis: Whether to convert axis values to strings.\n        **figure_kwargs: Additional figure-level kwargs.\n    \"\"\"\n    self.x_axis = x_axis\n    self.facet_col = facet_col\n    self.facet_row = facet_row\n    self.facet_col_wrap = facet_col_wrap\n    self.facet_col_order = facet_col_order\n    self.facet_row_order = facet_row_order\n    self.ratio_of_stat_col = ratio_of_stat_col\n    self.stat_aggs = stat_aggs or self.DEFAULT_STATISTICS\n    self.groupby_aggregation = groupby_aggregation\n    self.title = title\n\n    self.per_facet_col_colorscale = per_facet_col_colorscale\n    self.per_facet_row_colorscale = per_facet_row_colorscale\n\n    if per_facet_col_colorscale and per_facet_row_colorscale:\n        raise ValueError(\"Cannot use both per_facet_col_colorscale and per_facet_row_colorscale simultaneously\")\n    if facet_row_color_settings and not per_facet_row_colorscale:\n        raise ValueError(\"Set per_facet_row_colorscale to True in order to use facet_row_color_settings.\")\n    if facet_col_color_settings and not per_facet_col_colorscale:\n        raise ValueError(\"Set per_facet_col_colorscale to True in order to use facet_col_color_settings.\")\n\n    self.facet_row_color_settings = facet_row_color_settings or {}\n    self.facet_col_color_settings = facet_col_color_settings or {}\n\n    self.time_series_figure_kwargs = time_series_figure_kwargs or {}\n    self.stat_figure_kwargs = stat_figure_kwargs or {}\n\n    self.subplots_vertical_spacing = subplots_vertical_spacing\n    self.subplots_horizontal_spacing = subplots_horizontal_spacing\n\n    universal_figure_kwargs = universal_figure_kwargs or {}\n\n    self.figure_kwargs = {\n        'color_continuous_scale': color_continuous_scale,\n        'color_continuous_midpoint': color_continuous_midpoint,\n        'range_color': range_color,\n        **universal_figure_kwargs,\n        **figure_kwargs,\n    }\n    self.use_string_for_axis = use_string_for_axis\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.DataProcessor","title":"DataProcessor","text":"Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>class DataProcessor:\n    @staticmethod\n    def validate_input_data_and_config(data: pd.DataFrame, config: DashboardConfig) -&gt; None:\n        x_axis = config.x_axis\n        groupby_aggregation = config.groupby_aggregation\n        facet_col = config.facet_col\n        facet_row = config.facet_row\n        facet_col_wrap = config.facet_col_wrap\n\n        if facet_col_wrap is not None and facet_row is not None:\n            raise ValueError('You cannot set facet_row if you are setting a facet_col_wrap')\n\n        if isinstance(data, pd.Series):\n            if sum(facet not in [None, 'x_axis', 'groupby_aggregation'] for facet in [facet_col, facet_row]):\n                raise ValueError('You can not define facet_col or facet_row if you just have a pd.Series')\n        elif data.columns.nlevels &gt; 2:\n            raise ValueError('Your data must not have more than 2 column index levels.')\n        elif data.columns.nlevels == 2:\n            if (facet_col is None) and (facet_row is None):\n                raise ValueError('If you have two column levels, you must define both, facet_col and facet_row.')\n            if isinstance(x_axis, list) or isinstance(groupby_aggregation, list):\n                raise ValueError(\n                    'You cannot set x_axis or groupby_aggregation to a list if your data already has 2 levels.'\n                )\n        elif data.columns.nlevels == 1:\n            if sum(facet not in [None, 'x_axis', 'groupby_aggregation'] for facet in [facet_col, facet_row]) &gt; 1:\n                raise ValueError('You only have 1 column level. You can only define facet_col or facet_row')\n            if isinstance(x_axis, list) and isinstance(groupby_aggregation, list):\n                raise ValueError(\n                    'You cannot set x_axis and groupby_aggregation to a list if your data already has 1 level.'\n                )\n\n        if isinstance(x_axis, list):\n            if not any('x_axis' == facet for facet in [facet_col, facet_row]):\n                raise ValueError(\n                    \"x_axis must be either 'facet_col' or 'facet_row' when provided as a list.\"\n                )\n        else:\n            if any('x_axis' == facet for facet in [facet_col, facet_row]):\n                raise ValueError(\n                    \"You provided a str for x_axis, \"\n                    \"but set facet_col or facet_row to 'x_axis'. This is not allowed! \\n\"\n                    \"You must provide a List[str] and in order to use facet_row / facet_col \"\n                    \"for different x_axis.\"\n                )\n\n        if isinstance(groupby_aggregation, list):\n            if not any('groupby_aggregation' == facet for facet in [facet_col, facet_row]):\n                raise ValueError(\n                    \"groupby_aggregation must be either 'facet_col' or 'facet_row' when provided as a list.\"\n                )\n        else:\n            if any('groupby_aggregation' == facet for facet in [facet_col, facet_row]):\n                raise ValueError(\n                    \"You provided a str for groupby_aggregation, \"\n                    \"but set facet_col or facet_row to 'groupby_aggregation'. This is not allowed! \\n\"\n                    \"You must provide a List[str] and in order to use facet_row / facet_col \"\n                    \"for different groupby_aggregation.\"\n                )\n\n    @staticmethod\n    def prepare_dataframe_for_facet(data: pd.DataFrame, config: DashboardConfig) -&gt; pd.DataFrame:\n        for k in ['x_axis', 'groupby_aggregation']:\n            config_value = getattr(config, k)\n            if isinstance(config_value, list):\n                data = pd.concat(\n                    {i: data.copy(deep=True) for i in config_value},\n                    axis=1,\n                    names=[k],\n                )\n        return data\n\n    @classmethod\n    def ensure_df_has_two_column_levels(cls, data: pd.DataFrame, config: DashboardConfig) -&gt; pd.DataFrame:\n        if isinstance(data, pd.Series):\n            data = data.to_frame(data.name or 'Time series')\n\n        if data.columns.nlevels == 1:\n            data.columns.name = data.columns.name or 'variable'\n            data = cls._insert_empty_column_index_level(data)\n\n        if config.facet_col in [data.columns.names[0]]:\n            data.columns = data.columns.reorder_levels([1, 0])\n\n        return data\n\n    @staticmethod\n    def update_facet_config(data: pd.DataFrame, config: DashboardConfig) -&gt; None:\n        unique_facet_col_keys = data.columns.get_level_values(config.facet_col).unique().to_list()\n        if config.facet_col_order is None:\n            config.facet_col_order = unique_facet_col_keys\n        else:\n            config.facet_col_order += [c for c in unique_facet_col_keys if c not in config.facet_col_order]\n\n        unique_facet_row_keys = data.columns.get_level_values(config.facet_row).unique().to_list()\n        if config.facet_row_order is None:\n            config.facet_row_order = unique_facet_row_keys\n        else:\n            config.facet_row_order += [c for c in unique_facet_row_keys if c not in config.facet_row_order]\n\n        if config.facet_col_wrap is None:\n            config.facet_col_wrap = len(config.facet_col_order)\n\n    @staticmethod\n    def get_grouped_data(series: pd.Series, x_axis: str, groupby_aggregation: str) -&gt; pd.DataFrame:\n        \"\"\"Group and aggregate time series data into heatmap format.\n\n        Transforms timeseries data into a matrix suitable for heatmap visualization\n        with hour-of-day on y-axis and specified time aggregation on x-axis.\n\n        Args:\n            series: Input timeseries data with datetime index.\n            x_axis: Time aggregation method ('date', 'week', 'month', etc.).\n            groupby_aggregation: Aggregation function name ('mean', 'sum', etc.).\n\n        Returns:\n            DataFrame with time categories as columns and hour-of-day as rows.\n        \"\"\"\n        temp = series.to_frame('value')\n        temp.loc[:, 'time'] = temp.index.time\n        temp.loc[:, 'minute'] = temp.index.minute\n        temp.loc[:, 'hour'] = temp.index.hour + 1\n        temp.loc[:, 'date'] = temp.index.date\n        temp.loc[:, 'month'] = temp.index.month\n        temp.loc[:, 'week'] = temp.index.isocalendar().week\n        temp.loc[:, 'year_month'] = temp.index.strftime('%Y-%m')\n        temp.loc[:, 'year_week'] = temp.index.strftime('%Y-CW%U')\n\n        y_axis = 'time'\n        groupby = [y_axis, x_axis]\n        temp = temp.groupby(groupby)['value'].agg(groupby_aggregation)\n        temp = temp.unstack(x_axis)\n        temp_data = temp.sort_index(ascending=False)\n        return temp_data\n\n    @staticmethod\n    def _insert_empty_column_index_level(df: pd.DataFrame, level_name: str = None) -&gt; pd.DataFrame:\n        level_value = ''\n        return pd.concat({level_value: df}, axis=1, names=[level_name])\n\n    @staticmethod\n    def _prepend_empty_row(df: pd.DataFrame) -&gt; pd.DataFrame:\n        empty_row = pd.DataFrame([[np.nan] * len(df.columns)], index=[' '], columns=df.columns)\n        return pd.concat([empty_row, df])\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.DataProcessor.get_grouped_data","title":"get_grouped_data  <code>staticmethod</code>","text":"<pre><code>get_grouped_data(series: Series, x_axis: str, groupby_aggregation: str) -&gt; DataFrame\n</code></pre> <p>Group and aggregate time series data into heatmap format.</p> <p>Transforms timeseries data into a matrix suitable for heatmap visualization with hour-of-day on y-axis and specified time aggregation on x-axis.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>Input timeseries data with datetime index.</p> required <code>x_axis</code> <code>str</code> <p>Time aggregation method ('date', 'week', 'month', etc.).</p> required <code>groupby_aggregation</code> <code>str</code> <p>Aggregation function name ('mean', 'sum', etc.).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with time categories as columns and hour-of-day as rows.</p> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>@staticmethod\ndef get_grouped_data(series: pd.Series, x_axis: str, groupby_aggregation: str) -&gt; pd.DataFrame:\n    \"\"\"Group and aggregate time series data into heatmap format.\n\n    Transforms timeseries data into a matrix suitable for heatmap visualization\n    with hour-of-day on y-axis and specified time aggregation on x-axis.\n\n    Args:\n        series: Input timeseries data with datetime index.\n        x_axis: Time aggregation method ('date', 'week', 'month', etc.).\n        groupby_aggregation: Aggregation function name ('mean', 'sum', etc.).\n\n    Returns:\n        DataFrame with time categories as columns and hour-of-day as rows.\n    \"\"\"\n    temp = series.to_frame('value')\n    temp.loc[:, 'time'] = temp.index.time\n    temp.loc[:, 'minute'] = temp.index.minute\n    temp.loc[:, 'hour'] = temp.index.hour + 1\n    temp.loc[:, 'date'] = temp.index.date\n    temp.loc[:, 'month'] = temp.index.month\n    temp.loc[:, 'week'] = temp.index.isocalendar().week\n    temp.loc[:, 'year_month'] = temp.index.strftime('%Y-%m')\n    temp.loc[:, 'year_week'] = temp.index.strftime('%Y-CW%U')\n\n    y_axis = 'time'\n    groupby = [y_axis, x_axis]\n    temp = temp.groupby(groupby)['value'].agg(groupby_aggregation)\n    temp = temp.unstack(x_axis)\n    temp_data = temp.sort_index(ascending=False)\n    return temp_data\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.ColorManager","title":"ColorManager","text":"<p>Manages color settings and scale computation for dashboard visualizations.</p> <p>Handles colorscale selection, range computation, and facet-specific color customization for heatmap and statistics traces.</p> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>class ColorManager:\n    \"\"\"Manages color settings and scale computation for dashboard visualizations.\n\n    Handles colorscale selection, range computation, and facet-specific color\n    customization for heatmap and statistics traces.\n    \"\"\"\n    @staticmethod\n    def get_color_settings_for_facet_category(config: DashboardConfig, facet_key: tuple[str, str]):\n        \"\"\"Get color settings for a specific facet category.\n\n        Args:\n            config: Dashboard configuration object.\n            facet_key: Tuple of (row_key, col_key) identifying the facet.\n\n        Returns:\n            Dictionary of color settings for the specified facet category.\n        \"\"\"\n        row_key, col_key = facet_key\n        settings = {\n            'color_continuous_scale': config.figure_kwargs.get('color_continuous_scale'),\n            'color_continuous_midpoint': config.figure_kwargs.get('color_continuous_midpoint'),\n            'range_color': config.figure_kwargs.get('range_color')\n        }\n\n        if config.per_facet_row_colorscale and row_key in config.facet_row_color_settings:\n            settings.update(config.facet_row_color_settings.get(row_key, {}))\n        elif config.per_facet_col_colorscale and col_key in config.facet_col_color_settings:\n            settings.update(config.facet_col_color_settings.get(col_key, {}))\n\n        return settings\n\n    @staticmethod\n    def compute_color_params(data, config: DashboardConfig, facet_key: tuple[str, str] = None):\n        \"\"\"Compute color parameters for heatmap traces.\n\n        Calculates colorscale, min/max values, and other color-related parameters\n        based on data range and configuration settings.\n\n        Args:\n            data: Input data for color range calculation.\n            config: Dashboard configuration object.\n            facet_key: Optional facet identifier for per-facet colorscales.\n\n        Returns:\n            Dictionary of color parameters for plotly traces.\n        \"\"\"\n        if facet_key is not None:\n            settings = ColorManager.get_color_settings_for_facet_category(config, facet_key)\n        else:\n            settings = config.figure_kwargs\n\n        if facet_key is not None:\n            if config.per_facet_row_colorscale:\n                row_key, _ = facet_key\n                filtered_data = data.loc[:, (row_key, slice(None))]\n            elif config.per_facet_col_colorscale:\n                _, col_key = facet_key\n                filtered_data = data.loc[:, (slice(None), col_key)]\n            else:\n                filtered_data = data\n        else:\n            filtered_data = data\n\n        color_continuous_scale = settings.get('color_continuous_scale')\n        color_continuous_midpoint = settings.get('color_continuous_midpoint')\n        range_color = settings.get('range_color')\n\n        result = {}\n        if color_continuous_scale:\n            result['colorscale'] = color_continuous_scale\n\n        if range_color:\n            result['zmin'] = range_color[0]\n            result['zmax'] = range_color[1]\n        elif color_continuous_midpoint == 0:\n            _absmax = filtered_data.abs().max().max()\n            result['zmin'] = -_absmax\n            result['zmax'] = _absmax\n        elif color_continuous_midpoint:\n            raise NotImplementedError(\"color_continuous_midpoint other than 0 is not implemented\")\n        else:\n            result['zmin'] = filtered_data.min().min()\n            result['zmax'] = filtered_data.max().max()\n\n        return result\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.ColorManager.get_color_settings_for_facet_category","title":"get_color_settings_for_facet_category  <code>staticmethod</code>","text":"<pre><code>get_color_settings_for_facet_category(config: DashboardConfig, facet_key: tuple[str, str])\n</code></pre> <p>Get color settings for a specific facet category.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DashboardConfig</code> <p>Dashboard configuration object.</p> required <code>facet_key</code> <code>tuple[str, str]</code> <p>Tuple of (row_key, col_key) identifying the facet.</p> required <p>Returns:</p> Type Description <p>Dictionary of color settings for the specified facet category.</p> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>@staticmethod\ndef get_color_settings_for_facet_category(config: DashboardConfig, facet_key: tuple[str, str]):\n    \"\"\"Get color settings for a specific facet category.\n\n    Args:\n        config: Dashboard configuration object.\n        facet_key: Tuple of (row_key, col_key) identifying the facet.\n\n    Returns:\n        Dictionary of color settings for the specified facet category.\n    \"\"\"\n    row_key, col_key = facet_key\n    settings = {\n        'color_continuous_scale': config.figure_kwargs.get('color_continuous_scale'),\n        'color_continuous_midpoint': config.figure_kwargs.get('color_continuous_midpoint'),\n        'range_color': config.figure_kwargs.get('range_color')\n    }\n\n    if config.per_facet_row_colorscale and row_key in config.facet_row_color_settings:\n        settings.update(config.facet_row_color_settings.get(row_key, {}))\n    elif config.per_facet_col_colorscale and col_key in config.facet_col_color_settings:\n        settings.update(config.facet_col_color_settings.get(col_key, {}))\n\n    return settings\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.ColorManager.compute_color_params","title":"compute_color_params  <code>staticmethod</code>","text":"<pre><code>compute_color_params(data, config: DashboardConfig, facet_key: tuple[str, str] = None)\n</code></pre> <p>Compute color parameters for heatmap traces.</p> <p>Calculates colorscale, min/max values, and other color-related parameters based on data range and configuration settings.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>Input data for color range calculation.</p> required <code>config</code> <code>DashboardConfig</code> <p>Dashboard configuration object.</p> required <code>facet_key</code> <code>tuple[str, str]</code> <p>Optional facet identifier for per-facet colorscales.</p> <code>None</code> <p>Returns:</p> Type Description <p>Dictionary of color parameters for plotly traces.</p> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>@staticmethod\ndef compute_color_params(data, config: DashboardConfig, facet_key: tuple[str, str] = None):\n    \"\"\"Compute color parameters for heatmap traces.\n\n    Calculates colorscale, min/max values, and other color-related parameters\n    based on data range and configuration settings.\n\n    Args:\n        data: Input data for color range calculation.\n        config: Dashboard configuration object.\n        facet_key: Optional facet identifier for per-facet colorscales.\n\n    Returns:\n        Dictionary of color parameters for plotly traces.\n    \"\"\"\n    if facet_key is not None:\n        settings = ColorManager.get_color_settings_for_facet_category(config, facet_key)\n    else:\n        settings = config.figure_kwargs\n\n    if facet_key is not None:\n        if config.per_facet_row_colorscale:\n            row_key, _ = facet_key\n            filtered_data = data.loc[:, (row_key, slice(None))]\n        elif config.per_facet_col_colorscale:\n            _, col_key = facet_key\n            filtered_data = data.loc[:, (slice(None), col_key)]\n        else:\n            filtered_data = data\n    else:\n        filtered_data = data\n\n    color_continuous_scale = settings.get('color_continuous_scale')\n    color_continuous_midpoint = settings.get('color_continuous_midpoint')\n    range_color = settings.get('range_color')\n\n    result = {}\n    if color_continuous_scale:\n        result['colorscale'] = color_continuous_scale\n\n    if range_color:\n        result['zmin'] = range_color[0]\n        result['zmax'] = range_color[1]\n    elif color_continuous_midpoint == 0:\n        _absmax = filtered_data.abs().max().max()\n        result['zmin'] = -_absmax\n        result['zmax'] = _absmax\n    elif color_continuous_midpoint:\n        raise NotImplementedError(\"color_continuous_midpoint other than 0 is not implemented\")\n    else:\n        result['zmin'] = filtered_data.min().min()\n        result['zmax'] = filtered_data.max().max()\n\n    return result\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.TraceGenerator","title":"TraceGenerator","text":"<p>Generates plotly trace objects for dashboard visualization.</p> <p>Creates heatmap traces for timeseries data, statistics traces for KPI display, and colorscale traces for custom color legends.</p> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>class TraceGenerator:\n    \"\"\"Generates plotly trace objects for dashboard visualization.\n\n    Creates heatmap traces for timeseries data, statistics traces for KPI display,\n    and colorscale traces for custom color legends.\n    \"\"\"\n\n    @classmethod\n    def get_heatmap_trace(cls, data: pd.DataFrame, ts_kwargs, color_kwargs, use_string_for_axis, **kwargs):\n        \"\"\"Create a heatmap trace for timeseries data visualization.\n\n        Args:\n            data: DataFrame with time categories as columns and hour-of-day as rows.\n            ts_kwargs: Additional kwargs for the heatmap trace.\n            color_kwargs: Color-related parameters (colorscale, zmin, zmax).\n            use_string_for_axis: Whether to convert axis values to strings.\n            **kwargs: Additional plotly Heatmap parameters.\n\n        Returns:\n            Plotly Heatmap trace object for timeseries visualization.\n        \"\"\"\n        if set(data.columns).issubset(list(range(1, 13))):\n            x = [calendar.month_abbr[m] for m in range(1, 13)]\n        else:\n            if use_string_for_axis:\n                x = [str(i).replace('-', '_') for i in data.columns]\n            else:\n                x = data.columns\n\n        trace_kwargs = {**color_kwargs, **ts_kwargs, **kwargs}\n\n        assert 'colorscale' in trace_kwargs and 'zmin' in trace_kwargs and 'zmax' in trace_kwargs\n\n        trace_heatmap = go.Heatmap(x=x, z=data.values, y=data.index, **trace_kwargs)\n        return trace_heatmap\n\n    @classmethod\n    def get_stats_trace(cls, series: pd.Series, stat_aggs, stat_kwargs, color_kwargs, **kwargs):\n        \"\"\"Create a heatmap trace for displaying KPI statistics.\n\n        Args:\n            series: Input timeseries data for statistics calculation.\n            stat_aggs: Dictionary of statistic names to aggregation functions.\n            stat_kwargs: Additional kwargs for the statistics trace.\n            color_kwargs: Color-related parameters (colorscale, zmin, zmax).\n            **kwargs: Additional plotly Heatmap parameters.\n\n        Returns:\n            Plotly Heatmap trace object displaying calculated statistics.\n        \"\"\"\n        data_stats = pd.Series({agg: func(series) for agg, func in stat_aggs.items()})\n        data_stats = data_stats.to_frame('stats')\n        data_stats = DataProcessor._prepend_empty_row(data_stats)\n\n        if 'ygap' not in stat_kwargs:\n            stat_kwargs['ygap'] = 5\n\n        text_data = data_stats.map(lambda x: f'{x:.0f}')\n        text_data = text_data.replace('nan', '').replace('null', '')\n\n        trace_kwargs = {**color_kwargs, **stat_kwargs, **kwargs}\n        trace_kwargs['showscale'] = False  # Stats should never have a colorbar\n\n        assert 'colorscale' in trace_kwargs and 'zmin' in trace_kwargs and 'zmax' in trace_kwargs\n\n        trace_stats = go.Heatmap(\n            z=data_stats.values,\n            x=data_stats.columns,\n            y=data_stats.index,\n            text=text_data.values,\n            texttemplate=\"%{text}\",\n            **trace_kwargs\n        )\n        return trace_stats\n\n    @staticmethod\n    def create_colorscale_trace(z_min, z_max, colorscale, orientation='v', title=None):\n        \"\"\"Create a colorscale trace for custom color legend display.\n\n        Args:\n            z_min: Minimum value for colorscale range.\n            z_max: Maximum value for colorscale range.\n            colorscale: Plotly colorscale specification.\n            orientation: Colorscale orientation ('v' for vertical, 'h' for horizontal).\n            title: Optional title for the colorscale.\n\n        Returns:\n            Plotly Heatmap trace object representing the colorscale legend.\n        \"\"\"\n        if orientation == 'v':\n            z_vals = np.linspace(z_min, z_max, 100).reshape(-1, 1)\n        else:\n            z_vals = np.linspace(z_min, z_max, 100).reshape(1, -1)\n\n        axis_vals = np.linspace(z_min, z_max, 100)\n\n        colorbar_settings = {\n            'thickness': 15,\n            'title': title or ''\n        }\n\n        if orientation == 'h':\n            colorbar_settings.update({\n                'orientation': 'h',\n                'y': -0.15,\n                'xanchor': 'center',\n                'x': 0.5\n            })\n            x = axis_vals\n            y = None\n        else:\n            x = None\n            y = axis_vals\n\n        return go.Heatmap(\n            x=x,\n            y=y,\n            z=z_vals,\n            colorscale=colorscale,\n            showscale=False,\n            zmin=z_min,\n            zmax=z_max,\n            colorbar=colorbar_settings\n        )\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.TraceGenerator.get_heatmap_trace","title":"get_heatmap_trace  <code>classmethod</code>","text":"<pre><code>get_heatmap_trace(data: DataFrame, ts_kwargs, color_kwargs, use_string_for_axis, **kwargs)\n</code></pre> <p>Create a heatmap trace for timeseries data visualization.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame with time categories as columns and hour-of-day as rows.</p> required <code>ts_kwargs</code> <p>Additional kwargs for the heatmap trace.</p> required <code>color_kwargs</code> <p>Color-related parameters (colorscale, zmin, zmax).</p> required <code>use_string_for_axis</code> <p>Whether to convert axis values to strings.</p> required <code>**kwargs</code> <p>Additional plotly Heatmap parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Plotly Heatmap trace object for timeseries visualization.</p> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>@classmethod\ndef get_heatmap_trace(cls, data: pd.DataFrame, ts_kwargs, color_kwargs, use_string_for_axis, **kwargs):\n    \"\"\"Create a heatmap trace for timeseries data visualization.\n\n    Args:\n        data: DataFrame with time categories as columns and hour-of-day as rows.\n        ts_kwargs: Additional kwargs for the heatmap trace.\n        color_kwargs: Color-related parameters (colorscale, zmin, zmax).\n        use_string_for_axis: Whether to convert axis values to strings.\n        **kwargs: Additional plotly Heatmap parameters.\n\n    Returns:\n        Plotly Heatmap trace object for timeseries visualization.\n    \"\"\"\n    if set(data.columns).issubset(list(range(1, 13))):\n        x = [calendar.month_abbr[m] for m in range(1, 13)]\n    else:\n        if use_string_for_axis:\n            x = [str(i).replace('-', '_') for i in data.columns]\n        else:\n            x = data.columns\n\n    trace_kwargs = {**color_kwargs, **ts_kwargs, **kwargs}\n\n    assert 'colorscale' in trace_kwargs and 'zmin' in trace_kwargs and 'zmax' in trace_kwargs\n\n    trace_heatmap = go.Heatmap(x=x, z=data.values, y=data.index, **trace_kwargs)\n    return trace_heatmap\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.TraceGenerator.get_stats_trace","title":"get_stats_trace  <code>classmethod</code>","text":"<pre><code>get_stats_trace(series: Series, stat_aggs, stat_kwargs, color_kwargs, **kwargs)\n</code></pre> <p>Create a heatmap trace for displaying KPI statistics.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>Input timeseries data for statistics calculation.</p> required <code>stat_aggs</code> <p>Dictionary of statistic names to aggregation functions.</p> required <code>stat_kwargs</code> <p>Additional kwargs for the statistics trace.</p> required <code>color_kwargs</code> <p>Color-related parameters (colorscale, zmin, zmax).</p> required <code>**kwargs</code> <p>Additional plotly Heatmap parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Plotly Heatmap trace object displaying calculated statistics.</p> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>@classmethod\ndef get_stats_trace(cls, series: pd.Series, stat_aggs, stat_kwargs, color_kwargs, **kwargs):\n    \"\"\"Create a heatmap trace for displaying KPI statistics.\n\n    Args:\n        series: Input timeseries data for statistics calculation.\n        stat_aggs: Dictionary of statistic names to aggregation functions.\n        stat_kwargs: Additional kwargs for the statistics trace.\n        color_kwargs: Color-related parameters (colorscale, zmin, zmax).\n        **kwargs: Additional plotly Heatmap parameters.\n\n    Returns:\n        Plotly Heatmap trace object displaying calculated statistics.\n    \"\"\"\n    data_stats = pd.Series({agg: func(series) for agg, func in stat_aggs.items()})\n    data_stats = data_stats.to_frame('stats')\n    data_stats = DataProcessor._prepend_empty_row(data_stats)\n\n    if 'ygap' not in stat_kwargs:\n        stat_kwargs['ygap'] = 5\n\n    text_data = data_stats.map(lambda x: f'{x:.0f}')\n    text_data = text_data.replace('nan', '').replace('null', '')\n\n    trace_kwargs = {**color_kwargs, **stat_kwargs, **kwargs}\n    trace_kwargs['showscale'] = False  # Stats should never have a colorbar\n\n    assert 'colorscale' in trace_kwargs and 'zmin' in trace_kwargs and 'zmax' in trace_kwargs\n\n    trace_stats = go.Heatmap(\n        z=data_stats.values,\n        x=data_stats.columns,\n        y=data_stats.index,\n        text=text_data.values,\n        texttemplate=\"%{text}\",\n        **trace_kwargs\n    )\n    return trace_stats\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/time_series_dashboard/#mescal.visualizations.plotly_figures.timeseries_dashboard.TraceGenerator.create_colorscale_trace","title":"create_colorscale_trace  <code>staticmethod</code>","text":"<pre><code>create_colorscale_trace(z_min, z_max, colorscale, orientation='v', title=None)\n</code></pre> <p>Create a colorscale trace for custom color legend display.</p> <p>Parameters:</p> Name Type Description Default <code>z_min</code> <p>Minimum value for colorscale range.</p> required <code>z_max</code> <p>Maximum value for colorscale range.</p> required <code>colorscale</code> <p>Plotly colorscale specification.</p> required <code>orientation</code> <p>Colorscale orientation ('v' for vertical, 'h' for horizontal).</p> <code>'v'</code> <code>title</code> <p>Optional title for the colorscale.</p> <code>None</code> <p>Returns:</p> Type Description <p>Plotly Heatmap trace object representing the colorscale legend.</p> Source code in <code>submodules/mescal/mescal/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>@staticmethod\ndef create_colorscale_trace(z_min, z_max, colorscale, orientation='v', title=None):\n    \"\"\"Create a colorscale trace for custom color legend display.\n\n    Args:\n        z_min: Minimum value for colorscale range.\n        z_max: Maximum value for colorscale range.\n        colorscale: Plotly colorscale specification.\n        orientation: Colorscale orientation ('v' for vertical, 'h' for horizontal).\n        title: Optional title for the colorscale.\n\n    Returns:\n        Plotly Heatmap trace object representing the colorscale legend.\n    \"\"\"\n    if orientation == 'v':\n        z_vals = np.linspace(z_min, z_max, 100).reshape(-1, 1)\n    else:\n        z_vals = np.linspace(z_min, z_max, 100).reshape(1, -1)\n\n    axis_vals = np.linspace(z_min, z_max, 100)\n\n    colorbar_settings = {\n        'thickness': 15,\n        'title': title or ''\n    }\n\n    if orientation == 'h':\n        colorbar_settings.update({\n            'orientation': 'h',\n            'y': -0.15,\n            'xanchor': 'center',\n            'x': 0.5\n        })\n        x = axis_vals\n        y = None\n    else:\n        x = None\n        y = axis_vals\n\n    return go.Heatmap(\n        x=x,\n        y=y,\n        z=z_vals,\n        colorscale=colorscale,\n        showscale=False,\n        zmin=z_min,\n        zmax=z_max,\n        colorbar=colorbar_settings\n    )\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/","title":"MESCAL Folium Visualization System","text":"<p>Folium-based visualization system for MESCAL energy system analysis.</p> <p>This module provides a comprehensive visualization framework built on Folium for creating interactive maps and geospatial visualizations of energy system data. The system supports multiple visualization types including areas, lines, markers, icons, and text overlays, with automatic feature resolution and generation capabilities.</p> <p>The visualization system follows a modular architecture with: - Base visualization components for property mapping and feature resolution - Specialized visualizers for KPI collections and data management - Multiple visualization feature types (areas, lines, markers, overlays) - Data item abstractions for model and KPI data integration</p> <p>Example:</p> <pre><code>Basic usage for creating area visualizations:\n&gt;&gt;&gt; from mescal.visualizations.folium_viz_system import AreaGenerator, AreaFeatureResolver\n&gt;&gt;&gt; area_resolver = AreaFeatureResolver(property_mapper)\n&gt;&gt;&gt; area_generator = AreaGenerator()\n&gt;&gt;&gt; area_features = area_resolver.resolve_features(data_items)\n&gt;&gt;&gt; folium_map = area_generator.add_features_to_map(folium_map, area_features)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/areas/","title":"MESCAL Folium Area Visualization System","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/areas/#mescal.visualizations.folium_viz_system.viz_areas","title":"viz_areas","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/areas/#mescal.visualizations.folium_viz_system.viz_areas.ResolvedAreaFeature","title":"ResolvedAreaFeature  <code>dataclass</code>","text":"<p>               Bases: <code>ResolvedFeature</code></p> <p>Resolved visual properties for area/polygon map elements.</p> <p>Container for all computed styling properties of polygon visualizations, including fill colors, border styles, and interactive behaviors. Used by AreaGenerator to create folium GeoJson polygons.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_areas.py</code> <pre><code>@dataclass\nclass ResolvedAreaFeature(ResolvedFeature):\n    \"\"\"\n    Resolved visual properties for area/polygon map elements.\n\n    Container for all computed styling properties of polygon visualizations,\n    including fill colors, border styles, and interactive behaviors.\n    Used by AreaGenerator to create folium GeoJson polygons.\n    \"\"\"\n\n    @property\n    def geometry(self) -&gt; Polygon | MultiPolygon:\n        return self.get('geometry')\n\n    @property\n    def fill_color(self) -&gt; str:\n        return self.get('fill_color')\n\n    @property\n    def border_color(self) -&gt; str:\n        return self.get('border_color')\n\n    @property\n    def border_width(self) -&gt; float:\n        return self.get('border_width')\n\n    @property\n    def fill_opacity(self) -&gt; float:\n        return min(self.get('fill_opacity'), 1.0)\n\n    @property\n    def highlight_border_width(self) -&gt; float:\n        highlight_border_width = self.get('highlight_border_width')\n        if highlight_border_width is None:\n            return self.border_width\n        return highlight_border_width\n\n    @property\n    def highlight_fill_opacity(self) -&gt; float:\n        return min(self.get('highlight_fill_opacity'), 1.0)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/areas/#mescal.visualizations.folium_viz_system.viz_areas.AreaFeatureResolver","title":"AreaFeatureResolver","text":"<p>               Bases: <code>FeatureResolver[ResolvedAreaFeature]</code></p> <p>Resolves visual properties for polygon/area map elements.</p> <p>Specialized feature resolver for area visualizations that handles polygon geometries, fill colors, border styling, and opacity settings. Commonly used for visualizing bidding zones, geographic regions, or any spatial areas with associated data values.</p> <p>Parameters:</p> Name Type Description Default <code>fill_color</code> <code>PropertyMapper | str</code> <p>Area fill color (static value or PropertyMapper)</p> <code>'#D2D2D2'</code> <code>border_color</code> <code>PropertyMapper | str</code> <p>Border/stroke color (static value or PropertyMapper)  </p> <code>'white'</code> <code>border_width</code> <code>PropertyMapper | float</code> <p>Border width in pixels (static value or PropertyMapper)</p> <code>2.0</code> <code>fill_opacity</code> <code>PropertyMapper | float</code> <p>Fill transparency 0-1 (static value or PropertyMapper)</p> <code>0.8</code> <code>highlight_border_width</code> <code>PropertyMapper | float</code> <p>Border width on hover (defaults to border_width)</p> <code>None</code> <code>highlight_fill_opacity</code> <code>PropertyMapper | float</code> <p>Fill opacity on hover (defaults to 1.0)</p> <code>1.0</code> <code>tooltip</code> <code>PropertyMapper | str | bool</code> <p>Tooltip content (True for auto-generated, False for none)</p> <code>True</code> <code>popup</code> <code>PropertyMapper | Popup | bool</code> <p>Popup content (True/False/PropertyMapper)</p> <code>False</code> <code>geometry</code> <code>PropertyMapper | Polygon</code> <p>Polygon geometry (defaults to 'geometry' attribute)</p> <code>None</code> <code>**property_mappers</code> <code>PropertyMapper | Any</code> <p>Additional custom property mappings</p> <code>{}</code> <p>Examples:</p> <p>Basic area visualization:</p> <pre><code>&gt;&gt;&gt; resolver = AreaFeatureResolver(\n...     fill_color='#FF0000',\n...     border_color='white',\n...     fill_opacity=0.8\n... )\n</code></pre> <p>Data-driven coloring:</p> <pre><code>&gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n&gt;&gt;&gt; resolver = AreaFeatureResolver(\n...     fill_color=PropertyMapper.from_kpi_value(color_scale),\n...     fill_opacity=PropertyMapper.from_item_attr('confidence', lambda c: 0.3 + 0.6 * c)\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_areas.py</code> <pre><code>class AreaFeatureResolver(FeatureResolver[ResolvedAreaFeature]):\n    \"\"\"\n    Resolves visual properties for polygon/area map elements.\n\n    Specialized feature resolver for area visualizations that handles polygon\n    geometries, fill colors, border styling, and opacity settings. Commonly\n    used for visualizing bidding zones, geographic regions, or any spatial\n    areas with associated data values.\n\n    Args:\n        fill_color: Area fill color (static value or PropertyMapper)\n        border_color: Border/stroke color (static value or PropertyMapper)  \n        border_width: Border width in pixels (static value or PropertyMapper)\n        fill_opacity: Fill transparency 0-1 (static value or PropertyMapper)\n        highlight_border_width: Border width on hover (defaults to border_width)\n        highlight_fill_opacity: Fill opacity on hover (defaults to 1.0)\n        tooltip: Tooltip content (True for auto-generated, False for none)\n        popup: Popup content (True/False/PropertyMapper)\n        geometry: Polygon geometry (defaults to 'geometry' attribute)\n        **property_mappers: Additional custom property mappings\n\n    Examples:\n        Basic area visualization:\n        &gt;&gt;&gt; resolver = AreaFeatureResolver(\n        ...     fill_color='#FF0000',\n        ...     border_color='white',\n        ...     fill_opacity=0.8\n        ... )\n\n        Data-driven coloring:\n        &gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n        &gt;&gt;&gt; resolver = AreaFeatureResolver(\n        ...     fill_color=PropertyMapper.from_kpi_value(color_scale),\n        ...     fill_opacity=PropertyMapper.from_item_attr('confidence', lambda c: 0.3 + 0.6 * c)\n        ... )\n    \"\"\"\n    def __init__(\n            self,\n            fill_color: PropertyMapper | str = '#D2D2D2',\n            border_color: PropertyMapper | str = 'white',\n            border_width: PropertyMapper | float = 2.0,\n            fill_opacity: PropertyMapper | float = 0.8,\n            highlight_border_width: PropertyMapper | float = None,\n            highlight_fill_opacity: PropertyMapper | float = 1.0,\n            tooltip: PropertyMapper | str | bool = True,\n            popup: PropertyMapper | folium.Popup | bool = False,\n            geometry: PropertyMapper | Polygon = None,\n            **property_mappers: PropertyMapper | Any,\n    ):\n        mappers = dict(\n            fill_color=fill_color,\n            border_color=border_color,\n            border_width=border_width,\n            fill_opacity=fill_opacity,\n            highlight_border_width=highlight_border_width,\n            highlight_fill_opacity=highlight_fill_opacity,\n            tooltip=tooltip,\n            popup=popup,\n            geometry=self._explicit_or_fallback(geometry, self._default_geometry_mapper()),\n            **property_mappers\n        )\n        super().__init__(feature_type=ResolvedAreaFeature, **mappers)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/areas/#mescal.visualizations.folium_viz_system.viz_areas.AreaGenerator","title":"AreaGenerator","text":"<p>               Bases: <code>FoliumObjectGenerator[AreaFeatureResolver]</code></p> <p>Generates folium GeoJson polygon objects for area visualizations.</p> <p>Creates interactive map polygons from data items with computed styling properties. Handles polygon and multipolygon geometries, applies styling, and adds tooltips/popups for user interaction.</p> <p>Commonly used for visualizing: - Bidding zones colored by electricity prices - Geographic regions sized by population or economic data - Network areas colored by KPI values - Administrative boundaries with associated statistics</p> <p>Examples:</p> <p>Basic usage pattern:</p> <pre><code>&gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n&gt;&gt;&gt; generator = AreaGenerator(\n...     AreaFeatureResolver(\n...         fill_color=PropertyMapper.from_kpi_value(color_scale),\n...         tooltip=True\n...     )\n... )\n&gt;&gt;&gt; fg = folium.FeatureGroup('Bidding Zones')\n&gt;&gt;&gt; generator.generate_objects_for_kpi_collection(price_kpis, fg)\n&gt;&gt;&gt; fg.add_to(map)\n</code></pre> <p>Combined with other generators:</p> <pre><code>&gt;&gt;&gt; # Areas for zones, lines for interconnectors\n&gt;&gt;&gt; area_gen = AreaGenerator(...)\n&gt;&gt;&gt; line_gen = LineGenerator(...)\n&gt;&gt;&gt; \n&gt;&gt;&gt; area_gen.generate_objects_for_model_df(zones_df, feature_group)\n&gt;&gt;&gt; line_gen.generate_objects_for_model_df(borders_df, feature_group)\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_areas.py</code> <pre><code>class AreaGenerator(FoliumObjectGenerator[AreaFeatureResolver]):\n    \"\"\"\n    Generates folium GeoJson polygon objects for area visualizations.\n\n    Creates interactive map polygons from data items with computed styling\n    properties. Handles polygon and multipolygon geometries, applies styling,\n    and adds tooltips/popups for user interaction.\n\n    Commonly used for visualizing:\n    - Bidding zones colored by electricity prices\n    - Geographic regions sized by population or economic data  \n    - Network areas colored by KPI values\n    - Administrative boundaries with associated statistics\n\n    Examples:\n        Basic usage pattern:\n        &gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n        &gt;&gt;&gt; generator = AreaGenerator(\n        ...     AreaFeatureResolver(\n        ...         fill_color=PropertyMapper.from_kpi_value(color_scale),\n        ...         tooltip=True\n        ...     )\n        ... )\n        &gt;&gt;&gt; fg = folium.FeatureGroup('Bidding Zones')\n        &gt;&gt;&gt; generator.generate_objects_for_kpi_collection(price_kpis, fg)\n        &gt;&gt;&gt; fg.add_to(map)\n\n        Combined with other generators:\n        &gt;&gt;&gt; # Areas for zones, lines for interconnectors\n        &gt;&gt;&gt; area_gen = AreaGenerator(...)\n        &gt;&gt;&gt; line_gen = LineGenerator(...)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; area_gen.generate_objects_for_model_df(zones_df, feature_group)\n        &gt;&gt;&gt; line_gen.generate_objects_for_model_df(borders_df, feature_group)\n    \"\"\"\n    \"\"\"Generates folium GeoJson objects for area geometries.\"\"\"\n\n    def _feature_resolver_type(self) -&gt; Type[AreaFeatureResolver]:\n        return AreaFeatureResolver\n\n    def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n        \"\"\"\n        Generate and add a folium GeoJson polygon to the feature group.\n\n        Args:\n            data_item: Data item containing polygon geometry and associated data\n            feature_group: Folium feature group to add the polygon to\n        \"\"\"\n        style = self.feature_resolver.resolve_feature(data_item)\n\n        geometry = style.geometry\n        if not isinstance(geometry, (Polygon, MultiPolygon)):\n            return\n\n        style_dict = {\n            'fillColor': style.fill_color,\n            'color': style.border_color,\n            'weight': style.border_width,\n            'fillOpacity': style.fill_opacity\n        }\n\n        highlight_dict = style_dict.copy()\n        highlight_dict['weight'] = style.highlight_border_width\n        highlight_dict['fillOpacity'] = style.highlight_fill_opacity\n\n        geojson_data = {\n            \"type\": \"Feature\",\n            \"geometry\": geometry.__geo_interface__,\n            \"properties\": {\"tooltip\": style.tooltip}\n        }\n\n        folium.GeoJson(\n            geojson_data,\n            style_function=lambda x, s=style_dict: s,\n            highlight_function=lambda x, h=highlight_dict: h,\n            tooltip=folium.GeoJsonTooltip(fields=['tooltip'], aliases=[''], sticky=True) if style.tooltip else None,\n            popup=style.popup\n        ).add_to(feature_group)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/areas/#mescal.visualizations.folium_viz_system.viz_areas.AreaGenerator.generate","title":"generate","text":"<pre><code>generate(data_item: VisualizableDataItem, feature_group: FeatureGroup) -&gt; None\n</code></pre> <p>Generate and add a folium GeoJson polygon to the feature group.</p> <p>Parameters:</p> Name Type Description Default <code>data_item</code> <code>VisualizableDataItem</code> <p>Data item containing polygon geometry and associated data</p> required <code>feature_group</code> <code>FeatureGroup</code> <p>Folium feature group to add the polygon to</p> required Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_areas.py</code> <pre><code>def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n    \"\"\"\n    Generate and add a folium GeoJson polygon to the feature group.\n\n    Args:\n        data_item: Data item containing polygon geometry and associated data\n        feature_group: Folium feature group to add the polygon to\n    \"\"\"\n    style = self.feature_resolver.resolve_feature(data_item)\n\n    geometry = style.geometry\n    if not isinstance(geometry, (Polygon, MultiPolygon)):\n        return\n\n    style_dict = {\n        'fillColor': style.fill_color,\n        'color': style.border_color,\n        'weight': style.border_width,\n        'fillOpacity': style.fill_opacity\n    }\n\n    highlight_dict = style_dict.copy()\n    highlight_dict['weight'] = style.highlight_border_width\n    highlight_dict['fillOpacity'] = style.highlight_fill_opacity\n\n    geojson_data = {\n        \"type\": \"Feature\",\n        \"geometry\": geometry.__geo_interface__,\n        \"properties\": {\"tooltip\": style.tooltip}\n    }\n\n    folium.GeoJson(\n        geojson_data,\n        style_function=lambda x, s=style_dict: s,\n        highlight_function=lambda x, h=highlight_dict: h,\n        tooltip=folium.GeoJsonTooltip(fields=['tooltip'], aliases=[''], sticky=True) if style.tooltip else None,\n        popup=style.popup\n    ).add_to(feature_group)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/arrow_icon/","title":"MESCAL Folium Area Visualization System","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/arrow_icon/#mescal.visualizations.folium_viz_system.viz_arrow_icon","title":"viz_arrow_icon","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/arrow_icon/#mescal.visualizations.folium_viz_system.viz_arrow_icon.ResolvedArrowIconFeature","title":"ResolvedArrowIconFeature  <code>dataclass</code>","text":"<p>               Bases: <code>ResolvedFeature</code></p> <p>Resolved visual properties for animated arrow icon elements.</p> <p>Container for all computed styling properties of arrow icon visualizations, including position, orientation, colors, animation settings, and size. Used by ArrowIconGenerator to create folium markers with SVG arrow icons.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_arrow_icon.py</code> <pre><code>@dataclass\nclass ResolvedArrowIconFeature(ResolvedFeature):\n    \"\"\"\n    Resolved visual properties for animated arrow icon elements.\n\n    Container for all computed styling properties of arrow icon visualizations,\n    including position, orientation, colors, animation settings, and size.\n    Used by ArrowIconGenerator to create folium markers with SVG arrow icons.\n    \"\"\"\n    @property\n    def location(self) -&gt; Point:\n        return self.get('location')\n\n    @property\n    def offset(self) -&gt; tuple[float, float]:\n        return self.get('offset')\n\n    @property\n    def azimuth_angle(self) -&gt; float:\n        return self.get('azimuth_angle')\n\n    @property\n    def arrow_type(self) -&gt; 'ArrowTypeEnum':\n        return self.get('arrow_type')\n\n    @property\n    def color(self) -&gt; str:\n        return self.get('color')\n\n    @property\n    def stroke_width(self) -&gt; int:\n        return self.get('stroke_width')\n\n    @property\n    def width(self) -&gt; int:\n        return self.get('width')\n\n    @property\n    def height(self) -&gt; int:\n        return self.get('height')\n\n    @property\n    def speed_in_px_per_second(self) -&gt; float:\n        return self.get('speed_in_px_per_second')\n\n    @property\n    def speed_in_duration_seconds(self) -&gt; float:\n        return self.get('speed_in_duration_seconds')\n\n    @property\n    def num_arrows(self) -&gt; int:\n        return self.get('num_arrows')\n\n    @property\n    def opacity(self) -&gt; float:\n        return self.get('opacity')\n\n    @property\n    def reverse_direction(self) -&gt; bool:\n        return self.get('reverse_direction')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/arrow_icon/#mescal.visualizations.folium_viz_system.viz_arrow_icon.ArrowIconFeatureResolver","title":"ArrowIconFeatureResolver","text":"<p>               Bases: <code>FeatureResolver[ResolvedArrowIconFeature]</code></p> <p>Resolves visual properties for animated arrow icon elements.</p> <p>Specialized feature resolver for arrow icon visualizations that handles point locations, arrow styling, animation parameters, and directional indicators. Commonly used for visualizing directional flows, network connections, or any point-based data with directional significance.</p> <p>Integrates with the captain_arro library to provide various arrow types and animation effects for dynamic flow visualization.</p> <p>Parameters:</p> Name Type Description Default <code>arrow_type</code> <code>Union[PropertyMapper, ArrowTypeEnum]</code> <p>Type of arrow from ArrowTypeEnum (static value or PropertyMapper)</p> <code>None</code> <code>color</code> <code>PropertyMapper | str</code> <p>Arrow color (static value or PropertyMapper)</p> <code>'#2563eb'</code> <code>stroke_width</code> <code>PropertyMapper | int</code> <p>Arrow stroke width in pixels (static value or PropertyMapper)</p> <code>8</code> <code>width</code> <code>PropertyMapper | int</code> <p>Arrow icon width in pixels (static value or PropertyMapper)</p> <code>60</code> <code>height</code> <code>PropertyMapper | int</code> <p>Arrow icon height in pixels (static value or PropertyMapper)</p> <code>60</code> <code>speed_in_px_per_second</code> <code>PropertyMapper | float | None</code> <p>Animation speed in pixels/second (static value or PropertyMapper)</p> <code>20.0</code> <code>speed_in_duration_seconds</code> <code>PropertyMapper | float | None</code> <p>Animation duration in seconds (static value or PropertyMapper)</p> <code>None</code> <code>num_arrows</code> <code>PropertyMapper | int</code> <p>Number of animated arrows (static value or PropertyMapper)</p> <code>3</code> <code>opacity</code> <code>PropertyMapper | float</code> <p>Icon opacity 0-1 (static value or PropertyMapper)</p> <code>0.8</code> <code>reverse_direction</code> <code>PropertyMapper | bool</code> <p>Reverse arrow direction (static value or PropertyMapper)</p> <code>False</code> <code>tooltip</code> <code>PropertyMapper | str | bool</code> <p>Tooltip content (True for auto-generated, False for none)</p> <code>True</code> <code>popup</code> <code>PropertyMapper | Popup | bool</code> <p>Popup content (True/False/PropertyMapper)</p> <code>False</code> <code>location</code> <code>PropertyMapper | Point</code> <p>Point location (defaults to smart location detection)</p> <code>None</code> <code>rotation_angle</code> <p>Arrow rotation angle in degrees (static value or PropertyMapper)</p> required <code>**property_mappers</code> <code>PropertyMapper | Any</code> <p>Additional custom property mappings</p> <code>{}</code> <p>Examples:</p> <p>Basic directional flow arrows:</p> <pre><code>&gt;&gt;&gt; from captain_arro import ArrowTypeEnum\n&gt;&gt;&gt; resolver = ArrowIconFeatureResolver(\n...     arrow_type=ArrowTypeEnum.MOVING_FLOW_ARROW,\n...     color='#FF0000',\n...     width=50,\n...     height=30\n... )\n</code></pre> <p>Data-driven flow visualization:</p> <pre><code>&gt;&gt;&gt; flow_color_scale = SegmentedContinuousColorscale(...)\n&gt;&gt;&gt; size_scale = SegmentedContinuousInputToContinuousOutputMapping(...)\n&gt;&gt;&gt; resolver = ArrowIconFeatureResolver(\n...     arrow_type=PropertyMapper.from_kpi_value(\n...         lambda v: ArrowTypeEnum.SPOTLIGHT_FLOW_ARROW if abs(v) &gt; 100 \n...                   else ArrowTypeEnum.BOUNCING_SPREAD_ARROW\n...     ),\n...     color=PropertyMapper.from_kpi_value(flow_color_scale),\n...     width=PropertyMapper.from_kpi_value(size_scale),\n...     reverse_direction=PropertyMapper.from_kpi_value(lambda v: v &lt; 0),\n...     rotation_angle=PropertyMapper.from_item_attr('azimuth_angle')\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_arrow_icon.py</code> <pre><code>class ArrowIconFeatureResolver(FeatureResolver[ResolvedArrowIconFeature]):\n    \"\"\"\n    Resolves visual properties for animated arrow icon elements.\n\n    Specialized feature resolver for arrow icon visualizations that handles point\n    locations, arrow styling, animation parameters, and directional indicators.\n    Commonly used for visualizing directional flows, network connections, or\n    any point-based data with directional significance.\n\n    Integrates with the captain_arro library to provide various arrow types\n    and animation effects for dynamic flow visualization.\n\n    Args:\n        arrow_type: Type of arrow from ArrowTypeEnum (static value or PropertyMapper)\n        color: Arrow color (static value or PropertyMapper)\n        stroke_width: Arrow stroke width in pixels (static value or PropertyMapper)\n        width: Arrow icon width in pixels (static value or PropertyMapper)\n        height: Arrow icon height in pixels (static value or PropertyMapper)\n        speed_in_px_per_second: Animation speed in pixels/second (static value or PropertyMapper)\n        speed_in_duration_seconds: Animation duration in seconds (static value or PropertyMapper)\n        num_arrows: Number of animated arrows (static value or PropertyMapper)\n        opacity: Icon opacity 0-1 (static value or PropertyMapper)\n        reverse_direction: Reverse arrow direction (static value or PropertyMapper)\n        tooltip: Tooltip content (True for auto-generated, False for none)\n        popup: Popup content (True/False/PropertyMapper)\n        location: Point location (defaults to smart location detection)\n        rotation_angle: Arrow rotation angle in degrees (static value or PropertyMapper)\n        **property_mappers: Additional custom property mappings\n\n    Examples:\n        Basic directional flow arrows:\n        &gt;&gt;&gt; from captain_arro import ArrowTypeEnum\n        &gt;&gt;&gt; resolver = ArrowIconFeatureResolver(\n        ...     arrow_type=ArrowTypeEnum.MOVING_FLOW_ARROW,\n        ...     color='#FF0000',\n        ...     width=50,\n        ...     height=30\n        ... )\n\n        Data-driven flow visualization:\n        &gt;&gt;&gt; flow_color_scale = SegmentedContinuousColorscale(...)\n        &gt;&gt;&gt; size_scale = SegmentedContinuousInputToContinuousOutputMapping(...)\n        &gt;&gt;&gt; resolver = ArrowIconFeatureResolver(\n        ...     arrow_type=PropertyMapper.from_kpi_value(\n        ...         lambda v: ArrowTypeEnum.SPOTLIGHT_FLOW_ARROW if abs(v) &gt; 100 \n        ...                   else ArrowTypeEnum.BOUNCING_SPREAD_ARROW\n        ...     ),\n        ...     color=PropertyMapper.from_kpi_value(flow_color_scale),\n        ...     width=PropertyMapper.from_kpi_value(size_scale),\n        ...     reverse_direction=PropertyMapper.from_kpi_value(lambda v: v &lt; 0),\n        ...     rotation_angle=PropertyMapper.from_item_attr('azimuth_angle')\n        ... )\n    \"\"\"\n    def __init__(\n            self,\n            arrow_type: Union[PropertyMapper, 'ArrowTypeEnum'] = None,\n            color: PropertyMapper | str = '#2563eb',\n            stroke_width: PropertyMapper | int = 8,\n            width: PropertyMapper | int = 60,\n            height: PropertyMapper | int = 60,\n            speed_in_px_per_second: PropertyMapper | float | None = 20.0,\n            speed_in_duration_seconds: PropertyMapper | float | None = None,\n            num_arrows: PropertyMapper | int = 3,\n            opacity: PropertyMapper | float = 0.8,\n            reverse_direction: PropertyMapper | bool = False,\n            tooltip: PropertyMapper | str | bool = True,\n            popup: PropertyMapper | folium.Popup | bool = False,\n            location: PropertyMapper | Point = None,\n            offset: PropertyMapper | tuple[float, float] = (0, 0),\n            azimuth_angle: PropertyMapper | float = None,\n            **property_mappers: PropertyMapper | Any,\n    ):\n        from captain_arro import ArrowTypeEnum\n        mappers = dict(\n            arrow_type=arrow_type or ArrowTypeEnum.MOVING_FLOW_ARROW,\n            color=color,\n            stroke_width=stroke_width,\n            width=width,\n            height=height,\n            speed_in_px_per_second=speed_in_px_per_second,\n            speed_in_duration_seconds=speed_in_duration_seconds,\n            num_arrows=num_arrows,\n            opacity=opacity,\n            reverse_direction=reverse_direction,\n            tooltip=tooltip,\n            popup=popup,\n            location=self._explicit_or_fallback(location, self._default_location_mapper()),\n            offset=offset,\n            azimuth_angle=self._explicit_or_fallback(azimuth_angle, self._default_rotation_angle_mapper()),\n            **property_mappers\n        )\n        super().__init__(feature_type=ResolvedArrowIconFeature, **mappers)\n\n    @staticmethod\n    def _default_rotation_angle_mapper() -&gt; PropertyMapper:\n\n        def get_rotation_angle(data_item: VisualizableDataItem) -&gt; float | None:\n            for k in ['azimuth_angle', 'rotation_angle', 'projection_angle']:\n                if data_item.object_has_attribute(k):\n                    angle = data_item.get_object_attribute(k)\n                    return angle\n            return None\n\n        return PropertyMapper(get_rotation_angle)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/arrow_icon/#mescal.visualizations.folium_viz_system.viz_arrow_icon.ArrowIconGenerator","title":"ArrowIconGenerator","text":"<p>               Bases: <code>FoliumObjectGenerator[ArrowIconFeatureResolver]</code></p> <p>Generates folium Marker objects with animated SVG arrow icons.</p> <p>Creates interactive map markers featuring animated arrow icons from the captain_arro library. Handles SVG generation, base64 encoding, rotation, and integration with folium's marker system.</p> <p>Commonly used for visualizing: - Directional border flow directions with magnitude-based styling - Border price-spread with magnitude-based styling - Any point-based directional data with animation needs</p> <p>Examples:</p> <p>Power flow visualization:</p> <pre><code>&gt;&gt;&gt; from captain_arro import ArrowTypeEnum\n&gt;&gt;&gt; flow_color_scale = SegmentedContinuousColorscale(...)\n&gt;&gt;&gt; size_mapping = SegmentedContinuousInputToContinuousOutputMapping(...)\n&gt;&gt;&gt; \n&gt;&gt;&gt; generator = ArrowIconGenerator(\n...     ArrowIconFeatureResolver(\n...         arrow_type=PropertyMapper.from_kpi_value(\n...             lambda v: ArrowTypeEnum.MOVING_FLOW_ARROW if abs(v) &gt; 50\n...                       else ArrowTypeEnum.BOUNCING_SPREAD_ARROW\n...         ),\n...         color=PropertyMapper.from_kpi_value(lambda v: flow_color_scale(abs(v))),\n...         width=PropertyMapper.from_kpi_value(lambda v: size_mapping(abs(v))),\n...         reverse_direction=PropertyMapper.from_kpi_value(lambda v: v &lt; 0),\n...         rotation_angle=PropertyMapper.from_item_attr('azimuth_angle'),\n...         speed_in_duration_seconds=4\n...     )\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; fg = folium.FeatureGroup('Flow Arrows')\n&gt;&gt;&gt; generator.generate_objects_for_kpi_collection(flow_kpis, fg)\n&gt;&gt;&gt; fg.add_to(map)\n</code></pre> <p>Border flow indicators:</p> <pre><code>&gt;&gt;&gt; generator.generate_objects_for_model_df(border_df, feature_group)\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_arrow_icon.py</code> <pre><code>class ArrowIconGenerator(FoliumObjectGenerator[ArrowIconFeatureResolver]):\n    \"\"\"\n    Generates folium Marker objects with animated SVG arrow icons.\n\n    Creates interactive map markers featuring animated arrow icons from the\n    captain_arro library. Handles SVG generation, base64 encoding, rotation,\n    and integration with folium's marker system.\n\n    Commonly used for visualizing:\n    - Directional border flow directions with magnitude-based styling\n    - Border price-spread with magnitude-based styling\n    - Any point-based directional data with animation needs\n\n    Examples:\n        Power flow visualization:\n        &gt;&gt;&gt; from captain_arro import ArrowTypeEnum\n        &gt;&gt;&gt; flow_color_scale = SegmentedContinuousColorscale(...)\n        &gt;&gt;&gt; size_mapping = SegmentedContinuousInputToContinuousOutputMapping(...)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; generator = ArrowIconGenerator(\n        ...     ArrowIconFeatureResolver(\n        ...         arrow_type=PropertyMapper.from_kpi_value(\n        ...             lambda v: ArrowTypeEnum.MOVING_FLOW_ARROW if abs(v) &gt; 50\n        ...                       else ArrowTypeEnum.BOUNCING_SPREAD_ARROW\n        ...         ),\n        ...         color=PropertyMapper.from_kpi_value(lambda v: flow_color_scale(abs(v))),\n        ...         width=PropertyMapper.from_kpi_value(lambda v: size_mapping(abs(v))),\n        ...         reverse_direction=PropertyMapper.from_kpi_value(lambda v: v &lt; 0),\n        ...         rotation_angle=PropertyMapper.from_item_attr('azimuth_angle'),\n        ...         speed_in_duration_seconds=4\n        ...     )\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; fg = folium.FeatureGroup('Flow Arrows')\n        &gt;&gt;&gt; generator.generate_objects_for_kpi_collection(flow_kpis, fg)\n        &gt;&gt;&gt; fg.add_to(map)\n\n        Border flow indicators:\n        &gt;&gt;&gt; generator.generate_objects_for_model_df(border_df, feature_group)\n    \"\"\"\n    def _feature_resolver_type(self) -&gt; Type[ArrowIconFeatureResolver]:\n        return ArrowIconFeatureResolver\n\n    def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n        \"\"\"\n        Generate and add a folium Marker with animated SVG arrow icon.\n\n        Args:\n            data_item: Data item containing point location and associated data\n            feature_group: Folium feature group to add the arrow marker to\n        \"\"\"\n        style = self.feature_resolver.resolve_feature(data_item)\n        if style.location is None:\n            return\n\n        svg_content = self._generate_arrow_svg(style, data_item)\n        encoded_svg = base64.b64encode(svg_content.encode()).decode()\n\n        angle = float(style.azimuth_angle) - 90  # Folium counts clockwise from right-pointing direction; normal convention is CCW\n        x_offset, y_offset = style.offset\n\n        icon_html = f'''\n            &lt;div style=\"\n                position: absolute;\n                left: 50%;\n                top: 50%;\n                transform: translate(-50%, -50%) translate({x_offset}px, {-y_offset}px) rotate({angle}deg);\n                opacity: {style.opacity};\n            \"&gt;\n                &lt;img src=\"data:image/svg+xml;base64,{encoded_svg}\" \n                     width=\"{style.width}\" \n                     height=\"{style.height}\"&gt;\n                &lt;/img&gt;\n            &lt;/div&gt;\n        '''\n\n        icon = folium.DivIcon(\n            html=icon_html,\n            icon_size=(style.width, style.height),\n            icon_anchor=(style.width // 2, style.height // 2)\n        )\n        folium.Marker(\n            location=(style.location.y, style.location.x),\n            icon=icon,\n            tooltip=style.tooltip,\n            popup=style.popup\n        ).add_to(feature_group)\n\n    def _generate_arrow_svg(self, style: ResolvedArrowIconFeature, data_item: VisualizableDataItem) -&gt; str:\n        from inspect import signature\n        from captain_arro import get_generator_for_arrow_type\n\n        def safe_init(cls: type, kwargs: dict):\n            init_params = signature(cls.__init__).parameters\n            accepted_keys = {\n                k for k in init_params\n                if k != 'self' and init_params[k].kind in (\n                init_params[k].POSITIONAL_OR_KEYWORD, init_params[k].KEYWORD_ONLY)\n            }\n            filtered_kwargs = {k: v for k, v in kwargs.items() if k in accepted_keys}\n            return cls(**filtered_kwargs)\n\n        arrow_type = style.arrow_type\n        generator_class = get_generator_for_arrow_type(arrow_type)\n        arrow_style_kwargs = style.to_dict()\n        if 'direction' not in arrow_style_kwargs:\n            arrow_style_kwargs['direction'] = 'left' if style.reverse_direction else 'right'\n        return safe_init(generator_class, arrow_style_kwargs).generate_svg(unique_id=True)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/arrow_icon/#mescal.visualizations.folium_viz_system.viz_arrow_icon.ArrowIconGenerator.generate","title":"generate","text":"<pre><code>generate(data_item: VisualizableDataItem, feature_group: FeatureGroup) -&gt; None\n</code></pre> <p>Generate and add a folium Marker with animated SVG arrow icon.</p> <p>Parameters:</p> Name Type Description Default <code>data_item</code> <code>VisualizableDataItem</code> <p>Data item containing point location and associated data</p> required <code>feature_group</code> <code>FeatureGroup</code> <p>Folium feature group to add the arrow marker to</p> required Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_arrow_icon.py</code> <pre><code>def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n    \"\"\"\n    Generate and add a folium Marker with animated SVG arrow icon.\n\n    Args:\n        data_item: Data item containing point location and associated data\n        feature_group: Folium feature group to add the arrow marker to\n    \"\"\"\n    style = self.feature_resolver.resolve_feature(data_item)\n    if style.location is None:\n        return\n\n    svg_content = self._generate_arrow_svg(style, data_item)\n    encoded_svg = base64.b64encode(svg_content.encode()).decode()\n\n    angle = float(style.azimuth_angle) - 90  # Folium counts clockwise from right-pointing direction; normal convention is CCW\n    x_offset, y_offset = style.offset\n\n    icon_html = f'''\n        &lt;div style=\"\n            position: absolute;\n            left: 50%;\n            top: 50%;\n            transform: translate(-50%, -50%) translate({x_offset}px, {-y_offset}px) rotate({angle}deg);\n            opacity: {style.opacity};\n        \"&gt;\n            &lt;img src=\"data:image/svg+xml;base64,{encoded_svg}\" \n                 width=\"{style.width}\" \n                 height=\"{style.height}\"&gt;\n            &lt;/img&gt;\n        &lt;/div&gt;\n    '''\n\n    icon = folium.DivIcon(\n        html=icon_html,\n        icon_size=(style.width, style.height),\n        icon_anchor=(style.width // 2, style.height // 2)\n    )\n    folium.Marker(\n        location=(style.location.y, style.location.x),\n        icon=icon,\n        tooltip=style.tooltip,\n        popup=style.popup\n    ).add_to(feature_group)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/circle_marker/","title":"MESCAL Folium CircleMarker Visualization System","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/circle_marker/#mescal.visualizations.folium_viz_system.viz_circle_marker","title":"viz_circle_marker","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/circle_marker/#mescal.visualizations.folium_viz_system.viz_circle_marker.ResolvedCircleMarkerFeature","title":"ResolvedCircleMarkerFeature  <code>dataclass</code>","text":"<p>               Bases: <code>ResolvedFeature</code></p> <p>Specialized style container for circle marker visualizations.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_circle_marker.py</code> <pre><code>@dataclass\nclass ResolvedCircleMarkerFeature(ResolvedFeature):\n    \"\"\"Specialized style container for circle marker visualizations.\"\"\"\n\n    @property\n    def location(self) -&gt; Point:\n        return self.get('location')\n\n    @property\n    def fill_color(self) -&gt; str:\n        return self.get('fill_color')\n\n    @property\n    def border_color(self) -&gt; str:\n        return self.get('border_color')\n\n    @property\n    def radius(self) -&gt; float:\n        return self.get('radius')\n\n    @property\n    def border_width(self) -&gt; float:\n        return self.get('border_width')\n\n    @property\n    def fill_opacity(self) -&gt; float:\n        return self.get('fill_opacity')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/circle_marker/#mescal.visualizations.folium_viz_system.viz_circle_marker.CircleMarkerGenerator","title":"CircleMarkerGenerator","text":"<p>               Bases: <code>FoliumObjectGenerator[CircleMarkerFeatureResolver]</code></p> <p>Generates folium CircleMarker objects for point geometries.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_circle_marker.py</code> <pre><code>class CircleMarkerGenerator(FoliumObjectGenerator[CircleMarkerFeatureResolver]):\n    \"\"\"Generates folium CircleMarker objects for point geometries.\"\"\"\n\n    def _feature_resolver_type(self) -&gt; Type[CircleMarkerFeatureResolver]:\n        return CircleMarkerFeatureResolver\n\n    def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n        style = self.feature_resolver.resolve_feature(data_item)\n        if style.location is None:\n            return\n\n        folium.CircleMarker(\n            location=(style.location.y, style.location.x),\n            tooltip=style.tooltip,\n            popup=style.popup,\n            radius=style.radius,\n            color=style.border_color,\n            fillColor=style.fill_color,\n            fillOpacity=style.fill_opacity,\n            weight=style.border_width,\n        ).add_to(feature_group)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/data_item/","title":"MESCAL Folium-Visualizable Data Item","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mescal.visualizations.folium_viz_system.visualizable_data_item","title":"visualizable_data_item","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mescal.visualizations.folium_viz_system.visualizable_data_item.VisualizableDataItem","title":"VisualizableDataItem","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract interface for data items that can be visualized on maps.</p> <p>Defines the contract for all data items that can be processed by the folium visualization system. Provides attribute access, tooltip data generation, and text representation for map elements.</p> <p>This abstraction enables polymorphic handling of different data sources (model DataFrames, KPI objects, custom data) within the visualization pipeline while maintaining consistent interface expectations.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>class VisualizableDataItem(ABC):\n    \"\"\"\n    Abstract interface for data items that can be visualized on maps.\n\n    Defines the contract for all data items that can be processed by the\n    folium visualization system. Provides attribute access, tooltip data\n    generation, and text representation for map elements.\n\n    This abstraction enables polymorphic handling of different data sources\n    (model DataFrames, KPI objects, custom data) within the visualization\n    pipeline while maintaining consistent interface expectations.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n    @abstractmethod\n    def get_name(self) -&gt; str:\n        \"\"\"Get a representative name for the object.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_text_representation(self) -&gt; str:\n        \"\"\"Get a representative text for the object.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_tooltip_data(self) -&gt; dict:\n        \"\"\"Get data for tooltip display.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_object_attribute(self, attribute: str) -&gt; Any:\n        \"\"\"Get value for styling from specified column.\"\"\"\n        pass\n\n    @abstractmethod\n    def object_has_attribute(self, attribute: str) -&gt; bool:\n        pass\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mescal.visualizations.folium_viz_system.visualizable_data_item.VisualizableDataItem.get_name","title":"get_name  <code>abstractmethod</code>","text":"<pre><code>get_name() -&gt; str\n</code></pre> <p>Get a representative name for the object.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>@abstractmethod\ndef get_name(self) -&gt; str:\n    \"\"\"Get a representative name for the object.\"\"\"\n    pass\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mescal.visualizations.folium_viz_system.visualizable_data_item.VisualizableDataItem.get_text_representation","title":"get_text_representation  <code>abstractmethod</code>","text":"<pre><code>get_text_representation() -&gt; str\n</code></pre> <p>Get a representative text for the object.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>@abstractmethod\ndef get_text_representation(self) -&gt; str:\n    \"\"\"Get a representative text for the object.\"\"\"\n    pass\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mescal.visualizations.folium_viz_system.visualizable_data_item.VisualizableDataItem.get_tooltip_data","title":"get_tooltip_data  <code>abstractmethod</code>","text":"<pre><code>get_tooltip_data() -&gt; dict\n</code></pre> <p>Get data for tooltip display.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>@abstractmethod\ndef get_tooltip_data(self) -&gt; dict:\n    \"\"\"Get data for tooltip display.\"\"\"\n    pass\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mescal.visualizations.folium_viz_system.visualizable_data_item.VisualizableDataItem.get_object_attribute","title":"get_object_attribute  <code>abstractmethod</code>","text":"<pre><code>get_object_attribute(attribute: str) -&gt; Any\n</code></pre> <p>Get value for styling from specified column.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>@abstractmethod\ndef get_object_attribute(self, attribute: str) -&gt; Any:\n    \"\"\"Get value for styling from specified column.\"\"\"\n    pass\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mescal.visualizations.folium_viz_system.visualizable_data_item.ModelDataItem","title":"ModelDataItem","text":"<p>               Bases: <code>VisualizableDataItem</code></p> <p>Visualizable data item for model DataFrame rows.</p> <p>Wraps pandas Series objects (DataFrame rows) to provide the VisualizableDataItem interface. Commonly used for visualizing static model data like network topology, geographic boundaries, or reference datasets.</p> <p>Handles attribute access from DataFrame columns, provides meaningful object naming, and generates informative tooltips from available data.</p> <p>Parameters:</p> Name Type Description Default <code>object_data</code> <code>Series</code> <p>Pandas Series representing a DataFrame row</p> required <code>object_type</code> <code>str | Any</code> <p>Optional type identifier for the object</p> <code>None</code> <code>**kwargs</code> <p>Additional attributes to set on the data item</p> <code>{}</code> <p>Examples:</p> <p>Typical usage in generators:</p> <pre><code>&gt;&gt;&gt; for _, row in model_df.iterrows():\n...     data_item = ModelDataItem(row, object_type='BiddingZone')\n...     generator.generate(data_item, feature_group)\n</code></pre> <p>Access pattern:</p> <pre><code>&gt;&gt;&gt; data_item.get_object_attribute('geometry')  # From DataFrame column\n&gt;&gt;&gt; data_item.get_name()  # Object identifier\n&gt;&gt;&gt; data_item.get_tooltip_data()  # All available data for tooltip\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>class ModelDataItem(VisualizableDataItem):\n    \"\"\"\n    Visualizable data item for model DataFrame rows.\n\n    Wraps pandas Series objects (DataFrame rows) to provide the VisualizableDataItem\n    interface. Commonly used for visualizing static model data like network\n    topology, geographic boundaries, or reference datasets.\n\n    Handles attribute access from DataFrame columns, provides meaningful object\n    naming, and generates informative tooltips from available data.\n\n    Args:\n        object_data: Pandas Series representing a DataFrame row\n        object_type: Optional type identifier for the object\n        **kwargs: Additional attributes to set on the data item\n\n    Examples:\n        Typical usage in generators:\n        &gt;&gt;&gt; for _, row in model_df.iterrows():\n        ...     data_item = ModelDataItem(row, object_type='BiddingZone')\n        ...     generator.generate(data_item, feature_group)\n\n        Access pattern:\n        &gt;&gt;&gt; data_item.get_object_attribute('geometry')  # From DataFrame column\n        &gt;&gt;&gt; data_item.get_name()  # Object identifier\n        &gt;&gt;&gt; data_item.get_tooltip_data()  # All available data for tooltip\n    \"\"\"\n    OBJECT_NAME_COLUMNS = ['name', 'object_id', 'index', 'object_name']\n\n    def __init__(self, object_data: pd.Series, object_type: str | Any = None, **kwargs):\n        self.object_data = object_data\n        self.object_id = object_data.name\n        self.object_type = object_type\n        self.name_attributes = self.OBJECT_NAME_COLUMNS + [self.object_type]\n        super().__init__(**kwargs)\n\n    def get_name(self) -&gt; str:\n        return str(self.object_id)\n\n    def get_text_representation(self) -&gt; str:\n        return self.get_name()\n\n    def get_tooltip_data(self) -&gt; dict:\n        data = {'ID': self.object_id}\n        for col, value in self.object_data.items():\n            if pd.notna(value):\n                value_str = str(value)\n                if len(value_str) &gt; 50:\n                    value_str = value_str[:47] + \"...\"\n                data[col] = value_str\n        return data\n\n    def get_object_attribute(self, attribute: str) -&gt; Any:\n        if (attribute in self.name_attributes) and (attribute not in self.object_data):\n            return self.get_name()\n        return self.object_data.get(attribute)\n\n    def object_has_attribute(self, attribute: str) -&gt; bool:\n        return (attribute in self.object_data) or (attribute in self.name_attributes)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mescal.visualizations.folium_viz_system.visualizable_data_item.KPIDataItem","title":"KPIDataItem","text":"<p>               Bases: <code>VisualizableDataItem</code></p> <p>Visualizable data item for KPI objects.</p> <p>Wraps MESCAL KPI objects to provide the VisualizableDataItem interface. Combines KPI values with associated model object information for rich map visualization of computed energy system metrics.</p> <p>Handles attribute access from both KPI values and underlying model objects, provides formatted value representations, and generates enhanced tooltips showing both KPI information and object details.</p> <p>Parameters:</p> Name Type Description Default <code>kpi</code> <code>KPI</code> <p>MESCAL KPI object with computed value and metadata</p> required <code>kpi_collection</code> <code>KPICollection</code> <p>Optional KPI collection for context</p> <code>None</code> <code>**kwargs</code> <p>Additional attributes to set on the data item</p> <code>{}</code> <p>Examples:</p> <p>Typical usage in visualizers:</p> <pre><code>&gt;&gt;&gt; for kpi in kpi_collection:\n...     data_item = KPIDataItem(kpi, kpi_collection)\n...     generator.generate(data_item, feature_group)\n</code></pre> <p>Access patterns:</p> <pre><code>&gt;&gt;&gt; data_item.kpi.value  # Direct KPI value access\n&gt;&gt;&gt; data_item.get_object_attribute('geometry')  # From model object\n&gt;&gt;&gt; data_item.get_object_attribute('kpi_value')  # Alias for KPI value\n&gt;&gt;&gt; data_item.get_text_representation()  # Formatted value string\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>class KPIDataItem(VisualizableDataItem):\n    \"\"\"\n    Visualizable data item for KPI objects.\n\n    Wraps MESCAL KPI objects to provide the VisualizableDataItem interface.\n    Combines KPI values with associated model object information for rich\n    map visualization of computed energy system metrics.\n\n    Handles attribute access from both KPI values and underlying model objects,\n    provides formatted value representations, and generates enhanced tooltips\n    showing both KPI information and object details.\n\n    Args:\n        kpi: MESCAL KPI object with computed value and metadata\n        kpi_collection: Optional KPI collection for context\n        **kwargs: Additional attributes to set on the data item\n\n    Examples:\n        Typical usage in visualizers:\n        &gt;&gt;&gt; for kpi in kpi_collection:\n        ...     data_item = KPIDataItem(kpi, kpi_collection)\n        ...     generator.generate(data_item, feature_group)\n\n        Access patterns:\n        &gt;&gt;&gt; data_item.kpi.value  # Direct KPI value access\n        &gt;&gt;&gt; data_item.get_object_attribute('geometry')  # From model object\n        &gt;&gt;&gt; data_item.get_object_attribute('kpi_value')  # Alias for KPI value\n        &gt;&gt;&gt; data_item.get_text_representation()  # Formatted value string\n    \"\"\"\n\n    KPI_VALUE_COLUMNS = ['kpi_value', 'value', 'kpi']\n\n    def __init__(self, kpi: KPI, kpi_collection: KPICollection = None, **kwargs):\n        self.kpi = kpi\n        self.kpi_collection = kpi_collection\n        self._object_info = kpi.get_attributed_object_info_from_model()\n        self._model_item = ModelDataItem(self._object_info)\n        super().__init__(**kwargs)\n\n    def get_name(self) -&gt; str:\n        return str(self.kpi.name)\n\n    def get_text_representation(self) -&gt; str:\n        \"\"\"\n        Get formatted text representation of the KPI value.\n\n        Returns:\n            Formatted string representation of the KPI value\n        \"\"\"\n        return f\"{self.kpi.value:.1f}\"  # TODO: use pretty formatting and quantities etc.\n\n    def get_tooltip_data(self) -&gt; dict:\n        kpi_data = {\n            'KPI': self.kpi.get_kpi_name_with_dataset_name(),\n            'Value': str(self.kpi.quantity),\n        }\n        model_data = self._model_item.get_tooltip_data()\n        return {**kpi_data, **model_data}\n\n    def get_object_attribute(self, attribute: str) -&gt; Any:\n        if (attribute in self.KPI_VALUE_COLUMNS) and (not self._model_item.object_has_attribute(attribute)):\n            return self.kpi.value\n        return self._model_item.get_object_attribute(attribute)\n\n    def object_has_attribute(self, attribute: str) -&gt; bool:\n        if attribute in self.KPI_VALUE_COLUMNS:\n            return True\n        return self._model_item.object_has_attribute(attribute)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mescal.visualizations.folium_viz_system.visualizable_data_item.KPIDataItem.get_text_representation","title":"get_text_representation","text":"<pre><code>get_text_representation() -&gt; str\n</code></pre> <p>Get formatted text representation of the KPI value.</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string representation of the KPI value</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>def get_text_representation(self) -&gt; str:\n    \"\"\"\n    Get formatted text representation of the KPI value.\n\n    Returns:\n        Formatted string representation of the KPI value\n    \"\"\"\n    return f\"{self.kpi.value:.1f}\"  # TODO: use pretty formatting and quantities etc.\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/","title":"MESCAL Folium KPICollection Visualization System","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mescal.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPICollectionMapVisualizer","title":"KPICollectionMapVisualizer","text":"<p>High-level KPI collection map visualizer for energy system analysis.</p> <p>Main orchestrator for converting KPI collections into organized folium map visualizations. Handles KPI grouping, feature group creation, tooltip enhancement, and progress tracking. Designed to replicate and extend the functionality of the original KPIToMapVisualizerBase.</p> <p>Supports multiple generators for complex visualizations (e.g., areas with text overlays, lines with arrow indicators) and provides sophisticated KPI organization and related KPI discovery for enhanced user experience.</p> <p>Parameters:</p> Name Type Description Default <code>generators</code> <code>FoliumObjectGenerator | List[FoliumObjectGenerator]</code> <p>Single generator or list of generators for visualization</p> required <code>study_manager</code> <code>StudyManager</code> <p>StudyManager for enhanced KPI relationships and tooltips</p> <code>None</code> <code>include_related_kpis_in_tooltip</code> <code>bool</code> <p>Add related KPIs to tooltip display</p> <code>False</code> <code>kpi_grouping_manager</code> <code>KPIGroupingManager</code> <p>Custom grouping manager (optional)</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to data items</p> <code>{}</code> <p>Examples:</p> <p>Basic area visualization:</p> <pre><code>&gt;&gt;&gt; visualizer = KPICollectionMapVisualizer(\n...     generators=[\n...         AreaGenerator(\n...             AreaFeatureResolver(\n...                 fill_color=PropertyMapper.from_kpi_value(color_scale),\n...                 tooltip=True\n...             )\n...         )\n...     ],\n...     study_manager=study\n... )\n&gt;&gt;&gt; feature_groups = visualizer.get_feature_groups(price_kpis)\n&gt;&gt;&gt; for fg in feature_groups:\n...     fg.add_to(map)\n</code></pre> <p>Complex multi-layer visualization:</p> <pre><code>&gt;&gt;&gt; visualizer = KPICollectionMapVisualizer(\n...     generators=[\n...         AreaGenerator(AreaFeatureResolver(...)),\n...         TextOverlayGenerator(TextOverlayFeatureResolver(...))\n...     ],\n...     include_related_kpis_in_tooltip=True,\n...     study_manager=study\n... )\n&gt;&gt;&gt; visualizer.generate_and_add_feature_groups_to_map(\n...     kpi_collection, folium_map, show='first'\n... )\n</code></pre> <p>Flow visualization with arrows:</p> <pre><code>&gt;&gt;&gt; visualizer = KPICollectionMapVisualizer(\n...     generators=[\n...         ArrowIconGenerator(ArrowIconFeatureResolver(...)),\n...         TextOverlayGenerator(TextOverlayFeatureResolver(...))\n...     ]\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>class KPICollectionMapVisualizer:\n    \"\"\"\n    High-level KPI collection map visualizer for energy system analysis.\n\n    Main orchestrator for converting KPI collections into organized folium map\n    visualizations. Handles KPI grouping, feature group creation, tooltip\n    enhancement, and progress tracking. Designed to replicate and extend\n    the functionality of the original KPIToMapVisualizerBase.\n\n    Supports multiple generators for complex visualizations (e.g., areas with\n    text overlays, lines with arrow indicators) and provides sophisticated\n    KPI organization and related KPI discovery for enhanced user experience.\n\n    Args:\n        generators: Single generator or list of generators for visualization\n        study_manager: StudyManager for enhanced KPI relationships and tooltips\n        include_related_kpis_in_tooltip: Add related KPIs to tooltip display\n        kpi_grouping_manager: Custom grouping manager (optional)\n        **kwargs: Additional arguments passed to data items\n\n    Examples:\n        Basic area visualization:\n        &gt;&gt;&gt; visualizer = KPICollectionMapVisualizer(\n        ...     generators=[\n        ...         AreaGenerator(\n        ...             AreaFeatureResolver(\n        ...                 fill_color=PropertyMapper.from_kpi_value(color_scale),\n        ...                 tooltip=True\n        ...             )\n        ...         )\n        ...     ],\n        ...     study_manager=study\n        ... )\n        &gt;&gt;&gt; feature_groups = visualizer.get_feature_groups(price_kpis)\n        &gt;&gt;&gt; for fg in feature_groups:\n        ...     fg.add_to(map)\n\n        Complex multi-layer visualization:\n        &gt;&gt;&gt; visualizer = KPICollectionMapVisualizer(\n        ...     generators=[\n        ...         AreaGenerator(AreaFeatureResolver(...)),\n        ...         TextOverlayGenerator(TextOverlayFeatureResolver(...))\n        ...     ],\n        ...     include_related_kpis_in_tooltip=True,\n        ...     study_manager=study\n        ... )\n        &gt;&gt;&gt; visualizer.generate_and_add_feature_groups_to_map(\n        ...     kpi_collection, folium_map, show='first'\n        ... )\n\n        Flow visualization with arrows:\n        &gt;&gt;&gt; visualizer = KPICollectionMapVisualizer(\n        ...     generators=[\n        ...         ArrowIconGenerator(ArrowIconFeatureResolver(...)),\n        ...         TextOverlayGenerator(TextOverlayFeatureResolver(...))\n        ...     ]\n        ... )\n    \"\"\"\n\n    def __init__(\n            self,\n            generators: FoliumObjectGenerator | List[FoliumObjectGenerator],\n            study_manager: 'StudyManager' = None,\n            include_related_kpis_in_tooltip: bool = False,\n            kpi_grouping_manager: KPIGroupingManager = None,\n            **kwargs\n    ):\n        self.generators: List[FoliumObjectGenerator] = generators if isinstance(generators, list) else [generators]\n        self.study_manager = study_manager\n        self.include_related_kpis_in_tooltip = include_related_kpis_in_tooltip\n\n        self.grouping_manager = kpi_grouping_manager or KPIGroupingManager()\n\n        # Enhanced tooltip if needed\n        if self.include_related_kpis_in_tooltip:\n            for g in self.generators:\n                g.tooltip_generator = self._create_enhanced_tooltip_generator()\n        self.kwargs = kwargs\n\n    def generate_and_add_feature_groups_to_map(\n            self,\n            kpi_collection: KPICollection,\n            folium_map: folium.Map,\n            show: SHOW_OPTIONS = 'none',\n            overlay: bool = False,\n    ) -&gt; list[folium.FeatureGroup]:\n        fgs = self.get_feature_groups(kpi_collection, show=show, overlay=overlay)\n        for fg in fgs:\n            folium_map.add_child(fg)\n        return fgs\n\n    def get_feature_groups(\n            self,\n            kpi_collection: KPICollection,\n            show: SHOW_OPTIONS = 'none',\n            overlay: bool = False\n    ) -&gt; list[folium.FeatureGroup]:\n        \"\"\"\n        Create feature groups for KPI collection with organized grouping.\n\n        Main method that processes KPI collection through grouping, creates\n        folium FeatureGroups, and applies all configured generators to create\n        a complete map visualization.\n\n        Args:\n            kpi_collection: Collection of KPIs to visualize\n            show: Which feature groups to show initially ('first', 'last', 'none')\n            overlay: Whether feature groups should be overlay controls\n\n        Returns:\n            List of folium FeatureGroup objects ready to add to map\n        \"\"\"\n        \"\"\"Create feature groups for KPI collection, replicating original functionality.\"\"\"\n        from tqdm import tqdm\n        from mescal.utils.logging import get_logger\n\n        logger = get_logger(__name__)\n        feature_groups = []\n\n        pbar = tqdm(kpi_collection, total=kpi_collection.size, desc=f'{self.__class__.__name__}')\n        with pbar:\n            kpi_groups = self.grouping_manager.get_kpi_groups(kpi_collection)\n            for kpi_group in kpi_groups:\n                group_name = self.grouping_manager.get_feature_group_name(kpi_group)\n\n                if show == 'first':\n                    show_fg = kpi_group == kpi_groups[0]\n                elif show == 'last':\n                    show_fg = kpi_group == kpi_groups[-1]\n                else:\n                    show_fg = False\n\n                fg = folium.FeatureGroup(name=group_name, overlay=overlay, show=show_fg)\n\n                for kpi in kpi_group:\n                    data_item = KPIDataItem(kpi, kpi_collection, study_manager=self.study_manager, **self.kwargs)\n                    for generator in self.generators:\n                        if self.include_related_kpis_in_tooltip:\n                            _tmp = generator.feature_resolver.property_mappers.get('tooltip', None)\n                            generator.feature_resolver.property_mappers['tooltip'] = self._create_enhanced_tooltip_generator()\n                        try:\n                            generator.generate(data_item, fg)\n                        except Exception as e:\n                            logger.warning(\n                                f'Exception while trying to add KPI {kpi.name} to FeatureGroup {group_name}: {e}'\n                            )\n                        finally:\n                            if self.include_related_kpis_in_tooltip:\n                                if _tmp is not None:\n                                    generator.feature_resolver.property_mappers['tooltip'] = _tmp\n                                else:\n                                    generator.feature_resolver.property_mappers.pop('tooltip')\n                    pbar.update(1)\n\n                feature_groups.append(fg)\n\n        return feature_groups\n\n    def _create_enhanced_tooltip_generator(self) -&gt; PropertyMapper:\n        \"\"\"Create tooltip generator that includes related KPIs.\"\"\"\n\n        def generate_tooltip(data_item: KPIDataItem) -&gt; str:\n\n            kpi = data_item.kpi\n            kpi_name = kpi.get_kpi_name_with_dataset_name()\n\n            from mescal.units import Units\n            kpi_quantity = Units.get_quantity_in_pretty_unit(kpi.quantity)\n            kpi_text = Units.get_pretty_text_for_quantity(kpi_quantity, thousands_separator=' ')\n\n            html = '&lt;table style=\"border-collapse: collapse;\"&gt;\\n'\n            html += f'  &lt;tr&gt;&lt;td style=\"padding: 4px 8px;\"&gt;&lt;strong&gt;{kpi_name}&lt;/strong&gt;&lt;/td&gt;' \\\n                    f'&lt;td style=\"text-align: right; padding: 4px 8px;\"&gt;{kpi_text}&lt;/td&gt;&lt;/tr&gt;\\n'\n\n            if self.include_related_kpis_in_tooltip and self.study_manager:\n                related_groups = self.grouping_manager.get_related_kpi_groups(\n                    kpi, self.study_manager\n                )\n\n                if any(not g.empty for g in related_groups.values()):\n                    for name, group in related_groups.items():\n                        if group.empty:\n                            continue\n                        html += \"&lt;tr&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;/tr&gt;\"\n                        html += f'  &lt;tr&gt;&lt;th colspan=\"2\" style=\"text-align: left; padding: 8px;\"&gt;{name}&lt;/th&gt;&lt;/tr&gt;\\n'\n                        for related_kpi in group:\n                            related_kpi_name = related_kpi.get_kpi_name_with_dataset_name()\n                            related_kpi_quantity = Units.get_quantity_in_pretty_unit(related_kpi.quantity)\n                            related_kpi_value_text = Units.get_pretty_text_for_quantity(\n                                related_kpi_quantity,\n                                thousands_separator=' ',\n                            )\n                            html += f'  &lt;tr&gt;&lt;td style=\"padding: 4px 8px;\"&gt;{related_kpi_name}&lt;/td&gt;' \\\n                                    f'&lt;td style=\"text-align: right; padding: 4px 8px;\"&gt;{related_kpi_value_text}&lt;/td&gt;&lt;/tr&gt;\\n'\n\n            html += '&lt;br&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;/table&gt;'\n            return html\n\n        return PropertyMapper(generate_tooltip)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mescal.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPICollectionMapVisualizer.get_feature_groups","title":"get_feature_groups","text":"<pre><code>get_feature_groups(kpi_collection: KPICollection, show: SHOW_OPTIONS = 'none', overlay: bool = False) -&gt; list[FeatureGroup]\n</code></pre> <p>Create feature groups for KPI collection with organized grouping.</p> <p>Main method that processes KPI collection through grouping, creates folium FeatureGroups, and applies all configured generators to create a complete map visualization.</p> <p>Parameters:</p> Name Type Description Default <code>kpi_collection</code> <code>KPICollection</code> <p>Collection of KPIs to visualize</p> required <code>show</code> <code>SHOW_OPTIONS</code> <p>Which feature groups to show initially ('first', 'last', 'none')</p> <code>'none'</code> <code>overlay</code> <code>bool</code> <p>Whether feature groups should be overlay controls</p> <code>False</code> <p>Returns:</p> Type Description <code>list[FeatureGroup]</code> <p>List of folium FeatureGroup objects ready to add to map</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>def get_feature_groups(\n        self,\n        kpi_collection: KPICollection,\n        show: SHOW_OPTIONS = 'none',\n        overlay: bool = False\n) -&gt; list[folium.FeatureGroup]:\n    \"\"\"\n    Create feature groups for KPI collection with organized grouping.\n\n    Main method that processes KPI collection through grouping, creates\n    folium FeatureGroups, and applies all configured generators to create\n    a complete map visualization.\n\n    Args:\n        kpi_collection: Collection of KPIs to visualize\n        show: Which feature groups to show initially ('first', 'last', 'none')\n        overlay: Whether feature groups should be overlay controls\n\n    Returns:\n        List of folium FeatureGroup objects ready to add to map\n    \"\"\"\n    \"\"\"Create feature groups for KPI collection, replicating original functionality.\"\"\"\n    from tqdm import tqdm\n    from mescal.utils.logging import get_logger\n\n    logger = get_logger(__name__)\n    feature_groups = []\n\n    pbar = tqdm(kpi_collection, total=kpi_collection.size, desc=f'{self.__class__.__name__}')\n    with pbar:\n        kpi_groups = self.grouping_manager.get_kpi_groups(kpi_collection)\n        for kpi_group in kpi_groups:\n            group_name = self.grouping_manager.get_feature_group_name(kpi_group)\n\n            if show == 'first':\n                show_fg = kpi_group == kpi_groups[0]\n            elif show == 'last':\n                show_fg = kpi_group == kpi_groups[-1]\n            else:\n                show_fg = False\n\n            fg = folium.FeatureGroup(name=group_name, overlay=overlay, show=show_fg)\n\n            for kpi in kpi_group:\n                data_item = KPIDataItem(kpi, kpi_collection, study_manager=self.study_manager, **self.kwargs)\n                for generator in self.generators:\n                    if self.include_related_kpis_in_tooltip:\n                        _tmp = generator.feature_resolver.property_mappers.get('tooltip', None)\n                        generator.feature_resolver.property_mappers['tooltip'] = self._create_enhanced_tooltip_generator()\n                    try:\n                        generator.generate(data_item, fg)\n                    except Exception as e:\n                        logger.warning(\n                            f'Exception while trying to add KPI {kpi.name} to FeatureGroup {group_name}: {e}'\n                        )\n                    finally:\n                        if self.include_related_kpis_in_tooltip:\n                            if _tmp is not None:\n                                generator.feature_resolver.property_mappers['tooltip'] = _tmp\n                            else:\n                                generator.feature_resolver.property_mappers.pop('tooltip')\n                pbar.update(1)\n\n            feature_groups.append(fg)\n\n    return feature_groups\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mescal.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPIGroupingManager","title":"KPIGroupingManager","text":"<p>Manages sophisticated KPI grouping and organization for map visualization.</p> <p>Handles the complex logic of grouping KPIs by their attributes, creating meaningful feature group names, and finding related KPIs for enhanced tooltips. Supports custom sorting orders and category hierarchies.</p> <p>The grouping system is designed to create logical visual organization of energy system KPIs, where related metrics (same flag, different datasets) are grouped together and presented with consistent naming.</p> <p>Parameters:</p> Name Type Description Default <code>kpi_attribute_category_orders</code> <code>dict[str, list[str]]</code> <p>Custom ordering for specific attribute values</p> <code>None</code> <code>kpi_attribute_keys_to_exclude_from_grouping</code> <code>list[str]</code> <p>Attributes to ignore during grouping</p> <code>None</code> <code>kpi_attribute_sort_order</code> <code>list[str]</code> <p>Order of attributes for group sorting</p> <code>None</code> <p>Examples:</p> <p>Custom grouping configuration:</p> <pre><code>&gt;&gt;&gt; manager = KPIGroupingManager(\n...     kpi_attribute_category_orders={\n...         'dataset': ['reference', 'scenario_1', 'scenario_2'],\n...         'aggregation': ['Sum', 'Mean', 'Max']\n...     },\n...     kpi_attribute_keys_to_exclude_from_grouping=['object_name']\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>class KPIGroupingManager:\n    \"\"\"\n    Manages sophisticated KPI grouping and organization for map visualization.\n\n    Handles the complex logic of grouping KPIs by their attributes, creating\n    meaningful feature group names, and finding related KPIs for enhanced\n    tooltips. Supports custom sorting orders and category hierarchies.\n\n    The grouping system is designed to create logical visual organization\n    of energy system KPIs, where related metrics (same flag, different datasets)\n    are grouped together and presented with consistent naming.\n\n    Args:\n        kpi_attribute_category_orders: Custom ordering for specific attribute values\n        kpi_attribute_keys_to_exclude_from_grouping: Attributes to ignore during grouping\n        kpi_attribute_sort_order: Order of attributes for group sorting\n\n    Examples:\n        Custom grouping configuration:\n        &gt;&gt;&gt; manager = KPIGroupingManager(\n        ...     kpi_attribute_category_orders={\n        ...         'dataset': ['reference', 'scenario_1', 'scenario_2'],\n        ...         'aggregation': ['Sum', 'Mean', 'Max']\n        ...     },\n        ...     kpi_attribute_keys_to_exclude_from_grouping=['object_name']\n        ... )\n    \"\"\"\n\n    DEFAULT_EXCLUDE_FROM_GROUPING = ['name', 'object_name', 'column_subset']\n    DEFAULT_SORT_ORDER = [\n        'name_prefix', 'model_flag', 'flag', 'model_query', 'aggregation',\n        'reference_dataset', 'variation_dataset', 'dataset',\n        'value_comparison', 'value_operation', 'name_suffix'\n    ]\n    DEFAULT_INCLUDE_ATTRIBUTES = ['value_operation', 'aggregation', 'flag', 'dataset', 'unit']\n    DEFAULT_EXCLUDE_ATTRIBUTES = ['variation_dataset', 'reference_dataset', 'model_flag', 'base_unit', 'dataset_type']\n\n    def __init__(\n            self,\n            kpi_attribute_category_orders: dict[str, list[str]] = None,\n            kpi_attribute_keys_to_exclude_from_grouping: list[str] = None,\n            kpi_attribute_sort_order: list[str] = None\n    ):\n        self.kpi_attribute_category_orders = kpi_attribute_category_orders or {}\n        self.kpi_attribute_keys_to_exclude_from_grouping = (\n                kpi_attribute_keys_to_exclude_from_grouping or self.DEFAULT_EXCLUDE_FROM_GROUPING.copy()\n        )\n        self.kpi_attribute_sort_order = (\n                kpi_attribute_sort_order or self.DEFAULT_SORT_ORDER.copy()\n        )\n\n    def get_kpi_groups(self, kpi_collection: KPICollection) -&gt; list[KPICollection]:\n        \"\"\"\n        Group KPIs by attributes with sophisticated sorting.\n\n        Creates logical groups of KPIs based on their attributes, excluding\n        specified attributes from grouping and applying custom sort orders.\n\n        Args:\n            kpi_collection: Collection of KPIs to group\n\n        Returns:\n            List of KPICollection objects, each representing a logical group\n        \"\"\"\n        from mescal.utils.dict_combinations import dict_combination_iterator\n\n        attribute_sets = kpi_collection.get_all_kpi_attributes_and_value_sets(primitive_values=True)\n        relevant_attribute_sets = {\n            k: v for k, v in attribute_sets.items()\n            if k not in self.kpi_attribute_keys_to_exclude_from_grouping\n        }\n\n        ordered_keys = [k for k in self.kpi_attribute_sort_order if k in relevant_attribute_sets]\n\n        # Build attribute value rankings\n        attribute_value_rank: dict[str, dict[str, int]] = {}\n        for attr in ordered_keys:\n            existing_values = set(relevant_attribute_sets.get(attr, []))\n            manual_order = [v for v in self.kpi_attribute_category_orders.get(attr, []) if v in existing_values]\n            remaining = list(existing_values - set(manual_order))\n            try:\n                remaining = list(sorted(remaining))\n            except TypeError:\n                pass\n            full_order = manual_order + remaining\n            attribute_value_rank[attr] = {val: idx for idx, val in enumerate(full_order)}\n\n        def sorting_index(group_kwargs: dict[str, str]) -&gt; tuple:\n            return tuple(\n                attribute_value_rank[attr].get(group_kwargs.get(attr), float(\"inf\"))\n                for attr in ordered_keys\n            )\n\n        # Create and sort groups\n        group_kwargs_list = list(dict_combination_iterator(relevant_attribute_sets))\n        group_kwargs_list.sort(key=sorting_index)\n\n        groups: list[KPICollection] = []\n        for group_kwargs in group_kwargs_list:\n            g = kpi_collection.get_filtered_kpi_collection_by_attributes(**group_kwargs)\n            if not g.empty:\n                groups.append(g)\n\n        return groups\n\n    def get_feature_group_name(self, kpi_group: KPICollection) -&gt; str:\n        \"\"\"\n        Generate meaningful feature group name from KPI group.\n\n        Creates human-readable names for map feature groups based on\n        common KPI attributes, prioritizing important attributes.\n\n        Args:\n            kpi_group: KPI group to generate name for\n\n        Returns:\n            Human-readable feature group name\n        \"\"\"\n        attributes = kpi_group.get_in_common_kpi_attributes(primitive_values=True)\n\n        for k in self.DEFAULT_EXCLUDE_ATTRIBUTES:\n            attributes.pop(k, None)\n\n        components = []\n        include_attrs = self.DEFAULT_INCLUDE_ATTRIBUTES + [\n            k for k in attributes.keys() if k not in self.DEFAULT_INCLUDE_ATTRIBUTES\n        ]\n        for k in include_attrs:\n            value = attributes.pop(k, None)\n            if value is not None:\n                components.append(str(value))\n\n        return ' '.join(components)\n\n    def get_related_kpi_groups(self, kpi: KPI, study_manager) -&gt; dict[str, KPICollection]:\n        \"\"\"\n        Get related KPIs grouped by relationship type.\n\n        Finds KPIs related to the given KPI across different dimensions\n        (comparisons, aggregations, datasets) for enhanced tooltip display.\n\n        Args:\n            kpi: Source KPI to find relatives for\n            study_manager: StudyManager for accessing related KPIs\n\n        Returns:\n            Dict mapping relationship type to KPICollection of related KPIs\n        \"\"\"\n        from mescal.kpis import ValueComparisonKPI, ArithmeticValueOperationKPI\n\n        groups = {\n            'Different Comparisons / ValueOperations': KPICollection(),\n            'Different Aggregations': KPICollection(),\n            'Different Datasets': KPICollection(),\n        }\n\n        if not study_manager:\n            return groups\n\n        kpi_atts = kpi.attributes.as_dict(primitive_values=True)\n\n        _must_contain = ['flag', 'aggregation']\n        if any(kpi_atts.get(k, None) is None for k in _must_contain):\n            return groups\n\n        try:\n            pre_filtered = study_manager.scen_comp.get_merged_kpi_collection()\n            pre_filtered = pre_filtered.get_filtered_kpi_collection_by_attributes(\n                object_name=kpi.get_attributed_object_name(),\n                flag=kpi_atts['flag'],\n                model_flag=kpi.get_attributed_model_flag(),\n            )\n        except:\n            return groups\n\n        _main_kpi_is_value_op = isinstance(kpi, (ValueComparisonKPI, ArithmeticValueOperationKPI))\n\n        for potential_relative in pre_filtered:\n            pratts = potential_relative.attributes.as_dict(primitive_values=True)\n            if pratts.get('dataset') == kpi_atts.get('dataset'):  # same ds\n                if pratts.get('aggregation', None) == kpi_atts.get('aggregation'):  # same ds, agg\n                    if pratts.get('value_operation', None) != kpi_atts.get('value_operation', None):\n                        groups['Different Comparisons / ValueOperations'].add_kpi(potential_relative)\n                        continue\n                else:  # same ds, diff agg\n                    if pratts.get('value_operation', None) is None:\n                        groups['Different Aggregations'].add_kpi(potential_relative)\n                        continue\n                    elif pratts.get('value_operation') == kpi_atts.get('value_operation', None):\n                        groups['Different Aggregations'].add_kpi(potential_relative)\n                        continue\n            elif pratts.get('aggregation', None) == kpi_atts.get('aggregation'):  # same agg, diff ds\n                if pratts.get('value_operation', None) == kpi_atts.get('value_operation', None):\n                    groups['Different Datasets'].add_kpi(potential_relative)\n                    continue\n                if not _main_kpi_is_value_op:\n                    groups['Different Comparisons / ValueOperations'].add_kpi(potential_relative)\n                    continue\n\n        return groups\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mescal.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPIGroupingManager.get_kpi_groups","title":"get_kpi_groups","text":"<pre><code>get_kpi_groups(kpi_collection: KPICollection) -&gt; list[KPICollection]\n</code></pre> <p>Group KPIs by attributes with sophisticated sorting.</p> <p>Creates logical groups of KPIs based on their attributes, excluding specified attributes from grouping and applying custom sort orders.</p> <p>Parameters:</p> Name Type Description Default <code>kpi_collection</code> <code>KPICollection</code> <p>Collection of KPIs to group</p> required <p>Returns:</p> Type Description <code>list[KPICollection]</code> <p>List of KPICollection objects, each representing a logical group</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>def get_kpi_groups(self, kpi_collection: KPICollection) -&gt; list[KPICollection]:\n    \"\"\"\n    Group KPIs by attributes with sophisticated sorting.\n\n    Creates logical groups of KPIs based on their attributes, excluding\n    specified attributes from grouping and applying custom sort orders.\n\n    Args:\n        kpi_collection: Collection of KPIs to group\n\n    Returns:\n        List of KPICollection objects, each representing a logical group\n    \"\"\"\n    from mescal.utils.dict_combinations import dict_combination_iterator\n\n    attribute_sets = kpi_collection.get_all_kpi_attributes_and_value_sets(primitive_values=True)\n    relevant_attribute_sets = {\n        k: v for k, v in attribute_sets.items()\n        if k not in self.kpi_attribute_keys_to_exclude_from_grouping\n    }\n\n    ordered_keys = [k for k in self.kpi_attribute_sort_order if k in relevant_attribute_sets]\n\n    # Build attribute value rankings\n    attribute_value_rank: dict[str, dict[str, int]] = {}\n    for attr in ordered_keys:\n        existing_values = set(relevant_attribute_sets.get(attr, []))\n        manual_order = [v for v in self.kpi_attribute_category_orders.get(attr, []) if v in existing_values]\n        remaining = list(existing_values - set(manual_order))\n        try:\n            remaining = list(sorted(remaining))\n        except TypeError:\n            pass\n        full_order = manual_order + remaining\n        attribute_value_rank[attr] = {val: idx for idx, val in enumerate(full_order)}\n\n    def sorting_index(group_kwargs: dict[str, str]) -&gt; tuple:\n        return tuple(\n            attribute_value_rank[attr].get(group_kwargs.get(attr), float(\"inf\"))\n            for attr in ordered_keys\n        )\n\n    # Create and sort groups\n    group_kwargs_list = list(dict_combination_iterator(relevant_attribute_sets))\n    group_kwargs_list.sort(key=sorting_index)\n\n    groups: list[KPICollection] = []\n    for group_kwargs in group_kwargs_list:\n        g = kpi_collection.get_filtered_kpi_collection_by_attributes(**group_kwargs)\n        if not g.empty:\n            groups.append(g)\n\n    return groups\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mescal.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPIGroupingManager.get_feature_group_name","title":"get_feature_group_name","text":"<pre><code>get_feature_group_name(kpi_group: KPICollection) -&gt; str\n</code></pre> <p>Generate meaningful feature group name from KPI group.</p> <p>Creates human-readable names for map feature groups based on common KPI attributes, prioritizing important attributes.</p> <p>Parameters:</p> Name Type Description Default <code>kpi_group</code> <code>KPICollection</code> <p>KPI group to generate name for</p> required <p>Returns:</p> Type Description <code>str</code> <p>Human-readable feature group name</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>def get_feature_group_name(self, kpi_group: KPICollection) -&gt; str:\n    \"\"\"\n    Generate meaningful feature group name from KPI group.\n\n    Creates human-readable names for map feature groups based on\n    common KPI attributes, prioritizing important attributes.\n\n    Args:\n        kpi_group: KPI group to generate name for\n\n    Returns:\n        Human-readable feature group name\n    \"\"\"\n    attributes = kpi_group.get_in_common_kpi_attributes(primitive_values=True)\n\n    for k in self.DEFAULT_EXCLUDE_ATTRIBUTES:\n        attributes.pop(k, None)\n\n    components = []\n    include_attrs = self.DEFAULT_INCLUDE_ATTRIBUTES + [\n        k for k in attributes.keys() if k not in self.DEFAULT_INCLUDE_ATTRIBUTES\n    ]\n    for k in include_attrs:\n        value = attributes.pop(k, None)\n        if value is not None:\n            components.append(str(value))\n\n    return ' '.join(components)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mescal.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPIGroupingManager.get_related_kpi_groups","title":"get_related_kpi_groups","text":"<pre><code>get_related_kpi_groups(kpi: KPI, study_manager) -&gt; dict[str, KPICollection]\n</code></pre> <p>Get related KPIs grouped by relationship type.</p> <p>Finds KPIs related to the given KPI across different dimensions (comparisons, aggregations, datasets) for enhanced tooltip display.</p> <p>Parameters:</p> Name Type Description Default <code>kpi</code> <code>KPI</code> <p>Source KPI to find relatives for</p> required <code>study_manager</code> <p>StudyManager for accessing related KPIs</p> required <p>Returns:</p> Type Description <code>dict[str, KPICollection]</code> <p>Dict mapping relationship type to KPICollection of related KPIs</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>def get_related_kpi_groups(self, kpi: KPI, study_manager) -&gt; dict[str, KPICollection]:\n    \"\"\"\n    Get related KPIs grouped by relationship type.\n\n    Finds KPIs related to the given KPI across different dimensions\n    (comparisons, aggregations, datasets) for enhanced tooltip display.\n\n    Args:\n        kpi: Source KPI to find relatives for\n        study_manager: StudyManager for accessing related KPIs\n\n    Returns:\n        Dict mapping relationship type to KPICollection of related KPIs\n    \"\"\"\n    from mescal.kpis import ValueComparisonKPI, ArithmeticValueOperationKPI\n\n    groups = {\n        'Different Comparisons / ValueOperations': KPICollection(),\n        'Different Aggregations': KPICollection(),\n        'Different Datasets': KPICollection(),\n    }\n\n    if not study_manager:\n        return groups\n\n    kpi_atts = kpi.attributes.as_dict(primitive_values=True)\n\n    _must_contain = ['flag', 'aggregation']\n    if any(kpi_atts.get(k, None) is None for k in _must_contain):\n        return groups\n\n    try:\n        pre_filtered = study_manager.scen_comp.get_merged_kpi_collection()\n        pre_filtered = pre_filtered.get_filtered_kpi_collection_by_attributes(\n            object_name=kpi.get_attributed_object_name(),\n            flag=kpi_atts['flag'],\n            model_flag=kpi.get_attributed_model_flag(),\n        )\n    except:\n        return groups\n\n    _main_kpi_is_value_op = isinstance(kpi, (ValueComparisonKPI, ArithmeticValueOperationKPI))\n\n    for potential_relative in pre_filtered:\n        pratts = potential_relative.attributes.as_dict(primitive_values=True)\n        if pratts.get('dataset') == kpi_atts.get('dataset'):  # same ds\n            if pratts.get('aggregation', None) == kpi_atts.get('aggregation'):  # same ds, agg\n                if pratts.get('value_operation', None) != kpi_atts.get('value_operation', None):\n                    groups['Different Comparisons / ValueOperations'].add_kpi(potential_relative)\n                    continue\n            else:  # same ds, diff agg\n                if pratts.get('value_operation', None) is None:\n                    groups['Different Aggregations'].add_kpi(potential_relative)\n                    continue\n                elif pratts.get('value_operation') == kpi_atts.get('value_operation', None):\n                    groups['Different Aggregations'].add_kpi(potential_relative)\n                    continue\n        elif pratts.get('aggregation', None) == kpi_atts.get('aggregation'):  # same agg, diff ds\n            if pratts.get('value_operation', None) == kpi_atts.get('value_operation', None):\n                groups['Different Datasets'].add_kpi(potential_relative)\n                continue\n            if not _main_kpi_is_value_op:\n                groups['Different Comparisons / ValueOperations'].add_kpi(potential_relative)\n                continue\n\n    return groups\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/line_text_overlay/","title":"MESCAL Folium Line-Text-Overlay Visualization System","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/line_text_overlay/#mescal.visualizations.folium_viz_system.viz_line_text_overlay","title":"viz_line_text_overlay","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/lines/","title":"MESCAL Folium Text-Overlay Visualization System","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/lines/#mescal.visualizations.folium_viz_system.viz_text_overlay","title":"viz_text_overlay","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/lines/#mescal.visualizations.folium_viz_system.viz_text_overlay.ResolvedTextOverlayFeature","title":"ResolvedTextOverlayFeature  <code>dataclass</code>","text":"<p>               Bases: <code>ResolvedFeature</code></p> <p>Resolved visual properties for text overlay elements.</p> <p>Container for all computed styling properties of text overlay visualizations, including font styling, positioning, colors, and shadow effects. Used by TextOverlayGenerator to create folium markers with styled text content.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>@dataclass\nclass ResolvedTextOverlayFeature(ResolvedFeature):\n    \"\"\"\n    Resolved visual properties for text overlay elements.\n\n    Container for all computed styling properties of text overlay visualizations,\n    including font styling, positioning, colors, and shadow effects.\n    Used by TextOverlayGenerator to create folium markers with styled text content.\n    \"\"\"\n\n    @property\n    def location(self) -&gt; Point:\n        return self.get('location')\n\n    @property\n    def offset(self) -&gt; tuple[float, float]:\n        return self.get('offset')\n\n    @property\n    def azimuth_angle(self) -&gt; float:\n        return self.get('azimuth_angle')\n\n    @property\n    def text_color(self) -&gt; str:\n        return self.get('text_color')\n\n    @property\n    def font_size(self) -&gt; str:\n        return self.get('font_size')\n\n    @property\n    def font_weight(self) -&gt; str:\n        return self.get('font_weight')\n\n    @property\n    def background_color(self) -&gt; str:\n        return self.get('background_color')\n\n    @property\n    def shadow_size(self) -&gt; float:\n        return self.get('shadow_size')\n\n    @property\n    def shadow_color(self) -&gt; str:\n        return self.get('shadow_color')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/lines/#mescal.visualizations.folium_viz_system.viz_text_overlay.TextOverlayFeatureResolver","title":"TextOverlayFeatureResolver","text":"<p>               Bases: <code>FeatureResolver[ResolvedTextOverlayFeature]</code></p> <p>Resolves visual properties for text overlay elements.</p> <p>Specialized feature resolver for text overlay visualizations that handles text content, font styling, positioning, and visual effects. Commonly used for adding data labels, value displays, or annotations to map elements.</p> <p>Parameters:</p> Name Type Description Default <code>text_color</code> <code>PropertyMapper | str</code> <p>Text color (static value or PropertyMapper)</p> <code>'#3A3A3A'</code> <code>font_size</code> <code>PropertyMapper | str</code> <p>Font size with units like '10pt' (static value or PropertyMapper)</p> <code>'10pt'</code> <code>font_weight</code> <code>PropertyMapper | str</code> <p>Font weight like 'bold', 'normal' (static value or PropertyMapper)</p> <code>'bold'</code> <code>background_color</code> <code>PropertyMapper | str</code> <p>Background color for text (static value or PropertyMapper)</p> <code>None</code> <code>shadow_size</code> <code>PropertyMapper | str</code> <p>Text shadow size like '0.5px' (static value or PropertyMapper)</p> <code>'0.5px'</code> <code>shadow_color</code> <code>PropertyMapper | str</code> <p>Text shadow color (static value or PropertyMapper)</p> <code>'#F2F2F2'</code> <code>text_print_content</code> <code>PropertyMapper | str | bool</code> <p>Text content to display (True for auto-generated)</p> <code>True</code> <code>tooltip</code> <code>PropertyMapper | str | bool</code> <p>Tooltip content (True for auto-generated, False for none)</p> <code>True</code> <code>popup</code> <code>PropertyMapper | Popup | bool</code> <p>Popup content (True/False/PropertyMapper)</p> <code>False</code> <code>location</code> <code>PropertyMapper | Point</code> <p>Point location (defaults to smart location detection)</p> <code>None</code> <code>**property_mappers</code> <code>PropertyMapper | Any</code> <p>Additional custom property mappings</p> <code>{}</code> <p>Examples:</p> <p>Basic value labels:</p> <pre><code>&gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n...     text_color='#000000',\n...     font_size='12pt',\n...     font_weight='bold'\n... )\n</code></pre> <p>Data-driven text styling:</p> <pre><code>&gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n...     text_color=PropertyMapper.from_kpi_value(\n...         lambda v: '#FF0000' if v &gt; 100 else '#000000'\n...     ),\n...     font_size=PropertyMapper.from_kpi_value(\n...         lambda v: f'{min(max(8 + v/50, 8), 20)}pt'\n...     ),\n...     text_print_content=PropertyMapper.from_kpi_value(\n...         lambda v: f'{v:.0f} MW'\n...     )\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>class TextOverlayFeatureResolver(FeatureResolver[ResolvedTextOverlayFeature]):\n    \"\"\"\n    Resolves visual properties for text overlay elements.\n\n    Specialized feature resolver for text overlay visualizations that handles text\n    content, font styling, positioning, and visual effects. Commonly used for\n    adding data labels, value displays, or annotations to map elements.\n\n    Args:\n        text_color: Text color (static value or PropertyMapper)\n        font_size: Font size with units like '10pt' (static value or PropertyMapper)\n        font_weight: Font weight like 'bold', 'normal' (static value or PropertyMapper)\n        background_color: Background color for text (static value or PropertyMapper)\n        shadow_size: Text shadow size like '0.5px' (static value or PropertyMapper)\n        shadow_color: Text shadow color (static value or PropertyMapper)\n        text_print_content: Text content to display (True for auto-generated)\n        tooltip: Tooltip content (True for auto-generated, False for none)\n        popup: Popup content (True/False/PropertyMapper)\n        location: Point location (defaults to smart location detection)\n        **property_mappers: Additional custom property mappings\n\n    Examples:\n        Basic value labels:\n        &gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n        ...     text_color='#000000',\n        ...     font_size='12pt',\n        ...     font_weight='bold'\n        ... )\n\n        Data-driven text styling:\n        &gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n        ...     text_color=PropertyMapper.from_kpi_value(\n        ...         lambda v: '#FF0000' if v &gt; 100 else '#000000'\n        ...     ),\n        ...     font_size=PropertyMapper.from_kpi_value(\n        ...         lambda v: f'{min(max(8 + v/50, 8), 20)}pt'\n        ...     ),\n        ...     text_print_content=PropertyMapper.from_kpi_value(\n        ...         lambda v: f'{v:.0f} MW'\n        ...     )\n        ... )\n    \"\"\"\n    def __init__(\n            self,\n            text_color: PropertyMapper | str = '#3A3A3A',\n            font_size: PropertyMapper | str = '10pt',\n            font_weight: PropertyMapper | str = 'bold',\n            background_color: PropertyMapper | str = None,\n            shadow_size: PropertyMapper | str = '0.5px',\n            shadow_color: PropertyMapper | str = '#F2F2F2',\n            text_print_content: PropertyMapper | str | bool = True,\n            tooltip: PropertyMapper | str | bool = True,\n            popup: PropertyMapper | folium.Popup | bool = False,\n            location: PropertyMapper | Point = None,\n            offset: PropertyMapper | tuple[float, float] = (0, 0),\n            azimuth_angle: PropertyMapper | float = 90,\n            **property_mappers: PropertyMapper | Any,\n    ):\n        mappers = dict(\n            text_color=text_color,\n            font_size=font_size,\n            font_weight=font_weight,\n            background_color=background_color,\n            shadow_size=shadow_size,\n            shadow_color=shadow_color,\n            text_print_content=text_print_content,\n            tooltip=tooltip,\n            popup=popup,\n            location=self._explicit_or_fallback(location, self._default_location_mapper()),\n            offset=offset,\n            azimuth_angle=azimuth_angle,\n            **property_mappers\n        )\n        super().__init__(feature_type=ResolvedTextOverlayFeature, **mappers)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/lines/#mescal.visualizations.folium_viz_system.viz_text_overlay.TextOverlayGenerator","title":"TextOverlayGenerator","text":"<p>               Bases: <code>FoliumObjectGenerator[TextOverlayFeatureResolver]</code></p> <p>Generates text overlays for map data items.</p> <p>Creates folium Marker objects with styled HTML text content overlaid on the map. Handles text formatting, positioning, shadow effects, and responsive styling based on data values.</p> <p>Commonly used for displaying: - KPI values directly on map elements (power flows, prices, etc.) - Data labels for areas, lines, or points - Dynamic text that changes based on underlying data - Status indicators or categorical labels</p> <p>Examples:</p> <p>Value display on bidding zones:</p> <pre><code>&gt;&gt;&gt; from mescal.units import Units\n&gt;&gt;&gt; text_gen = TextOverlayGenerator(\n...     TextOverlayFeatureResolver(\n...         text_print_content=PropertyMapper(\n...             lambda di: Units.get_pretty_text_for_quantity(\n...                 di.kpi.quantity, decimals=0, include_unit=False\n...             )\n...         ),\n...         font_size='10pt',\n...         font_weight='bold'\n...     )\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; fg = folium.FeatureGroup('Price Labels')\n&gt;&gt;&gt; text_gen.generate_objects_for_kpi_collection(price_kpis, fg)\n&gt;&gt;&gt; fg.add_to(map)\n</code></pre> <p>Combined with area visualization:</p> <pre><code>&gt;&gt;&gt; area_gen = AreaGenerator(...)\n&gt;&gt;&gt; text_gen = TextOverlayGenerator(...)\n&gt;&gt;&gt; \n&gt;&gt;&gt; area_gen.generate_objects_for_model_df(zones_df, feature_group)\n&gt;&gt;&gt; text_gen.generate_objects_for_model_df(zones_df, feature_group)\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>class TextOverlayGenerator(FoliumObjectGenerator[TextOverlayFeatureResolver]):\n    \"\"\"\n    Generates text overlays for map data items.\n\n    Creates folium Marker objects with styled HTML text content overlaid on\n    the map. Handles text formatting, positioning, shadow effects, and\n    responsive styling based on data values.\n\n    Commonly used for displaying:\n    - KPI values directly on map elements (power flows, prices, etc.)\n    - Data labels for areas, lines, or points\n    - Dynamic text that changes based on underlying data\n    - Status indicators or categorical labels\n\n    Examples:\n        Value display on bidding zones:\n        &gt;&gt;&gt; from mescal.units import Units\n        &gt;&gt;&gt; text_gen = TextOverlayGenerator(\n        ...     TextOverlayFeatureResolver(\n        ...         text_print_content=PropertyMapper(\n        ...             lambda di: Units.get_pretty_text_for_quantity(\n        ...                 di.kpi.quantity, decimals=0, include_unit=False\n        ...             )\n        ...         ),\n        ...         font_size='10pt',\n        ...         font_weight='bold'\n        ...     )\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; fg = folium.FeatureGroup('Price Labels')\n        &gt;&gt;&gt; text_gen.generate_objects_for_kpi_collection(price_kpis, fg)\n        &gt;&gt;&gt; fg.add_to(map)\n\n        Combined with area visualization:\n        &gt;&gt;&gt; area_gen = AreaGenerator(...)\n        &gt;&gt;&gt; text_gen = TextOverlayGenerator(...)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; area_gen.generate_objects_for_model_df(zones_df, feature_group)\n        &gt;&gt;&gt; text_gen.generate_objects_for_model_df(zones_df, feature_group)\n    \"\"\"\n    \"\"\"Generates text overlays for map data items.\"\"\"\n\n    def _feature_resolver_type(self) -&gt; Type[TextOverlayFeatureResolver]:\n        return TextOverlayFeatureResolver\n\n    def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n        \"\"\"\n        Generate and add a folium Marker with styled text overlay.\n\n        Args:\n            data_item: Data item containing point location and text content\n            feature_group: Folium feature group to add the text marker to\n        \"\"\"\n        style = self.feature_resolver.resolve_feature(data_item)\n        if not isinstance(style.location, Point):\n            return\n\n        if not style.text_print_content:\n            return\n\n        text_content = style.text_print_content\n        text_color = style.text_color\n        font_size = style.font_size\n        font_weight = style.font_weight\n        shadow_size = style.shadow_size\n        shadow_color = style.shadow_color\n\n        angle = float(style.azimuth_angle) - 90  # Folium counts clockwise from right-pointing direction; normal convention is CCW\n        x_offset, y_offset = style.offset\n\n        icon_html = f'''\n            &lt;div style=\"\n                position: absolute;\n                left: 50%;\n                top: 50%;\n                transform: translate(-50%, -50%) translate({x_offset}px, {-y_offset}px) rotate({angle}deg);\n                text-align: center;\n                font-size: {font_size};\n                font-weight: {font_weight};\n                color: {text_color};\n                white-space: nowrap;\n                text-shadow:\n                   -{shadow_size} -{shadow_size} 0 {shadow_color},  \n                    {shadow_size} -{shadow_size} 0 {shadow_color},\n                   -{shadow_size}  {shadow_size} 0 {shadow_color},\n                    {shadow_size}  {shadow_size} 0 {shadow_color};\n            \"&gt;\n                {text_content}\n            &lt;/div&gt;\n        '''\n\n        folium.Marker(\n            location=(style.location.y, style.location.x),\n            icon=folium.DivIcon(html=icon_html),\n            tooltip=style.tooltip,\n            popup=style.popup,\n        ).add_to(feature_group)\n\n    def _get_contrasting_color(self, surface_color: str) -&gt; str:\n        \"\"\"Get contrasting text color for a surface color.\"\"\"\n        if self._is_dark(surface_color):\n            return '#F2F2F2'\n        return '#3A3A3A'\n\n    def _get_shadow_color(self, text_color: str) -&gt; str:\n        \"\"\"Get shadow color for text.\"\"\"\n        if text_color == '#F2F2F2':\n            return '#3A3A3A'\n        return '#F2F2F2'\n\n    @staticmethod\n    def _is_dark(color: str) -&gt; bool:\n        \"\"\"Check if a color is dark.\"\"\"\n        if not color.startswith('#'):\n            return False\n        try:\n            r, g, b = [int(color[i:i + 2], 16) for i in (1, 3, 5)]\n            return (0.299 * r + 0.587 * g + 0.114 * b) &lt; 160\n        except (ValueError, IndexError):\n            return False\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/lines/#mescal.visualizations.folium_viz_system.viz_text_overlay.TextOverlayGenerator.generate","title":"generate","text":"<pre><code>generate(data_item: VisualizableDataItem, feature_group: FeatureGroup) -&gt; None\n</code></pre> <p>Generate and add a folium Marker with styled text overlay.</p> <p>Parameters:</p> Name Type Description Default <code>data_item</code> <code>VisualizableDataItem</code> <p>Data item containing point location and text content</p> required <code>feature_group</code> <code>FeatureGroup</code> <p>Folium feature group to add the text marker to</p> required Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n    \"\"\"\n    Generate and add a folium Marker with styled text overlay.\n\n    Args:\n        data_item: Data item containing point location and text content\n        feature_group: Folium feature group to add the text marker to\n    \"\"\"\n    style = self.feature_resolver.resolve_feature(data_item)\n    if not isinstance(style.location, Point):\n        return\n\n    if not style.text_print_content:\n        return\n\n    text_content = style.text_print_content\n    text_color = style.text_color\n    font_size = style.font_size\n    font_weight = style.font_weight\n    shadow_size = style.shadow_size\n    shadow_color = style.shadow_color\n\n    angle = float(style.azimuth_angle) - 90  # Folium counts clockwise from right-pointing direction; normal convention is CCW\n    x_offset, y_offset = style.offset\n\n    icon_html = f'''\n        &lt;div style=\"\n            position: absolute;\n            left: 50%;\n            top: 50%;\n            transform: translate(-50%, -50%) translate({x_offset}px, {-y_offset}px) rotate({angle}deg);\n            text-align: center;\n            font-size: {font_size};\n            font-weight: {font_weight};\n            color: {text_color};\n            white-space: nowrap;\n            text-shadow:\n               -{shadow_size} -{shadow_size} 0 {shadow_color},  \n                {shadow_size} -{shadow_size} 0 {shadow_color},\n               -{shadow_size}  {shadow_size} 0 {shadow_color},\n                {shadow_size}  {shadow_size} 0 {shadow_color};\n        \"&gt;\n            {text_content}\n        &lt;/div&gt;\n    '''\n\n    folium.Marker(\n        location=(style.location.y, style.location.x),\n        icon=folium.DivIcon(html=icon_html),\n        tooltip=style.tooltip,\n        popup=style.popup,\n    ).add_to(feature_group)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/","title":"MESCAL Folium Visualization System Base","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/#mescal.visualizations.folium_viz_system.base_viz_system","title":"base_viz_system","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/#mescal.visualizations.folium_viz_system.base_viz_system.PropertyMapper","title":"PropertyMapper","text":"<p>Maps data item attributes to visual properties for folium map visualization.</p> <p>Core abstraction for converting model data, KPI values, or static values into visual properties (colors, sizes, positions, etc.) for map elements. Used throughout the folium visualization system to create dynamic, data-driven map styling.</p> <p>The PropertyMapper encapsulates a transformation function that takes a VisualizableDataItem and returns a styled value. This enables powerful declarative map styling where visual properties are automatically computed from underlying data.</p> <p>Examples:</p> <p>Basic color mapping from KPI values:</p> <pre><code>&gt;&gt;&gt; color_mapper = PropertyMapper.from_kpi_value(lambda v: 'red' if v &gt; 0 else 'blue')\n</code></pre> <p>Size mapping from model attributes:</p> <pre><code>&gt;&gt;&gt; size_mapper = PropertyMapper.from_item_attr('capacity', lambda c: c / 100)\n</code></pre> <p>Static styling:</p> <pre><code>&gt;&gt;&gt; border_mapper = PropertyMapper.from_static_value('#000000')\n</code></pre> <p>Complex conditional styling:</p> <pre><code>&gt;&gt;&gt; def complex_color(data_item: KPIDataItem):\n...     kpi_val = data_item.kpi.value\n...     threshold = data_item.get_object_attribute('threshold')\n...     return 'green' if kpi_val &gt; threshold else 'red'\n&gt;&gt;&gt; mapper = PropertyMapper(complex_color)\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>class PropertyMapper:\n    \"\"\"\n    Maps data item attributes to visual properties for folium map visualization.\n\n    Core abstraction for converting model data, KPI values, or static values into\n    visual properties (colors, sizes, positions, etc.) for map elements. Used throughout\n    the folium visualization system to create dynamic, data-driven map styling.\n\n    The PropertyMapper encapsulates a transformation function that takes a VisualizableDataItem\n    and returns a styled value. This enables powerful declarative map styling where\n    visual properties are automatically computed from underlying data.\n\n    Examples:\n        Basic color mapping from KPI values:\n        &gt;&gt;&gt; color_mapper = PropertyMapper.from_kpi_value(lambda v: 'red' if v &gt; 0 else 'blue')\n\n        Size mapping from model attributes:\n        &gt;&gt;&gt; size_mapper = PropertyMapper.from_item_attr('capacity', lambda c: c / 100)\n\n        Static styling:\n        &gt;&gt;&gt; border_mapper = PropertyMapper.from_static_value('#000000')\n\n        Complex conditional styling:\n        &gt;&gt;&gt; def complex_color(data_item: KPIDataItem):\n        ...     kpi_val = data_item.kpi.value\n        ...     threshold = data_item.get_object_attribute('threshold')\n        ...     return 'green' if kpi_val &gt; threshold else 'red'\n        &gt;&gt;&gt; mapper = PropertyMapper(complex_color)\n    \"\"\"\n    def __init__(self, mapping: Callable[[VisualizableDataItem], Any]):\n        self.mapping = mapping\n\n    def map_data_item(self, data_item: VisualizableDataItem) -&gt; Any:\n        return self.mapping(data_item)\n\n    @classmethod\n    def from_static_value(cls, value: Any) -&gt; 'PropertyMapper':\n        \"\"\"\n        Create mapper that returns the same value for all data items.\n\n        Used for consistent styling across all map elements (e.g., all borders\n        the same color, all markers the same size).\n\n        Args:\n            value: Static value to return for all data items\n\n        Returns:\n            PropertyMapper that always returns the static value\n\n        Examples:\n            &gt;&gt;&gt; border_color = PropertyMapper.from_static_value('#FFFFFF')\n            &gt;&gt;&gt; opacity = PropertyMapper.from_static_value(0.8)\n        \"\"\"\n        return cls(lambda data_item: value)\n\n    @classmethod\n    def from_item_attr(\n            cls,\n            attribute: str,\n            mapping: Callable[[Any], Any] = None,\n    ) -&gt; 'PropertyMapper':\n        \"\"\"\n        Create mapper from model/object attribute with optional transformation.\n\n        Extracts values from model data attributes (geometry, capacity, name, etc.)\n        and optionally applies a transformation function. The attribute is resolved\n        from the underlying model DataFrame or object data.\n\n        Args:\n            attribute: Name of the attribute to extract (e.g., 'geometry', 'capacity'), must be an entry in the object pd.Series\n            mapping: Optional transformation function to apply to the attribute value\n\n        Returns:\n            PropertyMapper that extracts and optionally transforms the attribute\n\n        Examples:\n            &gt;&gt;&gt; # Direct attribute access\n            &gt;&gt;&gt; geom_mapper = PropertyMapper.from_item_attr('geometry')\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With color scale transformation\n            &gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n            &gt;&gt;&gt; color_mapper = PropertyMapper.from_item_attr('capacity', color_scale)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With custom transformation\n            &gt;&gt;&gt; size_mapper = PropertyMapper.from_item_attr('power_mw',\n            ...                                           lambda mw: min(max(mw/10, 5), 50))\n        \"\"\"\n        if mapping is None:\n            mapping = lambda x: x\n        return cls(lambda data_item: mapping(data_item.get_object_attribute(attribute)))\n\n    @classmethod\n    def from_kpi_value(cls, mapping: Callable[[Any], Any]) -&gt; 'PropertyMapper':\n        \"\"\"\n        Create mapper from KPI values with transformation function.\n\n        Specifically designed for KPIDataItem objects, extracts the computed KPI value\n        and applies a transformation. Used for styling based on energy system metrics\n        like power flows, prices, or emissions.\n\n        Args:\n            mapping: Transformation function applied to the KPI value\n\n        Returns:\n            PropertyMapper that transforms KPI values\n\n        Examples:\n            &gt;&gt;&gt; # Color mapping for power flows\n            &gt;&gt;&gt; flow_colors = PropertyMapper.from_kpi_value(\n            ...     lambda v: 'red' if v &gt; 1000 else 'green'\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Size mapping for prices\n            &gt;&gt;&gt; price_sizes = PropertyMapper.from_kpi_value(\n            ...     lambda p: min(max(p * 2, 10), 100)\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Using value mapping system\n            &gt;&gt;&gt; colorscale = SegmentedContinuousColorscale(...)\n            &gt;&gt;&gt; colors = PropertyMapper.from_kpi_value(colorscale)\n        \"\"\"\n        return cls(lambda data_item: mapping(data_item.kpi.value))\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/#mescal.visualizations.folium_viz_system.base_viz_system.PropertyMapper.from_static_value","title":"from_static_value  <code>classmethod</code>","text":"<pre><code>from_static_value(value: Any) -&gt; PropertyMapper\n</code></pre> <p>Create mapper that returns the same value for all data items.</p> <p>Used for consistent styling across all map elements (e.g., all borders the same color, all markers the same size).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Static value to return for all data items</p> required <p>Returns:</p> Type Description <code>PropertyMapper</code> <p>PropertyMapper that always returns the static value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; border_color = PropertyMapper.from_static_value('#FFFFFF')\n&gt;&gt;&gt; opacity = PropertyMapper.from_static_value(0.8)\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>@classmethod\ndef from_static_value(cls, value: Any) -&gt; 'PropertyMapper':\n    \"\"\"\n    Create mapper that returns the same value for all data items.\n\n    Used for consistent styling across all map elements (e.g., all borders\n    the same color, all markers the same size).\n\n    Args:\n        value: Static value to return for all data items\n\n    Returns:\n        PropertyMapper that always returns the static value\n\n    Examples:\n        &gt;&gt;&gt; border_color = PropertyMapper.from_static_value('#FFFFFF')\n        &gt;&gt;&gt; opacity = PropertyMapper.from_static_value(0.8)\n    \"\"\"\n    return cls(lambda data_item: value)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/#mescal.visualizations.folium_viz_system.base_viz_system.PropertyMapper.from_item_attr","title":"from_item_attr  <code>classmethod</code>","text":"<pre><code>from_item_attr(attribute: str, mapping: Callable[[Any], Any] = None) -&gt; PropertyMapper\n</code></pre> <p>Create mapper from model/object attribute with optional transformation.</p> <p>Extracts values from model data attributes (geometry, capacity, name, etc.) and optionally applies a transformation function. The attribute is resolved from the underlying model DataFrame or object data.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>Name of the attribute to extract (e.g., 'geometry', 'capacity'), must be an entry in the object pd.Series</p> required <code>mapping</code> <code>Callable[[Any], Any]</code> <p>Optional transformation function to apply to the attribute value</p> <code>None</code> <p>Returns:</p> Type Description <code>PropertyMapper</code> <p>PropertyMapper that extracts and optionally transforms the attribute</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Direct attribute access\n&gt;&gt;&gt; geom_mapper = PropertyMapper.from_item_attr('geometry')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With color scale transformation\n&gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n&gt;&gt;&gt; color_mapper = PropertyMapper.from_item_attr('capacity', color_scale)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With custom transformation\n&gt;&gt;&gt; size_mapper = PropertyMapper.from_item_attr('power_mw',\n...                                           lambda mw: min(max(mw/10, 5), 50))\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>@classmethod\ndef from_item_attr(\n        cls,\n        attribute: str,\n        mapping: Callable[[Any], Any] = None,\n) -&gt; 'PropertyMapper':\n    \"\"\"\n    Create mapper from model/object attribute with optional transformation.\n\n    Extracts values from model data attributes (geometry, capacity, name, etc.)\n    and optionally applies a transformation function. The attribute is resolved\n    from the underlying model DataFrame or object data.\n\n    Args:\n        attribute: Name of the attribute to extract (e.g., 'geometry', 'capacity'), must be an entry in the object pd.Series\n        mapping: Optional transformation function to apply to the attribute value\n\n    Returns:\n        PropertyMapper that extracts and optionally transforms the attribute\n\n    Examples:\n        &gt;&gt;&gt; # Direct attribute access\n        &gt;&gt;&gt; geom_mapper = PropertyMapper.from_item_attr('geometry')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With color scale transformation\n        &gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n        &gt;&gt;&gt; color_mapper = PropertyMapper.from_item_attr('capacity', color_scale)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With custom transformation\n        &gt;&gt;&gt; size_mapper = PropertyMapper.from_item_attr('power_mw',\n        ...                                           lambda mw: min(max(mw/10, 5), 50))\n    \"\"\"\n    if mapping is None:\n        mapping = lambda x: x\n    return cls(lambda data_item: mapping(data_item.get_object_attribute(attribute)))\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/#mescal.visualizations.folium_viz_system.base_viz_system.PropertyMapper.from_kpi_value","title":"from_kpi_value  <code>classmethod</code>","text":"<pre><code>from_kpi_value(mapping: Callable[[Any], Any]) -&gt; PropertyMapper\n</code></pre> <p>Create mapper from KPI values with transformation function.</p> <p>Specifically designed for KPIDataItem objects, extracts the computed KPI value and applies a transformation. Used for styling based on energy system metrics like power flows, prices, or emissions.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Callable[[Any], Any]</code> <p>Transformation function applied to the KPI value</p> required <p>Returns:</p> Type Description <code>PropertyMapper</code> <p>PropertyMapper that transforms KPI values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Color mapping for power flows\n&gt;&gt;&gt; flow_colors = PropertyMapper.from_kpi_value(\n...     lambda v: 'red' if v &gt; 1000 else 'green'\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Size mapping for prices\n&gt;&gt;&gt; price_sizes = PropertyMapper.from_kpi_value(\n...     lambda p: min(max(p * 2, 10), 100)\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Using value mapping system\n&gt;&gt;&gt; colorscale = SegmentedContinuousColorscale(...)\n&gt;&gt;&gt; colors = PropertyMapper.from_kpi_value(colorscale)\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>@classmethod\ndef from_kpi_value(cls, mapping: Callable[[Any], Any]) -&gt; 'PropertyMapper':\n    \"\"\"\n    Create mapper from KPI values with transformation function.\n\n    Specifically designed for KPIDataItem objects, extracts the computed KPI value\n    and applies a transformation. Used for styling based on energy system metrics\n    like power flows, prices, or emissions.\n\n    Args:\n        mapping: Transformation function applied to the KPI value\n\n    Returns:\n        PropertyMapper that transforms KPI values\n\n    Examples:\n        &gt;&gt;&gt; # Color mapping for power flows\n        &gt;&gt;&gt; flow_colors = PropertyMapper.from_kpi_value(\n        ...     lambda v: 'red' if v &gt; 1000 else 'green'\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Size mapping for prices\n        &gt;&gt;&gt; price_sizes = PropertyMapper.from_kpi_value(\n        ...     lambda p: min(max(p * 2, 10), 100)\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Using value mapping system\n        &gt;&gt;&gt; colorscale = SegmentedContinuousColorscale(...)\n        &gt;&gt;&gt; colors = PropertyMapper.from_kpi_value(colorscale)\n    \"\"\"\n    return cls(lambda data_item: mapping(data_item.kpi.value))\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/#mescal.visualizations.folium_viz_system.base_viz_system.ResolvedFeature","title":"ResolvedFeature  <code>dataclass</code>","text":"<p>Container for resolved feature properties.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>@dataclass\nclass ResolvedFeature:\n    \"\"\"Container for resolved feature properties.\"\"\"\n    properties: dict = field(default_factory=dict)\n    tooltip: str = None\n    popup: folium.Popup = None\n    text_print_content: str = None\n\n    def get(self, property: str, default=None):\n        return self.properties.get(property, default)\n\n    def __getitem__(self, key):\n        return self.properties[key]\n\n    def __setitem__(self, key, value):\n        self.properties[key] = value\n\n    def __contains__(self, key):\n        return key in self.properties\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        out = dict(self.properties)\n        for name in dir(self.__class__):\n            attr = getattr(self.__class__, name)\n            if isinstance(attr, property):\n                out[name] = getattr(self, name)\n        return out\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/#mescal.visualizations.folium_viz_system.base_viz_system.FeatureResolver","title":"FeatureResolver","text":"<p>               Bases: <code>Generic[ResolvedFeatureType]</code></p> <p>Resolves visual feature properties from data items using PropertyMappers.</p> <p>Central orchestrator for map element styling that takes a VisualizableDataItem and a collection of PropertyMappers, then produces a ResolvedFeature containing all computed visual properties. Handles default values, tooltip generation, and property normalization.</p> <p>The FeatureResolver acts as a bridge between data and visualization, converting raw data items into styled features ready for folium map rendering. It supports automatic tooltip/popup generation and flexible property mapping.</p> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>ResolvedFeatureType</code> <p>The specific resolved feature type (e.g., ResolvedAreaFeature)</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; resolver = AreaFeatureResolver(\n...     fill_color=PropertyMapper.from_kpi_value(color_scale),\n...     fill_opacity=PropertyMapper.from_static_value(0.8),\n...     tooltip=True  # Auto-generate tooltip\n... )\n&gt;&gt;&gt; resolved = resolver.resolve_feature(kpi_data_item)\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>class FeatureResolver(Generic[ResolvedFeatureType]):\n    \"\"\"\n    Resolves visual feature properties from data items using PropertyMappers.\n\n    Central orchestrator for map element styling that takes a VisualizableDataItem\n    and a collection of PropertyMappers, then produces a ResolvedFeature containing\n    all computed visual properties. Handles default values, tooltip generation,\n    and property normalization.\n\n    The FeatureResolver acts as a bridge between data and visualization, converting\n    raw data items into styled features ready for folium map rendering. It supports\n    automatic tooltip/popup generation and flexible property mapping.\n\n    Type Parameters:\n        ResolvedFeatureType: The specific resolved feature type (e.g., ResolvedAreaFeature)\n\n    Examples:\n        &gt;&gt;&gt; resolver = AreaFeatureResolver(\n        ...     fill_color=PropertyMapper.from_kpi_value(color_scale),\n        ...     fill_opacity=PropertyMapper.from_static_value(0.8),\n        ...     tooltip=True  # Auto-generate tooltip\n        ... )\n        &gt;&gt;&gt; resolved = resolver.resolve_feature(kpi_data_item)\n    \"\"\"\n    def __init__(self, feature_type: Type[ResolvedFeatureType] = None, **property_mappers: PropertyMapper | Any):\n        self.feature_type: Type[ResolvedFeatureType] = feature_type or ResolvedFeatureType.__constraints__[0]\n\n        _defaults_if_true = dict(\n            tooltip=self._default_tooltip_generator,\n            popup=self._default_popup_generator,\n            text_print_content=self._default_text_print_generator,\n        )\n        for k, mapper in _defaults_if_true.items():\n            if property_mappers.get(k, None) is True:\n                property_mappers[k] = mapper()\n            elif property_mappers.get(k, None) is False:\n                property_mappers[k] = None\n\n        self.property_mappers: dict[str, PropertyMapper] = self._normalize_property_mappers(property_mappers)\n\n    def resolve_feature(self, data_item: VisualizableDataItem) -&gt; ResolvedFeatureType:\n        resolved = self.feature_type()\n        for prop, mapper in self.property_mappers.items():\n            resolved[prop] = mapper.map_data_item(data_item)\n            if prop in ['tooltip', 'popup', 'text_print_content']:\n                setattr(resolved, prop, mapper.map_data_item(data_item))\n        return resolved\n\n    @staticmethod\n    def _normalize_property_mappers(mappers: dict[str, PropertyMapper | Any]) -&gt; dict[str, PropertyMapper]:\n        return {\n            key: mapper if isinstance(mapper, PropertyMapper) else PropertyMapper.from_static_value(mapper)\n            for key, mapper in mappers.items()\n        }\n\n    @staticmethod\n    def _explicit_or_fallback(explicit: Any, fallback: PropertyMapper = None) -&gt; PropertyMapper:\n        if explicit is not None:\n            return explicit\n        return fallback\n\n    @staticmethod\n    def _default_tooltip_generator() -&gt; PropertyMapper:\n        \"\"\"\n        Create default tooltip generator showing data item information.\n\n        Returns:\n            PropertyMapper that generates HTML table tooltips with data item attributes\n        \"\"\"\n\n        def get_tooltip(data_item: VisualizableDataItem) -&gt; str:\n            tooltip_data = data_item.get_tooltip_data()\n\n            html = '&lt;table style=\"border-collapse: collapse;\"&gt;\\n'\n            for key, value in tooltip_data.items():\n                html += f'  &lt;tr&gt;&lt;td style=\"padding: 4px 8px;\"&gt;&lt;strong&gt;{key}&lt;/strong&gt;&lt;/td&gt;' \\\n                        f'&lt;td style=\"text-align: right; padding: 4px 8px;\"&gt;{value}&lt;/td&gt;&lt;/tr&gt;\\n'\n            html += '&lt;/table&gt;'\n\n            return html\n\n        return PropertyMapper(get_tooltip)\n\n    @staticmethod\n    def _default_popup_generator() -&gt; PropertyMapper:\n        \"\"\"\n        Create default popup generator with formatted data item information.\n\n        Returns:\n            PropertyMapper that generates folium.Popup objects with data tables\n        \"\"\"\n\n        def get_popup(data_item: VisualizableDataItem) -&gt; folium.Popup:\n            tooltip_data = data_item.get_tooltip_data()\n\n            html = '&lt;table style=\"border-collapse: collapse;\"&gt;\\n'\n            for key, value in tooltip_data.items():\n                html += f'  &lt;tr&gt;&lt;td style=\"padding: 4px 8px;\"&gt;&lt;strong&gt;{key}&lt;/strong&gt;&lt;/td&gt;' \\\n                        f'&lt;td style=\"text-align: right; padding: 4px 8px;\"&gt;{value}&lt;/td&gt;&lt;/tr&gt;\\n'\n            html += '&lt;/table&gt;'\n\n            return folium.Popup(html, max_width=300)\n\n        return PropertyMapper(get_popup)\n\n    @staticmethod\n    def _default_text_print_generator() -&gt; PropertyMapper:\n        \"\"\"\n        Create default text content generator for overlay labels.\n\n        Returns:\n            PropertyMapper that returns data item text representation\n        \"\"\"\n        return PropertyMapper(lambda d: d.get_text_representation())\n\n    @staticmethod\n    def _default_geometry_mapper() -&gt; PropertyMapper:\n        \"\"\"\n        Create default geometry mapper that extracts geometric objects.\n\n        Returns:\n            PropertyMapper that extracts 'geometry' attribute from data items\n        \"\"\"\n\n        def get_geometry(data_item: VisualizableDataItem) -&gt; Polygon | None:\n            if data_item.object_has_attribute('geometry'):\n                return data_item.get_object_attribute('geometry')\n            return None\n\n        return PropertyMapper(get_geometry)\n\n    @staticmethod\n    def _default_location_mapper() -&gt; PropertyMapper:\n        \"\"\"\n        Create smart location mapper with multiple fallback strategies.\n\n        Attempts to extract Point locations from data items using various\n        attribute names and geometric calculations. Handles common location\n        attribute names and derives locations from complex geometries.\n\n        Returns:\n            PropertyMapper that intelligently extracts Point locations\n        \"\"\"\n\n        def get_location(data_item: VisualizableDataItem) -&gt; Point | None:\n            for k in ['location', 'projection_point', 'centroid', 'midpoint']:\n                if data_item.object_has_attribute(k):\n                    location = data_item.get_object_attribute(k)\n                    if isinstance(location, Point):\n                        return location\n\n            for lat, lon in [('lat', 'lon'), ('latitude', 'longitude')]:\n                if data_item.object_has_attribute(lat) and data_item.object_has_attribute(lon):\n                    lat_value = data_item.get_object_attribute(lat)\n                    lon_value = data_item.get_object_attribute(lon)\n                    if all(isinstance(v, (int, float)) for v in [lat_value, lon_value]):\n                        return Point([lon_value, lat_value])\n\n            if data_item.object_has_attribute('geometry'):\n                geometry = data_item.get_object_attribute('geometry')\n                if isinstance(geometry, Point):\n                    return geometry\n                elif isinstance(geometry, (Polygon, MultiPolygon)):\n                    return geometry.representative_point()\n                elif isinstance(geometry, (LineString, MultiLineString)):\n                    return geometry.interpolate(0.5, normalized=True)\n            return None\n\n        return PropertyMapper(get_location)\n\n    @staticmethod\n    def _default_line_string_mapper() -&gt; PropertyMapper:\n        \"\"\"\n        Create default LineString geometry mapper for line visualizations.\n\n        Returns:\n            PropertyMapper that extracts LineString geometries from data items\n        \"\"\"\n\n        def get_line_string(data_item: VisualizableDataItem) -&gt; LineString | None:\n            for k in ['geometry', 'line_string']:\n                if data_item.object_has_attribute(k):\n                    line_string = data_item.get_object_attribute(k)\n                    if isinstance(line_string, (LineString, MultiLineString)):\n                        return line_string\n            return None\n\n        return PropertyMapper(get_line_string)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/#mescal.visualizations.folium_viz_system.base_viz_system.FoliumObjectGenerator","title":"FoliumObjectGenerator","text":"<p>               Bases: <code>Generic[FeatureResolverType]</code>, <code>ABC</code></p> <p>Abstract base class for generating folium map objects from data items.</p> <p>Defines the interface for converting VisualizableDataItems into folium map elements (areas, lines, markers, etc.). Each generator type handles a specific kind of map visualization and uses a corresponding FeatureResolver to compute visual properties.</p> <p>The generator pattern enables modular, composable map building where different visualization types can be combined within the same map. Generators can process both model DataFrames and KPI collections.</p> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>FeatureResolverType</code> <p>The specific feature resolver type used by this generator</p> required <p>Examples:</p> <p>Typical usage in map building:</p> <pre><code>&gt;&gt;&gt; area_gen = AreaGenerator(AreaFeatureResolver(fill_color=...))\n&gt;&gt;&gt; line_gen = LineGenerator(LineFeatureResolver(line_color=...))\n&gt;&gt;&gt;\n&gt;&gt;&gt; fg = folium.FeatureGroup('My Data')\n&gt;&gt;&gt; area_gen.generate_objects_for_model_df(model_df, fg)\n&gt;&gt;&gt; line_gen.generate_objects_for_kpi_collection(kpi_collection, fg)\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>class FoliumObjectGenerator(Generic[FeatureResolverType], ABC):\n    \"\"\"\n    Abstract base class for generating folium map objects from data items.\n\n    Defines the interface for converting VisualizableDataItems into folium\n    map elements (areas, lines, markers, etc.). Each generator type handles\n    a specific kind of map visualization and uses a corresponding FeatureResolver\n    to compute visual properties.\n\n    The generator pattern enables modular, composable map building where different\n    visualization types can be combined within the same map. Generators can process\n    both model DataFrames and KPI collections.\n\n    Type Parameters:\n        FeatureResolverType: The specific feature resolver type used by this generator\n\n    Examples:\n        Typical usage in map building:\n        &gt;&gt;&gt; area_gen = AreaGenerator(AreaFeatureResolver(fill_color=...))\n        &gt;&gt;&gt; line_gen = LineGenerator(LineFeatureResolver(line_color=...))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; fg = folium.FeatureGroup('My Data')\n        &gt;&gt;&gt; area_gen.generate_objects_for_model_df(model_df, fg)\n        &gt;&gt;&gt; line_gen.generate_objects_for_kpi_collection(kpi_collection, fg)\n    \"\"\"\n\n    def __init__(\n            self,\n            feature_resolver: FeatureResolverType = None,\n    ):\n        self.feature_resolver: FeatureResolverType = feature_resolver or self._feature_resolver_type()()\n\n    @abstractmethod\n    def _feature_resolver_type(self) -&gt; Type[FeatureResolverType]:\n        return FeatureResolver\n\n    @abstractmethod\n    def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n        \"\"\"Generate folium object and add it to the feature group.\"\"\"\n        pass\n\n    def generate_objects_for_model_df(\n            self,\n            model_df: pd.DataFrame,\n            feature_group: folium.FeatureGroup,\n            **kwargs\n    ) -&gt; folium.FeatureGroup:\n        \"\"\"Add model DataFrame data to the map.\"\"\"\n        model_dff = add_index_as_column(model_df)\n        object_type = model_dff.index.name if isinstance(model_dff.index.name, str) else None\n        for _, row in model_dff.iterrows():\n            data_item = ModelDataItem(row, object_type=object_type, **kwargs)\n            self.generate(data_item, feature_group)\n        return feature_group\n\n    def generate_objects_for_kpi_collection(\n            self,\n            kpi_collection: 'KPICollection',\n            feature_group: folium.FeatureGroup,\n            **kwargs\n    ) -&gt; folium.FeatureGroup:\n        \"\"\"Add KPI data to the map.\"\"\"\n        for kpi in kpi_collection:\n            data_item = KPIDataItem(kpi, kpi_collection, **kwargs)\n            self.generate(data_item, feature_group)\n        return feature_group\n\n    def generate_object_for_single_kpi(\n            self,\n            kpi: 'KPI',\n            feature_group: folium.FeatureGroup,\n            kpi_collection: 'KPICollection' = None,\n            **kwargs\n    ) -&gt; folium.FeatureGroup:\n        \"\"\"Add a single KPI to the map with optional context.\"\"\"\n        data_item = KPIDataItem(kpi, kpi_collection, **kwargs)\n        self.generate(data_item, feature_group)\n        return feature_group\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/#mescal.visualizations.folium_viz_system.base_viz_system.FoliumObjectGenerator.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(data_item: VisualizableDataItem, feature_group: FeatureGroup) -&gt; None\n</code></pre> <p>Generate folium object and add it to the feature group.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>@abstractmethod\ndef generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n    \"\"\"Generate folium object and add it to the feature group.\"\"\"\n    pass\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/#mescal.visualizations.folium_viz_system.base_viz_system.FoliumObjectGenerator.generate_objects_for_model_df","title":"generate_objects_for_model_df","text":"<pre><code>generate_objects_for_model_df(model_df: DataFrame, feature_group: FeatureGroup, **kwargs) -&gt; FeatureGroup\n</code></pre> <p>Add model DataFrame data to the map.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>def generate_objects_for_model_df(\n        self,\n        model_df: pd.DataFrame,\n        feature_group: folium.FeatureGroup,\n        **kwargs\n) -&gt; folium.FeatureGroup:\n    \"\"\"Add model DataFrame data to the map.\"\"\"\n    model_dff = add_index_as_column(model_df)\n    object_type = model_dff.index.name if isinstance(model_dff.index.name, str) else None\n    for _, row in model_dff.iterrows():\n        data_item = ModelDataItem(row, object_type=object_type, **kwargs)\n        self.generate(data_item, feature_group)\n    return feature_group\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/#mescal.visualizations.folium_viz_system.base_viz_system.FoliumObjectGenerator.generate_objects_for_kpi_collection","title":"generate_objects_for_kpi_collection","text":"<pre><code>generate_objects_for_kpi_collection(kpi_collection: KPICollection, feature_group: FeatureGroup, **kwargs) -&gt; FeatureGroup\n</code></pre> <p>Add KPI data to the map.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>def generate_objects_for_kpi_collection(\n        self,\n        kpi_collection: 'KPICollection',\n        feature_group: folium.FeatureGroup,\n        **kwargs\n) -&gt; folium.FeatureGroup:\n    \"\"\"Add KPI data to the map.\"\"\"\n    for kpi in kpi_collection:\n        data_item = KPIDataItem(kpi, kpi_collection, **kwargs)\n        self.generate(data_item, feature_group)\n    return feature_group\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/system/#mescal.visualizations.folium_viz_system.base_viz_system.FoliumObjectGenerator.generate_object_for_single_kpi","title":"generate_object_for_single_kpi","text":"<pre><code>generate_object_for_single_kpi(kpi: KPI, feature_group: FeatureGroup, kpi_collection: KPICollection = None, **kwargs) -&gt; FeatureGroup\n</code></pre> <p>Add a single KPI to the map with optional context.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>def generate_object_for_single_kpi(\n        self,\n        kpi: 'KPI',\n        feature_group: folium.FeatureGroup,\n        kpi_collection: 'KPICollection' = None,\n        **kwargs\n) -&gt; folium.FeatureGroup:\n    \"\"\"Add a single KPI to the map with optional context.\"\"\"\n    data_item = KPIDataItem(kpi, kpi_collection, **kwargs)\n    self.generate(data_item, feature_group)\n    return feature_group\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/text_overlay/","title":"MESCAL Folium Text-Overlay Visualization System","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/text_overlay/#mescal.visualizations.folium_viz_system.viz_text_overlay","title":"viz_text_overlay","text":""},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/text_overlay/#mescal.visualizations.folium_viz_system.viz_text_overlay.ResolvedTextOverlayFeature","title":"ResolvedTextOverlayFeature  <code>dataclass</code>","text":"<p>               Bases: <code>ResolvedFeature</code></p> <p>Resolved visual properties for text overlay elements.</p> <p>Container for all computed styling properties of text overlay visualizations, including font styling, positioning, colors, and shadow effects. Used by TextOverlayGenerator to create folium markers with styled text content.</p> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>@dataclass\nclass ResolvedTextOverlayFeature(ResolvedFeature):\n    \"\"\"\n    Resolved visual properties for text overlay elements.\n\n    Container for all computed styling properties of text overlay visualizations,\n    including font styling, positioning, colors, and shadow effects.\n    Used by TextOverlayGenerator to create folium markers with styled text content.\n    \"\"\"\n\n    @property\n    def location(self) -&gt; Point:\n        return self.get('location')\n\n    @property\n    def offset(self) -&gt; tuple[float, float]:\n        return self.get('offset')\n\n    @property\n    def azimuth_angle(self) -&gt; float:\n        return self.get('azimuth_angle')\n\n    @property\n    def text_color(self) -&gt; str:\n        return self.get('text_color')\n\n    @property\n    def font_size(self) -&gt; str:\n        return self.get('font_size')\n\n    @property\n    def font_weight(self) -&gt; str:\n        return self.get('font_weight')\n\n    @property\n    def background_color(self) -&gt; str:\n        return self.get('background_color')\n\n    @property\n    def shadow_size(self) -&gt; float:\n        return self.get('shadow_size')\n\n    @property\n    def shadow_color(self) -&gt; str:\n        return self.get('shadow_color')\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/text_overlay/#mescal.visualizations.folium_viz_system.viz_text_overlay.TextOverlayFeatureResolver","title":"TextOverlayFeatureResolver","text":"<p>               Bases: <code>FeatureResolver[ResolvedTextOverlayFeature]</code></p> <p>Resolves visual properties for text overlay elements.</p> <p>Specialized feature resolver for text overlay visualizations that handles text content, font styling, positioning, and visual effects. Commonly used for adding data labels, value displays, or annotations to map elements.</p> <p>Parameters:</p> Name Type Description Default <code>text_color</code> <code>PropertyMapper | str</code> <p>Text color (static value or PropertyMapper)</p> <code>'#3A3A3A'</code> <code>font_size</code> <code>PropertyMapper | str</code> <p>Font size with units like '10pt' (static value or PropertyMapper)</p> <code>'10pt'</code> <code>font_weight</code> <code>PropertyMapper | str</code> <p>Font weight like 'bold', 'normal' (static value or PropertyMapper)</p> <code>'bold'</code> <code>background_color</code> <code>PropertyMapper | str</code> <p>Background color for text (static value or PropertyMapper)</p> <code>None</code> <code>shadow_size</code> <code>PropertyMapper | str</code> <p>Text shadow size like '0.5px' (static value or PropertyMapper)</p> <code>'0.5px'</code> <code>shadow_color</code> <code>PropertyMapper | str</code> <p>Text shadow color (static value or PropertyMapper)</p> <code>'#F2F2F2'</code> <code>text_print_content</code> <code>PropertyMapper | str | bool</code> <p>Text content to display (True for auto-generated)</p> <code>True</code> <code>tooltip</code> <code>PropertyMapper | str | bool</code> <p>Tooltip content (True for auto-generated, False for none)</p> <code>True</code> <code>popup</code> <code>PropertyMapper | Popup | bool</code> <p>Popup content (True/False/PropertyMapper)</p> <code>False</code> <code>location</code> <code>PropertyMapper | Point</code> <p>Point location (defaults to smart location detection)</p> <code>None</code> <code>**property_mappers</code> <code>PropertyMapper | Any</code> <p>Additional custom property mappings</p> <code>{}</code> <p>Examples:</p> <p>Basic value labels:</p> <pre><code>&gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n...     text_color='#000000',\n...     font_size='12pt',\n...     font_weight='bold'\n... )\n</code></pre> <p>Data-driven text styling:</p> <pre><code>&gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n...     text_color=PropertyMapper.from_kpi_value(\n...         lambda v: '#FF0000' if v &gt; 100 else '#000000'\n...     ),\n...     font_size=PropertyMapper.from_kpi_value(\n...         lambda v: f'{min(max(8 + v/50, 8), 20)}pt'\n...     ),\n...     text_print_content=PropertyMapper.from_kpi_value(\n...         lambda v: f'{v:.0f} MW'\n...     )\n... )\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>class TextOverlayFeatureResolver(FeatureResolver[ResolvedTextOverlayFeature]):\n    \"\"\"\n    Resolves visual properties for text overlay elements.\n\n    Specialized feature resolver for text overlay visualizations that handles text\n    content, font styling, positioning, and visual effects. Commonly used for\n    adding data labels, value displays, or annotations to map elements.\n\n    Args:\n        text_color: Text color (static value or PropertyMapper)\n        font_size: Font size with units like '10pt' (static value or PropertyMapper)\n        font_weight: Font weight like 'bold', 'normal' (static value or PropertyMapper)\n        background_color: Background color for text (static value or PropertyMapper)\n        shadow_size: Text shadow size like '0.5px' (static value or PropertyMapper)\n        shadow_color: Text shadow color (static value or PropertyMapper)\n        text_print_content: Text content to display (True for auto-generated)\n        tooltip: Tooltip content (True for auto-generated, False for none)\n        popup: Popup content (True/False/PropertyMapper)\n        location: Point location (defaults to smart location detection)\n        **property_mappers: Additional custom property mappings\n\n    Examples:\n        Basic value labels:\n        &gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n        ...     text_color='#000000',\n        ...     font_size='12pt',\n        ...     font_weight='bold'\n        ... )\n\n        Data-driven text styling:\n        &gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n        ...     text_color=PropertyMapper.from_kpi_value(\n        ...         lambda v: '#FF0000' if v &gt; 100 else '#000000'\n        ...     ),\n        ...     font_size=PropertyMapper.from_kpi_value(\n        ...         lambda v: f'{min(max(8 + v/50, 8), 20)}pt'\n        ...     ),\n        ...     text_print_content=PropertyMapper.from_kpi_value(\n        ...         lambda v: f'{v:.0f} MW'\n        ...     )\n        ... )\n    \"\"\"\n    def __init__(\n            self,\n            text_color: PropertyMapper | str = '#3A3A3A',\n            font_size: PropertyMapper | str = '10pt',\n            font_weight: PropertyMapper | str = 'bold',\n            background_color: PropertyMapper | str = None,\n            shadow_size: PropertyMapper | str = '0.5px',\n            shadow_color: PropertyMapper | str = '#F2F2F2',\n            text_print_content: PropertyMapper | str | bool = True,\n            tooltip: PropertyMapper | str | bool = True,\n            popup: PropertyMapper | folium.Popup | bool = False,\n            location: PropertyMapper | Point = None,\n            offset: PropertyMapper | tuple[float, float] = (0, 0),\n            azimuth_angle: PropertyMapper | float = 90,\n            **property_mappers: PropertyMapper | Any,\n    ):\n        mappers = dict(\n            text_color=text_color,\n            font_size=font_size,\n            font_weight=font_weight,\n            background_color=background_color,\n            shadow_size=shadow_size,\n            shadow_color=shadow_color,\n            text_print_content=text_print_content,\n            tooltip=tooltip,\n            popup=popup,\n            location=self._explicit_or_fallback(location, self._default_location_mapper()),\n            offset=offset,\n            azimuth_angle=azimuth_angle,\n            **property_mappers\n        )\n        super().__init__(feature_type=ResolvedTextOverlayFeature, **mappers)\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/text_overlay/#mescal.visualizations.folium_viz_system.viz_text_overlay.TextOverlayGenerator","title":"TextOverlayGenerator","text":"<p>               Bases: <code>FoliumObjectGenerator[TextOverlayFeatureResolver]</code></p> <p>Generates text overlays for map data items.</p> <p>Creates folium Marker objects with styled HTML text content overlaid on the map. Handles text formatting, positioning, shadow effects, and responsive styling based on data values.</p> <p>Commonly used for displaying: - KPI values directly on map elements (power flows, prices, etc.) - Data labels for areas, lines, or points - Dynamic text that changes based on underlying data - Status indicators or categorical labels</p> <p>Examples:</p> <p>Value display on bidding zones:</p> <pre><code>&gt;&gt;&gt; from mescal.units import Units\n&gt;&gt;&gt; text_gen = TextOverlayGenerator(\n...     TextOverlayFeatureResolver(\n...         text_print_content=PropertyMapper(\n...             lambda di: Units.get_pretty_text_for_quantity(\n...                 di.kpi.quantity, decimals=0, include_unit=False\n...             )\n...         ),\n...         font_size='10pt',\n...         font_weight='bold'\n...     )\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; fg = folium.FeatureGroup('Price Labels')\n&gt;&gt;&gt; text_gen.generate_objects_for_kpi_collection(price_kpis, fg)\n&gt;&gt;&gt; fg.add_to(map)\n</code></pre> <p>Combined with area visualization:</p> <pre><code>&gt;&gt;&gt; area_gen = AreaGenerator(...)\n&gt;&gt;&gt; text_gen = TextOverlayGenerator(...)\n&gt;&gt;&gt; \n&gt;&gt;&gt; area_gen.generate_objects_for_model_df(zones_df, feature_group)\n&gt;&gt;&gt; text_gen.generate_objects_for_model_df(zones_df, feature_group)\n</code></pre> Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>class TextOverlayGenerator(FoliumObjectGenerator[TextOverlayFeatureResolver]):\n    \"\"\"\n    Generates text overlays for map data items.\n\n    Creates folium Marker objects with styled HTML text content overlaid on\n    the map. Handles text formatting, positioning, shadow effects, and\n    responsive styling based on data values.\n\n    Commonly used for displaying:\n    - KPI values directly on map elements (power flows, prices, etc.)\n    - Data labels for areas, lines, or points\n    - Dynamic text that changes based on underlying data\n    - Status indicators or categorical labels\n\n    Examples:\n        Value display on bidding zones:\n        &gt;&gt;&gt; from mescal.units import Units\n        &gt;&gt;&gt; text_gen = TextOverlayGenerator(\n        ...     TextOverlayFeatureResolver(\n        ...         text_print_content=PropertyMapper(\n        ...             lambda di: Units.get_pretty_text_for_quantity(\n        ...                 di.kpi.quantity, decimals=0, include_unit=False\n        ...             )\n        ...         ),\n        ...         font_size='10pt',\n        ...         font_weight='bold'\n        ...     )\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; fg = folium.FeatureGroup('Price Labels')\n        &gt;&gt;&gt; text_gen.generate_objects_for_kpi_collection(price_kpis, fg)\n        &gt;&gt;&gt; fg.add_to(map)\n\n        Combined with area visualization:\n        &gt;&gt;&gt; area_gen = AreaGenerator(...)\n        &gt;&gt;&gt; text_gen = TextOverlayGenerator(...)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; area_gen.generate_objects_for_model_df(zones_df, feature_group)\n        &gt;&gt;&gt; text_gen.generate_objects_for_model_df(zones_df, feature_group)\n    \"\"\"\n    \"\"\"Generates text overlays for map data items.\"\"\"\n\n    def _feature_resolver_type(self) -&gt; Type[TextOverlayFeatureResolver]:\n        return TextOverlayFeatureResolver\n\n    def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n        \"\"\"\n        Generate and add a folium Marker with styled text overlay.\n\n        Args:\n            data_item: Data item containing point location and text content\n            feature_group: Folium feature group to add the text marker to\n        \"\"\"\n        style = self.feature_resolver.resolve_feature(data_item)\n        if not isinstance(style.location, Point):\n            return\n\n        if not style.text_print_content:\n            return\n\n        text_content = style.text_print_content\n        text_color = style.text_color\n        font_size = style.font_size\n        font_weight = style.font_weight\n        shadow_size = style.shadow_size\n        shadow_color = style.shadow_color\n\n        angle = float(style.azimuth_angle) - 90  # Folium counts clockwise from right-pointing direction; normal convention is CCW\n        x_offset, y_offset = style.offset\n\n        icon_html = f'''\n            &lt;div style=\"\n                position: absolute;\n                left: 50%;\n                top: 50%;\n                transform: translate(-50%, -50%) translate({x_offset}px, {-y_offset}px) rotate({angle}deg);\n                text-align: center;\n                font-size: {font_size};\n                font-weight: {font_weight};\n                color: {text_color};\n                white-space: nowrap;\n                text-shadow:\n                   -{shadow_size} -{shadow_size} 0 {shadow_color},  \n                    {shadow_size} -{shadow_size} 0 {shadow_color},\n                   -{shadow_size}  {shadow_size} 0 {shadow_color},\n                    {shadow_size}  {shadow_size} 0 {shadow_color};\n            \"&gt;\n                {text_content}\n            &lt;/div&gt;\n        '''\n\n        folium.Marker(\n            location=(style.location.y, style.location.x),\n            icon=folium.DivIcon(html=icon_html),\n            tooltip=style.tooltip,\n            popup=style.popup,\n        ).add_to(feature_group)\n\n    def _get_contrasting_color(self, surface_color: str) -&gt; str:\n        \"\"\"Get contrasting text color for a surface color.\"\"\"\n        if self._is_dark(surface_color):\n            return '#F2F2F2'\n        return '#3A3A3A'\n\n    def _get_shadow_color(self, text_color: str) -&gt; str:\n        \"\"\"Get shadow color for text.\"\"\"\n        if text_color == '#F2F2F2':\n            return '#3A3A3A'\n        return '#F2F2F2'\n\n    @staticmethod\n    def _is_dark(color: str) -&gt; bool:\n        \"\"\"Check if a color is dark.\"\"\"\n        if not color.startswith('#'):\n            return False\n        try:\n            r, g, b = [int(color[i:i + 2], 16) for i in (1, 3, 5)]\n            return (0.299 * r + 0.587 * g + 0.114 * b) &lt; 160\n        except (ValueError, IndexError):\n            return False\n</code></pre>"},{"location":"mescal-package-documentation/api_reference/visualization/folium_viz_system/text_overlay/#mescal.visualizations.folium_viz_system.viz_text_overlay.TextOverlayGenerator.generate","title":"generate","text":"<pre><code>generate(data_item: VisualizableDataItem, feature_group: FeatureGroup) -&gt; None\n</code></pre> <p>Generate and add a folium Marker with styled text overlay.</p> <p>Parameters:</p> Name Type Description Default <code>data_item</code> <code>VisualizableDataItem</code> <p>Data item containing point location and text content</p> required <code>feature_group</code> <code>FeatureGroup</code> <p>Folium feature group to add the text marker to</p> required Source code in <code>submodules/mescal/mescal/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n    \"\"\"\n    Generate and add a folium Marker with styled text overlay.\n\n    Args:\n        data_item: Data item containing point location and text content\n        feature_group: Folium feature group to add the text marker to\n    \"\"\"\n    style = self.feature_resolver.resolve_feature(data_item)\n    if not isinstance(style.location, Point):\n        return\n\n    if not style.text_print_content:\n        return\n\n    text_content = style.text_print_content\n    text_color = style.text_color\n    font_size = style.font_size\n    font_weight = style.font_weight\n    shadow_size = style.shadow_size\n    shadow_color = style.shadow_color\n\n    angle = float(style.azimuth_angle) - 90  # Folium counts clockwise from right-pointing direction; normal convention is CCW\n    x_offset, y_offset = style.offset\n\n    icon_html = f'''\n        &lt;div style=\"\n            position: absolute;\n            left: 50%;\n            top: 50%;\n            transform: translate(-50%, -50%) translate({x_offset}px, {-y_offset}px) rotate({angle}deg);\n            text-align: center;\n            font-size: {font_size};\n            font-weight: {font_weight};\n            color: {text_color};\n            white-space: nowrap;\n            text-shadow:\n               -{shadow_size} -{shadow_size} 0 {shadow_color},  \n                {shadow_size} -{shadow_size} 0 {shadow_color},\n               -{shadow_size}  {shadow_size} 0 {shadow_color},\n                {shadow_size}  {shadow_size} 0 {shadow_color};\n        \"&gt;\n            {text_content}\n        &lt;/div&gt;\n    '''\n\n    folium.Marker(\n        location=(style.location.y, style.location.x),\n        icon=folium.DivIcon(html=icon_html),\n        tooltip=style.tooltip,\n        popup=style.popup,\n    ).add_to(feature_group)\n</code></pre>"},{"location":"mescal-study-01/","title":"Study 01: Intro to MESCAL","text":"<p>The intro study primarily uses a PyPSA example network to introduce the MESCAL modules and framework. The series is structured as follows and will continuously be updated. So stay tuned.</p>"},{"location":"mescal-study-01/#getting-started-series","title":"Getting Started Series","text":"<ul> <li>mescal_101_study_manager_and_basic_fetching - Getting started with scenarios and comparisons  </li> <li>mescal_102_mastering_data_fetching - Mastering the fetch method and data access patterns  </li> <li>mescal_103_scenario_attributes - Managing and utilizing scenario metadata effectively  </li> </ul>"},{"location":"mescal-study-01/#kpi-framework-series","title":"KPI Framework Series","text":"<ul> <li>mescal_201_kpi_framework_and_units - Building a structured KPI system with proper unit handling</li> <li>mescal_202_kpi_collections_and_tables - Extracting pretty KPI tables</li> </ul>"},{"location":"mescal-study-01/#visualization-series","title":"Visualization Series","text":"<ul> <li>mescal_301_time_series_dashboard - The best way to visualize, compare and understand time-series  </li> <li>mescal_302_segmented_colormap - A useful colormap to merge multiple linear segments into a single colormap  </li> <li>mescal_303_folium_model_df_map - Creating interactive Folium maps visualizing model info  </li> <li>mescal_304_folium_area_kpi_map - Creating interactive Folium maps with area KPIs for scenarios and comparisons  </li> <li>mescal_305_area_border_kpi_map - Creating interactive Folium maps with area-border KPIs for scenarios and comparisons</li> <li>mescal_306_icon_kpi_map - Creating interactive Folium maps with KPIs projected as icons for scenarios and comparisons</li> <li>mescal_307_country_plotter_util - GeoJSON library of country shapes  </li> <li>mescal_308_html_dashboards - Multiple HTML plots (e.g. plotly figures) in one share-able html file </li> </ul>"},{"location":"mescal-study-01/#advanced-dataset-handling-series","title":"Advanced Dataset Handling Series","text":"<ul> <li>mescal_401_dataset_collections - Linking, merging and managing multiple data sources</li> <li>mescal_402_custom_interpreters - Creating study-specific data interpreters and variables</li> <li>mescal_403_model_timeseries_integration - Combining static model data with time series for richer analysis</li> <li>mescal_404_dot_notation_api - Using Python's dot notation for elegant data access</li> </ul>"},{"location":"mescal-study-01/#expert-dataset-handling-series","title":"Expert Dataset Handling Series","text":"<ul> <li>mescal_501_configuration_hierarchy - Mastering dataset and fetch-level configurations</li> <li>mescal_502_validation_framework - Ensuring data consistency across scenarios</li> <li>mescal_503_pickle_database_integration - Intro to simple .pickle DB integration for faster fetching </li> </ul>"},{"location":"mescal-study-01/#managing-multiple-studies-series","title":"Managing Multiple Studies Series","text":"<ul> <li>mescal_601_multi_study_architecture - Organizing repositories with multiple studies</li> <li>mescal_602_color_themes - Managing different color themes across different studies</li> </ul>"},{"location":"mescal-study-01/#interface-builder-series","title":"Interface Builder Series","text":"<ul> <li>mescal_701_platform_dataset - An abstract container class for data interpreters to cover all aspects of handling data from a platform</li> <li>mescal_702_flag_index_system - Understanding relationships between model and time series data</li> <li>mescal_703_model_enrichment_patterns - Advanced techniques for property and membership enrichment</li> <li>mescal_704_variable_aggregation_patterns - Advanced techniques for automatic variable aggregations</li> </ul>"},{"location":"mescal-study-01/#advanced-dataframe-handling-series","title":"Advanced DataFrame Handling Series","text":"<ul> <li>mescal_801_granularity_analysis_and_conversion - Handling time-series of different granularities</li> <li>mescal_802_multi_index_utils - xs_df, set_new_column, sort_multi_index</li> </ul>"},{"location":"mescal-study-01/#references-and-third-party-assets","title":"References and Third-Party Assets","text":"<ul> <li>PyPSA and Scigrid-DE example network:  - [MIT License]</li> <li>GeoJSON of DE control areas:  - [CC BY 4.0]</li> </ul>"},{"location":"mescal-study-01/non_versioned/_tmp/nbviewer/","title":"Nbviewer","text":"<p>If you are viewing this on github, you will not see the interactive plotly plots. Open the Notebook on nbviewer through the following link to enjoy full interactivity for all plots: https://nbviewer.org/github/helgeesch/mescal-vanilla-studies/blob/main/studies/study_01_intro_to_mescal/notebooks/000_intro_to_study_manager.ipynb</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/","title":"MESCAL 101: StudyManager and Dataset Fundamentals","text":""},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#introduction","title":"Introduction","text":"<p>This notebook demonstrates the core functionality of MESCAL's StudyManager - the central component for handling multiple scenarios and scenario comparisons in energy system modeling studies. It showcases how MESCAL's architecture simplifies working with complex multi-scenario analyses through a consistent and powerful interface.</p> <p>Rather than juggling separate data structures for each scenario, MESCAL provides a unified framework where: - Every data element is accessible through a consistent API - Scenarios and comparisons are handled through the same paradigm - Data relationships are automatically preserved and utilized</p> <p>We'll use PyPSA's Scigrid DE example dataset for this demonstration, but the same principles apply regardless of which modeling platform you use.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#setup","title":"Setup","text":"<pre><code>import os\nimport logging\nimport warnings\nimport pandas as pd\nimport pypsa\nfrom IPython.display import Image\n\nfrom mescal import StudyManager\nfrom mescal.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom mescal_pypsa import PyPSADataset\n\n# Directory setup\nos.chdir(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))))\n\n# Configuration for cleaner output\nlogging.basicConfig(level=logging.ERROR)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\npd.set_option('display.max_columns', 6)\npd.set_option('display.width', 1000)\nPlotlyTheme().apply()\n</code></pre> <pre><code># Register study-specific interpreters (details on this will be covered in a later notebook)\nfrom studies.study_01_intro_to_mescal.src.study_specific_model_interpreters import ControlAreaModelInterpreter, ScigridDEBusModelInterpreter\n\nPyPSADataset.register_interpreter(ControlAreaModelInterpreter)\nPyPSADataset.register_interpreter(ScigridDEBusModelInterpreter)\n</code></pre>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#loading-example-data","title":"Loading Example Data","text":"<p>For this demonstration, we use a PyPSA Scigrid DE example with a base network and four scenarios with increased solar and wind capacity. All networks have already been optimized.</p> <pre><code># Loading networks (all have already been optimized, so the results are included)\nstudy_folder = 'studies/study_01_intro_to_mescal'\nnetworks_folder = os.path.join(study_folder, 'data/networks_scigrid_de')\n\nn_base = pypsa.Network(os.path.join(networks_folder, 'base.nc'))\nn_solar_150 = pypsa.Network(os.path.join(networks_folder, 'solar_150.nc'))\nn_solar_200 = pypsa.Network(os.path.join(networks_folder, 'solar_200.nc'))\nn_wind_150 = pypsa.Network(os.path.join(networks_folder, 'wind_150.nc'))\nn_wind_200 = pypsa.Network(os.path.join(networks_folder, 'wind_200.nc'))\n</code></pre>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#the-studymanager","title":"The StudyManager","text":"<p>The StudyManager is the central component of MESCAL, organizing all scenarios and scenario comparisons for efficient access and analysis.</p> <pre><code>study = StudyManager.factory_from_scenarios(\n    scenarios=[\n        PyPSADataset(n_base,        name='base'),\n        PyPSADataset(n_solar_150,   name='solar_150'),\n        PyPSADataset(n_solar_200,   name='solar_200'),\n        PyPSADataset(n_wind_150,    name='wind_150'),\n        PyPSADataset(n_wind_200,    name='wind_200'),\n    ],\n    comparisons=[('solar_150', 'base'), ('solar_200', 'base'), ('wind_150', 'base'), ('wind_200', 'base')],\n    export_folder=os.path.join(study_folder, 'non_versioned/output'),\n)\n</code></pre> <p>In just a few lines of code, we've organized all scenarios and defined which comparisons we're interested in (here, comparing each scenario to the base case).</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#the-dataset-concept","title":"The Dataset Concept","text":"<p>The core building block in MESCAL is the Dataset class. The key insight is that:</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#everything-is-a-dataset","title":"Everything is a Dataset!","text":"<ul> <li>Individual scenarios are Datasets</li> <li>Collections of scenarios are Datasets</li> <li>Scenario comparisons are Datasets</li> <li>Collections of comparisons are Datasets</li> </ul> <p>This means you interact with all entities through a consistent interface, regardless of whether you're working with a single scenario or a complex collection of scenario comparisons.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#working-with-datasets","title":"Working with Datasets","text":"<p>Let's explore the fundamental operations with Datasets:</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#accessing-a-single-dataset","title":"Accessing a Single Dataset","text":"<pre><code>ds_base = study.scen.get_dataset('base')\n</code></pre>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#fetching-data","title":"Fetching Data","text":"<p>The primary method for interacting with Datasets is the <code>fetch()</code> method:</p> <pre><code>df_price_base = ds_base.fetch('buses_t.marginal_price')\nprint(df_price_base.head())\n</code></pre> <pre><code>Bus                         1        10        100  ...        98         99   99_220kV\nsnapshot                                            ...                                \n2011-01-01 00:00:00 -0.439753  5.772135  23.120287  ...  1.890824  23.723792  23.685443\n2011-01-01 01:00:00 -0.578449  6.100599  22.531275  ...  1.960914  23.186991  23.144293\n2011-01-01 02:00:00 -0.582087  6.071084  22.106222  ...  1.954744  22.747326  22.705514\n2011-01-01 03:00:00 -0.596210  6.139415  21.498113  ...  1.995093  22.117870  22.077360\n2011-01-01 04:00:00 -0.609622  6.164469  20.391625  ...  2.031251  20.979582  20.940890\n\n[5 rows x 585 columns]\n</code></pre> <p>For PyPSA users, note that this produces the same output as <code>n_base.buses_t.marginal_price</code> but provides a consistent interface across all platforms.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#discovering-available-data","title":"Discovering Available Data","text":"<p>To see what data is available in a Dataset:</p> <pre><code>accepted_flags = ds_base.accepted_flags\nlist(sorted(accepted_flags))[:15]  # Just showing the first 15\n</code></pre> <pre><code>['buses',\n 'buses_t.marginal_price',\n 'buses_t.p',\n 'buses_t.q',\n 'buses_t.v_ang',\n 'buses_t.v_mag_pu',\n 'buses_t.v_mag_pu_set',\n 'carriers',\n 'control_areas',\n 'generators',\n 'generators_t.efficiency',\n 'generators_t.marginal_cost',\n 'generators_t.marginal_cost_quadratic',\n 'generators_t.mu_lower',\n 'generators_t.mu_p_set']\n</code></pre> <p>Or to find specific types of data:</p> <pre><code>accepted_flags_for_lines = ds_base.get_accepted_flags_containing_x('lines')\naccepted_flags_for_lines\n</code></pre> <pre><code>{'lines',\n 'lines_t.mu_lower',\n 'lines_t.mu_upper',\n 'lines_t.p0',\n 'lines_t.p1',\n 'lines_t.q0',\n 'lines_t.q1',\n 'lines_t.s_max_pu'}\n</code></pre>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#from-simple-to-powerful-scenario-collections","title":"From Simple to Powerful: Scenario Collections","text":"<p>While the individual Dataset interface is useful, MESCAL's true power emerges when working with multiple scenarios.</p> <p>Let's fetch the marginal price data for all scenarios at once:</p> <pre><code>df_price = study.scen.fetch('buses_t.marginal_price')\nprint(df_price.head())\n</code></pre> <pre><code>dataset                  base                       ...  wind_200                      \nBus                         1        10        100  ...        98         99   99_220kV\nsnapshot                                            ...                                \n2011-01-01 00:00:00 -0.439753  5.772135  23.120287  ...  0.072306  23.828566  23.794191\n2011-01-01 01:00:00 -0.578449  6.100599  22.531275  ... -0.056472  22.380868  22.329086\n2011-01-01 02:00:00 -0.582087  6.071084  22.106222  ... -0.052310  20.490690  20.442924\n2011-01-01 03:00:00 -0.596210  6.139415  21.498113  ... -0.152777  18.640298  18.596510\n2011-01-01 04:00:00 -0.609622  6.164469  20.391625  ... -0.120450  15.923631  15.889469\n\n[5 rows x 2925 columns]\n</code></pre> <p>The result is a MultiIndex DataFrame with an additional 'dataset' level containing all scenario data in a single structure.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#scenario-comparisons","title":"Scenario Comparisons","text":"<p>Similarly, we can get comparison data (deltas between scenarios):</p> <pre><code>df_price_change = study.comp.fetch('buses_t.marginal_price')\nprint(df_price_change.head())\n</code></pre> <pre><code>dataset             solar_150 vs base                      ... wind_200 vs base                    \nBus                                 1        10       100  ...               98        99  99_220kV\nsnapshot                                                   ...                                     \n2011-01-01 00:00:00          0.209435  0.022953  0.020959  ...        -1.818518  0.104774  0.108749\n2011-01-01 01:00:00          0.017964  0.064642 -0.080598  ...        -2.017386 -0.806122 -0.815207\n2011-01-01 02:00:00          0.013235  0.082471 -0.002842  ...        -2.007054 -2.256636 -2.262590\n2011-01-01 03:00:00          0.026943  0.012270 -0.156701  ...        -2.147870 -3.477572 -3.480850\n2011-01-01 04:00:00          0.000889 -0.124771 -0.887855  ...        -2.151701 -5.055950 -5.051421\n\n[5 rows x 2340 columns]\n</code></pre> <p>Each column in this DataFrame represents the difference between a variation scenario and the base scenario.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#visualization-example","title":"Visualization Example","text":"<p>Now let's see this in action with a visualization. We'll create a unified analysis of average generation by carrier, control area, and scenario:</p> <pre><code>import plotly.express as px\nfrom mescal.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query\n\ngenerators_model_df = study.scen.get_dataset('base').fetch('generators')\ndata = study.scen_comp.fetch('generators_t.p')\ndata = prepend_model_prop_levels(data, generators_model_df, 'bus_control_area', 'carrier')\ndata = data.mean().groupby(level=['dataset', 'bus_control_area', 'carrier']).sum()\ndata = data / 1e3  # MW to GW\ndata_flat = data.to_frame('value').reset_index()\nfig = px.bar(\n    data_frame=data_flat,\n    y='value',\n    x='dataset',\n    facet_col='bus_control_area',\n    color='carrier',\n    category_orders={'bus_control_area': ['Amprion', 'TransnetBW', 'TenneTDE', '50Hertz']},\n    labels={'value': 'Average generation [GW]'},\n)\nfig.update_layout(title='&lt;b&gt;Average generation per carrier and scenario (change per comparison)&lt;/b&gt;', width=1200)\nfig.update_xaxes(title=None)\n\n# Save and display the image\nimage_path = study.export_path('generation_barchart.png')\nfig.update_layout(height=600, width=1200)\nfig.write_image(image_path)\nImage(image_path)\n</code></pre> <p></p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Unified Interface: Whether working with individual scenarios or complex collections, the same methods apply</li> <li>Efficient Analysis: Analyze multiple scenarios with the same code you'd use for one</li> <li>Automatic Comparison: Calculate scenario deltas without manual calculations</li> <li>Hierarchical Organization: Study \u2192 Scenarios \u2192 Individual Datasets provides a logical structure</li> <li>Consistency Across Platforms: The same code works regardless of your modeling platform</li> </ul> <p>In the next notebook, we'll explore more advanced data fetching and transformation techniques that build on these fundamentals.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/","title":"MESCAL 102: Advanced Data Fetching Techniques","text":""},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#introduction","title":"Introduction","text":"<p>Building on the StudyManager fundamentals, this notebook demonstrates MESCAL's advanced data fetching and transformation capabilities. These techniques significantly streamline multi-scenario analysis by leveraging pandas' power while abstracting away common boilerplate code.</p> <p>MESCAL's data handling utilities provide efficient ways to: - Work with multi-index DataFrames from scenario and comparison collections - Filter data using model properties - Aggregate data across complex dimensions - Combine scenarios and comparisons in unified analyses</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#setup","title":"Setup","text":"<pre><code>import os\nimport logging\nimport warnings\nimport pandas as pd\nimport pypsa\nfrom IPython.display import Image\nimport plotly.express as px\n\nfrom mescal import StudyManager\nfrom mescal.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query\nfrom mescal.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom mescal_pypsa import PyPSADataset\n\n# Directory setup\nos.chdir(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))))\n\n# Configuration for cleaner output\nlogging.basicConfig(level=logging.ERROR)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\npd.set_option('display.max_columns', 6)\npd.set_option('display.width', 1000)\nPlotlyTheme().apply()\n</code></pre> <pre><code># Same study setup as in 101, only this time it's from a method so we can re-use it\nfrom studies.study_01_intro_to_mescal.scripts.setup_study_manager import get_scigrid_de_study_manager\n\nstudy = get_scigrid_de_study_manager()\n</code></pre>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#working-with-multi-index-dataframes","title":"Working with Multi-Index DataFrames","text":"<p>Fetching data across multiple scenarios naturally creates multi-index DataFrames. Let's examine and transform these structures:</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#calculating-averages-per-scenario","title":"Calculating Averages Per Scenario","text":"<pre><code>df_price_change = study.comp.fetch('buses_t.marginal_price')\ndf_price_change_mean = df_price_change.mean().unstack('dataset')\nprint(df_price_change_mean)\n</code></pre> <pre><code>dataset    solar_150 vs base  solar_200 vs base  wind_150 vs base  wind_200 vs base\nBus                                                                                \n1                   0.210744           0.230623         -6.644124         -7.292750\n10                  0.250483           0.214094         -6.220950         -8.214383\n100                -0.712512          -1.279986         -1.500320         -3.403255\n100_220kV          -0.717737          -1.288928         -1.496799         -3.412561\n101                 0.144366           0.100898         -6.115556         -7.510163\n...                      ...                ...               ...               ...\n96_220kV           -0.173702          -0.405244         -4.028766         -4.810695\n97                 -0.694532          -1.244591         -1.385540         -2.557227\n98                  0.166729           0.132677         -6.155495         -7.608517\n99                 -0.711075          -1.284553         -1.324793         -3.213848\n99_220kV           -0.711609          -1.284904         -1.338588         -3.229785\n\n[585 rows x 4 columns]\n</code></pre> <p>The result is a DataFrame showing the average price change per bus for each scenario comparison.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#combining-scenarios-and-comparisons","title":"Combining Scenarios and Comparisons","text":"<p>To analyze both raw values and deltas in one operation:</p> <pre><code>df_price_all = study.scen_comp.fetch('buses_t.marginal_price')\nprint(df_price_all.head())\n</code></pre> <pre><code>type                 scenario                       ...       comparison                    \ndataset                  base                       ... wind_200 vs base                    \nBus                         1        10        100  ...               98        99  99_220kV\nsnapshot                                            ...                                     \n2011-01-01 00:00:00 -0.439753  5.772135  23.120287  ...        -1.818518  0.104774  0.108749\n2011-01-01 01:00:00 -0.578449  6.100599  22.531275  ...        -2.017386 -0.806122 -0.815207\n2011-01-01 02:00:00 -0.582087  6.071084  22.106222  ...        -2.007054 -2.256636 -2.262590\n2011-01-01 03:00:00 -0.596210  6.139415  21.498113  ...        -2.147870 -3.477572 -3.480850\n2011-01-01 04:00:00 -0.609622  6.164469  20.391625  ...        -2.151701 -5.055950 -5.051421\n\n[5 rows x 5265 columns]\n</code></pre> <p>This DataFrame contains both the scenario data and comparison data, distinguished by the 'type' level in the MultiIndex.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#unified-analysis","title":"Unified Analysis","text":"<p>We can perform operations on this unified structure:</p> <pre><code>df_price_all_mean = df_price_all.mean().droplevel('type').unstack('dataset').sort_index(axis=1)\nprint(df_price_all_mean)\n</code></pre> <pre><code>dataset         base  solar_150  solar_150 vs base  ...  wind_150 vs base   wind_200  wind_200 vs base\nBus                                                 ...                                               \n1           7.676257   7.887001           0.210744  ...         -6.644124   0.383507         -7.292750\n10         10.994609  11.245092           0.250483  ...         -6.220950   2.780226         -8.214383\n100        21.880995  21.168483          -0.712512  ...         -1.500320  18.477740         -3.403255\n100_220kV  21.977638  21.259901          -0.717737  ...         -1.496799  18.565077         -3.412561\n101         8.245265   8.389631           0.144366  ...         -6.115556   0.735102         -7.510163\n...              ...        ...                ...  ...               ...        ...               ...\n96_220kV   13.212272  13.038570          -0.173702  ...         -4.028766   8.401577         -4.810695\n97         21.288071  20.593539          -0.694532  ...         -1.385540  18.730844         -2.557227\n98          8.576509   8.743238           0.166729  ...         -6.155495   0.967991         -7.608517\n99         22.222045  21.510970          -0.711075  ...         -1.324793  19.008197         -3.213848\n99_220kV   22.202444  21.490835          -0.711609  ...         -1.338588  18.972659         -3.229785\n\n[585 rows x 9 columns]\n</code></pre> <p>This creates a table with average prices and deltas side by side, sorted alphabetically by dataset name.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#integrating-model-data-with-time-series","title":"Integrating Model Data with Time Series","text":"<p>A powerful MESCAL capability is the integration of static model data with time series data.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#accessing-model-data","title":"Accessing Model Data","text":"<pre><code>buses_model_df = study.scen.get_dataset('base').fetch('buses')\nprint(buses_model_df)\n</code></pre> <pre><code>           v_nom type          x  ...  ref                   location control_area\nBus                               ...                                             \n1          220.0        9.522576  ...        POINT (9.52258 52.36041)     TenneTDE\n2          380.0        9.113210  ...        POINT (9.11321 52.54385)     TenneTDE\n3          380.0        9.389745  ...        POINT (9.38975 52.02631)     TenneTDE\n4          380.0        9.125266  ...        POINT (9.12527 52.53826)     TenneTDE\n5          380.0       10.366275  ...       POINT (10.36627 52.28465)     TenneTDE\n...          ...  ...        ...  ...  ...                        ...          ...\n404_220kV  220.0        8.232094  ...        POINT (8.23209 47.55614)   TransnetBW\n413_220kV  220.0        8.673717  ...        POINT (8.67372 49.29044)   TransnetBW\n421_220kV  220.0        9.091835  ...        POINT (9.09184 49.29462)   TransnetBW\n450_220kV  220.0        7.416708  ...        POINT (7.41671 51.45705)      Amprion\n458_220kV  220.0        7.419464  ...        POINT (7.41946 51.45751)      Amprion\n\n[585 rows x 21 columns]\n</code></pre>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#filtering-by-model-properties","title":"Filtering by Model Properties","text":"<p>Let's filter our time series to include only high voltage buses (v_nom &gt;= 380 kV):</p> <pre><code>filtered_price_df = filter_by_model_query(df_price_all, buses_model_df, 'v_nom &gt;= 380')\nprint(filtered_price_df.head())\n</code></pre> <pre><code>type                 scenario                       ...       comparison                    \ndataset                  base                       ... wind_200 vs base                    \nBus                        10        100       101  ...               97        98        99\nsnapshot                                            ...                                     \n2011-01-01 00:00:00  5.772135  23.120287  1.358870  ...        -1.095655 -1.818518  0.104774\n2011-01-01 01:00:00  6.100599  22.531275  1.347486  ...         0.510926 -2.017386 -0.806122\n2011-01-01 02:00:00  6.071084  22.106222  1.344766  ...        -1.208963 -2.007054 -2.256636\n2011-01-01 03:00:00  6.139415  21.498113  1.376138  ...        -2.351651 -2.147870 -3.477572\n2011-01-01 04:00:00  6.164469  20.391625  1.405119  ...        -5.122186 -2.151701 -5.055950\n\n[5 rows x 2592 columns]\n</code></pre> <p>The <code>filter_by_model_query</code> utility applies pandas query syntax to filter time series based on model properties.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#prepending-model-properties","title":"Prepending Model Properties","text":"<p>We can add model properties as additional index levels:</p> <pre><code>price_with_control_area = prepend_model_prop_levels(filtered_price_df, buses_model_df, 'control_area')\nprint(price_with_control_area.head())\n</code></pre> <pre><code>control_area          50Hertz    Amprion   50Hertz  ...         TenneTDE          50Hertz          Amprion\ntype                 scenario   scenario  scenario  ...       comparison       comparison       comparison\ndataset                  base       base      base  ... wind_200 vs base wind_200 vs base wind_200 vs base\nBus                        10        100       101  ...               97               98               99\nsnapshot                                            ...                                                   \n2011-01-01 00:00:00  5.772135  23.120287  1.358870  ...        -1.095655        -1.818518         0.104774\n2011-01-01 01:00:00  6.100599  22.531275  1.347486  ...         0.510926        -2.017386        -0.806122\n2011-01-01 02:00:00  6.071084  22.106222  1.344766  ...        -1.208963        -2.007054        -2.256636\n2011-01-01 03:00:00  6.139415  21.498113  1.376138  ...        -2.351651        -2.147870        -3.477572\n2011-01-01 04:00:00  6.164469  20.391625  1.405119  ...        -5.122186        -2.151701        -5.055950\n\n[5 rows x 2592 columns]\n</code></pre> <p>This operation adds the 'control_area' property from the bus model as a new level in our multi-index DataFrame.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#aggregating-by-model-properties","title":"Aggregating by Model Properties","text":"<p>Now we can efficiently aggregate by control area:</p> <pre><code>price_by_control_area = price_with_control_area.mean().groupby(level=['control_area', 'dataset']).mean().unstack('control_area')\nprint(price_by_control_area)\n</code></pre> <pre><code>control_area         50Hertz    Amprion   TenneTDE  TransnetBW\ndataset                                                       \nbase               11.657645  18.804042  15.284996   23.333147\nsolar_150          11.628050  18.179100  14.830030   22.741469\nsolar_150 vs base  -0.029595  -0.624942  -0.454965   -0.591678\nsolar_200          11.448025  17.720580  14.463570   22.177620\nsolar_200 vs base  -0.209620  -1.083462  -0.821425   -1.155527\nwind_150            7.087738  16.973092  12.332776   22.664264\nwind_150 vs base   -4.569907  -1.830950  -2.952220   -0.668883\nwind_200            4.765799  15.546364  10.669218   21.040701\nwind_200 vs base   -6.891846  -3.257678  -4.615778   -2.292446\n</code></pre>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#sophisticated-visualization-example","title":"Sophisticated Visualization Example","text":"<p>Let's demonstrate these techniques with a more sophisticated visualization. We'll create a boxplot showing the distribution of hourly prices by control area and scenario:</p> <pre><code>buses_model_df = study.scen.get_dataset('base').fetch('buses')\ndata = study.scen_comp.fetch('buses_t.marginal_price')\ndata = filter_by_model_query(data, buses_model_df, query='v_nom &gt;= 380')\ndata = prepend_model_prop_levels(data, buses_model_df, 'control_area')\ndata = data.T.groupby(level=['dataset', 'control_area']).mean().T\ndata_flat = flatten_df(data)\nfig = px.box(\n    data_frame=data_flat,\n    x='control_area',\n    color='dataset',\n    y='value',\n    category_orders={'control_area': ['Amprion', 'TransnetBW', 'TenneTDE', '50Hertz']},\n    labels={'value': 'Hourly marginal price [\u20ac/MWh]'},\n)\nfig.update_traces(boxmean=True)\nfig.update_layout(title='&lt;b&gt;Distribution of hourly price occurrences per scenario (price change per comparison)&lt;/b&gt;', width=1200)\nfig.update_xaxes(title=None)\n\n\n# Save and display the image (using this only so that the picture is shown on GitHub)\nimage_path = study.export_path('price_boxplot.png')\nfig.update_layout(height=600, width=1200)\nfig.write_image(image_path)\nImage(image_path)\n\n# fig.show()  # Use this instead when you are running locally and want to have full interactivity in the plot\n</code></pre> <p></p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#advanced-transformation-pipeline","title":"Advanced Transformation Pipeline","text":"<p>Let's trace through a complete data transformation pipeline that demonstrates MESCAL's efficiency:</p> <pre><code># Start with raw data\ndf_raw = study.scen_comp.fetch('buses_t.marginal_price')\n\n# Filter to high voltage buses\nbuses_model_df = study.scen.get_dataset('base').fetch('buses')\ndf_filtered = filter_by_model_query(df_raw, buses_model_df, 'v_nom &gt;= 380')\n\n# Add control area information\ndf_with_areas = prepend_model_prop_levels(df_filtered, buses_model_df, 'control_area')\n\n# Calculate hourly area prices\nhourly_area_prices = df_with_areas.T.groupby(['dataset', 'control_area']).mean().T.unstack()\n\n# Reshape for analysis\narea_price_stats = hourly_area_prices.groupby(['dataset', 'control_area']).agg(['mean', 'std', 'min', 'max'])\n\nprint(area_price_stats)\n</code></pre> <pre><code>                                     mean       std        min        max\ndataset           control_area                                           \nbase              50Hertz       11.657645  4.898477   7.536946  22.367118\n                  Amprion       18.804042  3.200822  14.203696  23.352252\n                  TenneTDE      15.284996  4.839524   8.528858  22.238178\n                  TransnetBW    23.333147  2.799360  19.340758  25.851732\nsolar_150         50Hertz       11.628050  4.882195   7.616330  22.173180\n                  Amprion       18.179100  3.853877  12.146565  23.361021\n                  TenneTDE      14.830030  5.304740   7.223666  22.119134\n                  TransnetBW    22.741469  3.428135  16.896716  25.850474\nsolar_150 vs base 50Hertz       -0.029595  0.171920  -0.479044   0.261827\n                  Amprion       -0.624942  0.918766  -2.567606   0.157824\n                  TenneTDE      -0.454965  0.770981  -2.298001   0.484883\n                  TransnetBW    -0.591678  1.017996  -4.115210   0.065954\nsolar_200         50Hertz       11.448025  4.873367   6.854107  21.823334\n                  Amprion       17.720580  4.270065  11.023602  23.250576\n                  TenneTDE      14.463570  5.558940   6.534276  22.052944\n                  TransnetBW    22.177620  4.143174  14.744938  25.850372\nsolar_200 vs base 50Hertz       -0.209620  0.247018  -0.812198   0.268009\n                  Amprion       -1.083462  1.470448  -4.480616   0.099372\n                  TenneTDE      -0.821425  1.220437  -3.984937   0.524086\n                  TransnetBW    -1.155527  1.658060  -4.964659   0.177578\nwind_150          50Hertz        7.087738  2.572691   4.442086  14.417100\n                  Amprion       16.973092  2.468944  12.684241  20.197305\n                  TenneTDE      12.332776  3.101766   7.424682  17.277102\n                  TransnetBW    22.664264  3.346420  18.083046  25.879449\nwind_150 vs base  50Hertz       -4.569907  2.687352  -8.674692  -1.054884\n                  Amprion       -1.830950  1.360582  -3.565816   0.881127\n                  TenneTDE      -2.952220  2.143834  -6.159601  -0.075203\n                  TransnetBW    -0.668883  1.014195  -4.205612   0.135676\nwind_200          50Hertz        4.765799  2.160594   2.005988   8.848858\n                  Amprion       15.546364  3.183015  10.261299  19.354840\n                  TenneTDE      10.669218  3.488676   5.135119  15.777315\n                  TransnetBW    21.040701  5.060456  12.374443  25.935143\nwind_200 vs base  50Hertz       -6.891846  2.991629 -13.518259  -2.810534\n                  Amprion       -3.257678  1.231673  -5.057282   0.263154\n                  TenneTDE      -4.615778  2.107637  -8.368801  -0.593846\n                  TransnetBW    -2.292446  2.495500  -7.521899   0.191369\n</code></pre> <p>What would normally take dozens of lines of data manipulation code is condensed into a few expressive operations that maintain the relationships between different types of data.</p>"},{"location":"mescal-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Unified Analysis: Work with scenarios and comparisons in the same analytical framework</li> <li>Model and Time-Series Data Fetching: Unified framework for fetching model and time-series data</li> <li>Useful Integration Utilities: MESCAL provides tools like <code>filter_by_model_query</code> and <code>prepend_model_prop_levels</code> that efficiently establish relationships between model properties and time series data and provide frequently needed filtering and property mapping features</li> </ul> <p>These capabilities make MESCAL particularly powerful for complex multi-scenario analyses that would otherwise require extensive custom code.</p>"},{"location":"mescal-study-01/notebooks/_mescal_000_study_template/","title":"MESCAL 000: Template Jupyter Notebook","text":"In\u00a0[4]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git\n        %cd mescal-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mescal -U\n        !pip install git+https://github.com/helgeesch/mescal-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git         %cd mescal-vanilla-studies/          !pip install git+https://github.com/helgeesch/mescal -U         !pip install git+https://github.com/helgeesch/mescal-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mescal-vanilla-studies')\n    os.chdir('/content/mescal-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mescal import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mescal-vanilla-studies')     os.chdir('/content/mescal-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mescal import StudyManager except ImportError:     raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") In\u00a0[2]: Copied! <pre>from mescal.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nPlotlyTheme().apply()\nrenderer = ConditionalRenderer()\n</pre> from mescal.utils.plotly_utils.plotly_theme import PlotlyTheme from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() PlotlyTheme().apply() renderer = ConditionalRenderer()"},{"location":"mescal-study-01/notebooks/_mescal_000_study_template/#mescal-000-template-jupyter-notebook","title":"MESCAL 000: Template Jupyter Notebook\u00b6","text":""},{"location":"mescal-study-01/notebooks/_mescal_000_study_template/#introduction","title":"Introduction\u00b6","text":""},{"location":"mescal-study-01/notebooks/_mescal_000_study_template/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mescal and all requirements are installed.</p>"},{"location":"mescal-study-01/notebooks/_mescal_000_study_template/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ul> <li>Bing</li> <li>Bong</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/","title":"MESCAL 101: StudyManager and Dataset Fundamentals","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git\n        %cd mescal-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mescal -U\n        !pip install git+https://github.com/helgeesch/mescal-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git         %cd mescal-vanilla-studies/          !pip install git+https://github.com/helgeesch/mescal -U         !pip install git+https://github.com/helgeesch/mescal-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>Running locally, let's continue.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mescal-vanilla-studies')\n    os.chdir('/content/mescal-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mescal import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mescal-vanilla-studies')     os.chdir('/content/mescal-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mescal import StudyManager except ImportError:     raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport pypsa\n\nfrom mescal import StudyManager\nfrom mescal.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom mescal_pypsa import PyPSADataset\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nPlotlyTheme().apply()\nrenderer = ConditionalRenderer()\n</pre> import os import pypsa  from mescal import StudyManager from mescal.utils.plotly_utils.plotly_theme import PlotlyTheme from mescal_pypsa import PyPSADataset from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() PlotlyTheme().apply() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Register study-specific interpreters (details on this will be covered in a later notebook)\nfrom studies.study_01_intro_to_mescal.src.study_specific_model_interpreters import ControlAreaModelInterpreter, ScigridDEBusModelInterpreter\n\nPyPSADataset.register_interpreter(ControlAreaModelInterpreter)\nPyPSADataset.register_interpreter(ScigridDEBusModelInterpreter);\n</pre> # Register study-specific interpreters (details on this will be covered in a later notebook) from studies.study_01_intro_to_mescal.src.study_specific_model_interpreters import ControlAreaModelInterpreter, ScigridDEBusModelInterpreter  PyPSADataset.register_interpreter(ControlAreaModelInterpreter) PyPSADataset.register_interpreter(ScigridDEBusModelInterpreter); In\u00a0[5]: Copied! <pre># Loading networks (all have already been optimized, so the results are included)\nstudy_folder = 'studies/study_01_intro_to_mescal'\nnetworks_folder = os.path.join(study_folder, 'data/networks_scigrid_de')\n\nn_base = pypsa.Network(os.path.join(networks_folder, 'base.nc'))\nn_solar_150 = pypsa.Network(os.path.join(networks_folder, 'solar_150.nc'))\nn_solar_200 = pypsa.Network(os.path.join(networks_folder, 'solar_200.nc'))\nn_wind_150 = pypsa.Network(os.path.join(networks_folder, 'wind_150.nc'))\nn_wind_200 = pypsa.Network(os.path.join(networks_folder, 'wind_200.nc'))\n</pre> # Loading networks (all have already been optimized, so the results are included) study_folder = 'studies/study_01_intro_to_mescal' networks_folder = os.path.join(study_folder, 'data/networks_scigrid_de')  n_base = pypsa.Network(os.path.join(networks_folder, 'base.nc')) n_solar_150 = pypsa.Network(os.path.join(networks_folder, 'solar_150.nc')) n_solar_200 = pypsa.Network(os.path.join(networks_folder, 'solar_200.nc')) n_wind_150 = pypsa.Network(os.path.join(networks_folder, 'wind_150.nc')) n_wind_200 = pypsa.Network(os.path.join(networks_folder, 'wind_200.nc')) In\u00a0[6]: Copied! <pre>study = StudyManager.factory_from_scenarios(\n    scenarios=[\n        PyPSADataset(n_base,        name='base'),\n        PyPSADataset(n_solar_150,   name='solar_150'),\n        PyPSADataset(n_solar_200,   name='solar_200'),\n        PyPSADataset(n_wind_150,    name='wind_150'),\n        PyPSADataset(n_wind_200,    name='wind_200'),\n    ],\n    comparisons=[('solar_150', 'base'), ('solar_200', 'base'), ('wind_150', 'base'), ('wind_200', 'base')],\n    export_folder=os.path.join(study_folder, 'non_versioned/output'),\n)\n</pre> study = StudyManager.factory_from_scenarios(     scenarios=[         PyPSADataset(n_base,        name='base'),         PyPSADataset(n_solar_150,   name='solar_150'),         PyPSADataset(n_solar_200,   name='solar_200'),         PyPSADataset(n_wind_150,    name='wind_150'),         PyPSADataset(n_wind_200,    name='wind_200'),     ],     comparisons=[('solar_150', 'base'), ('solar_200', 'base'), ('wind_150', 'base'), ('wind_200', 'base')],     export_folder=os.path.join(study_folder, 'non_versioned/output'), ) <p>In just a few lines of code, we've organized all scenarios and defined which comparisons we're interested in (here, comparing each scenario to the base case).</p> In\u00a0[7]: Copied! <pre>ds_base = study.scen.get_dataset('base')\n</pre> ds_base = study.scen.get_dataset('base') In\u00a0[8]: Copied! <pre>df_price_base = ds_base.fetch('buses_t.marginal_price')\nprint(df_price_base.head())\n</pre> df_price_base = ds_base.fetch('buses_t.marginal_price') print(df_price_base.head()) <pre>Bus                     1    10    100  ...    98     99  99_220kV\nsnapshot                                ...                       \n2011-01-01 00:00:00 -0.44  5.77  23.12  ...  1.89  23.72     23.69\n2011-01-01 01:00:00 -0.58  6.10  22.53  ...  1.96  23.19     23.14\n2011-01-01 02:00:00 -0.58  6.07  22.11  ...  1.95  22.75     22.71\n2011-01-01 03:00:00 -0.60  6.14  21.50  ...  2.00  22.12     22.08\n2011-01-01 04:00:00 -0.61  6.16  20.39  ...  2.03  20.98     20.94\n\n[5 rows x 585 columns]\n</pre> <p>For PyPSA users, note that this produces the same output as <code>n_base.buses_t.marginal_price</code> but provides a consistent interface across all platforms.</p> In\u00a0[9]: Copied! <pre>accepted_flags = ds_base.accepted_flags\nlist(sorted(accepted_flags))[:15]  # Just showing the first 15\n</pre> accepted_flags = ds_base.accepted_flags list(sorted(accepted_flags))[:15]  # Just showing the first 15 Out[9]: <pre>['buses',\n 'buses_t.marginal_price',\n 'buses_t.p',\n 'buses_t.q',\n 'buses_t.v_ang',\n 'buses_t.v_mag_pu',\n 'buses_t.v_mag_pu_set',\n 'carriers',\n 'control_areas',\n 'generators',\n 'generators_t.efficiency',\n 'generators_t.marginal_cost',\n 'generators_t.marginal_cost_quadratic',\n 'generators_t.mu_lower',\n 'generators_t.mu_p_set']</pre> <p>Or to find specific types of data:</p> In\u00a0[10]: Copied! <pre>accepted_flags_for_lines = ds_base.get_accepted_flags_containing_x('lines')\naccepted_flags_for_lines\n</pre> accepted_flags_for_lines = ds_base.get_accepted_flags_containing_x('lines') accepted_flags_for_lines Out[10]: <pre>{'lines',\n 'lines_t.mu_lower',\n 'lines_t.mu_upper',\n 'lines_t.p0',\n 'lines_t.p1',\n 'lines_t.q0',\n 'lines_t.q1',\n 'lines_t.s_max_pu'}</pre> In\u00a0[11]: Copied! <pre>df_price = study.scen.fetch('buses_t.marginal_price')\nprint(df_price.head())\n</pre> df_price = study.scen.fetch('buses_t.marginal_price') print(df_price.head()) <pre>dataset              base               ... wind_200                \nBus                     1    10    100  ...       98     99 99_220kV\nsnapshot                                ...                         \n2011-01-01 00:00:00 -0.44  5.77  23.12  ...     0.07  23.83    23.79\n2011-01-01 01:00:00 -0.58  6.10  22.53  ...    -0.06  22.38    22.33\n2011-01-01 02:00:00 -0.58  6.07  22.11  ...    -0.05  20.49    20.44\n2011-01-01 03:00:00 -0.60  6.14  21.50  ...    -0.15  18.64    18.60\n2011-01-01 04:00:00 -0.61  6.16  20.39  ...    -0.12  15.92    15.89\n\n[5 rows x 2925 columns]\n</pre> <p>The result is a MultiIndex DataFrame with an additional 'dataset' level containing all scenario data in a single structure.</p> In\u00a0[12]: Copied! <pre>df_price_change = study.comp.fetch('buses_t.marginal_price')\nprint(df_price_change.head())\n</pre> df_price_change = study.comp.fetch('buses_t.marginal_price') print(df_price_change.head()) <pre>dataset             solar_150 vs base                  ... wind_200 vs base               \nBus                                 1    10       100  ...               98    99 99_220kV\nsnapshot                                               ...                                \n2011-01-01 00:00:00          2.09e-01  0.02  2.10e-02  ...            -1.82  0.10     0.11\n2011-01-01 01:00:00          1.80e-02  0.06 -8.06e-02  ...            -2.02 -0.81    -0.82\n2011-01-01 02:00:00          1.32e-02  0.08 -2.84e-03  ...            -2.01 -2.26    -2.26\n2011-01-01 03:00:00          2.69e-02  0.01 -1.57e-01  ...            -2.15 -3.48    -3.48\n2011-01-01 04:00:00          8.89e-04 -0.12 -8.88e-01  ...            -2.15 -5.06    -5.05\n\n[5 rows x 2340 columns]\n</pre> <p>Each column in this DataFrame represents the difference between a variation scenario and the base scenario.</p> In\u00a0[13]: Copied! <pre>import plotly.express as px\nfrom mescal.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query\n\ngenerators_model_df = study.scen.get_dataset('base').fetch('generators')\ndata = study.scen_comp.fetch('generators_t.p')\ndata = prepend_model_prop_levels(data, generators_model_df, 'bus_control_area', 'carrier')\ndata = data.mean().groupby(level=['dataset', 'bus_control_area', 'carrier']).sum()\ndata = data / 1e3  # MW to GW\ndata_flat = data.to_frame('value').reset_index()\nfig = px.bar(\n    data_frame=data_flat,\n    y='value',\n    x='dataset',\n    facet_col='bus_control_area',\n    color='carrier',\n    category_orders={'bus_control_area': ['Amprion', 'TransnetBW', 'TenneTDE', '50Hertz']},\n    labels={'value': 'Average generation [GW]'},\n)\nfig.update_layout(title='&lt;b&gt;Average generation per carrier and scenario (change per comparison)&lt;/b&gt;', width=1200)\nfig.update_xaxes(title=None)\n\nrenderer.show_plotly(fig)\n</pre> import plotly.express as px from mescal.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query  generators_model_df = study.scen.get_dataset('base').fetch('generators') data = study.scen_comp.fetch('generators_t.p') data = prepend_model_prop_levels(data, generators_model_df, 'bus_control_area', 'carrier') data = data.mean().groupby(level=['dataset', 'bus_control_area', 'carrier']).sum() data = data / 1e3  # MW to GW data_flat = data.to_frame('value').reset_index() fig = px.bar(     data_frame=data_flat,     y='value',     x='dataset',     facet_col='bus_control_area',     color='carrier',     category_orders={'bus_control_area': ['Amprion', 'TransnetBW', 'TenneTDE', '50Hertz']},     labels={'value': 'Average generation [GW]'}, ) fig.update_layout(title='Average generation per carrier and scenario (change per comparison)', width=1200) fig.update_xaxes(title=None)  renderer.show_plotly(fig)"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#mescal-101-studymanager-and-dataset-fundamentals","title":"MESCAL 101: StudyManager and Dataset Fundamentals\u00b6","text":""},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#introduction","title":"Introduction\u00b6","text":"<p>This notebook demonstrates the core functionality of MESCAL's StudyManager - the central component for handling multiple scenarios and scenario comparisons in energy system modeling studies. It showcases how MESCAL's architecture simplifies working with complex multi-scenario analyses through a consistent and powerful interface.</p> <p>Rather than juggling separate data structures for each scenario, MESCAL provides a unified framework where:</p> <ul> <li>Every data element is accessible through a consistent API</li> <li>Scenarios and comparisons are handled through the same paradigm</li> <li>Data relationships are automatically preserved and utilized</li> </ul> <p>We'll use PyPSA's Scigrid DE example dataset for this demonstration, but the same principles apply regardless of which modeling platform you use.</p>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mescal and all requirements are installed.</p>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#loading-example-data","title":"Loading Example Data\u00b6","text":"<p>For this demonstration, we use a PyPSA Scigrid DE example with a base network and four scenarios with increased solar and wind capacity. All networks have already been optimized.</p>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#the-studymanager","title":"The StudyManager\u00b6","text":"<p>The StudyManager is the central component of MESCAL, organizing all scenarios and scenario comparisons for efficient access and analysis.</p>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#the-dataset-concept","title":"The Dataset Concept\u00b6","text":"<p>The core building block in MESCAL is the Dataset class. The key insight is that:</p>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#everything-is-a-dataset","title":"Everything is a Dataset!\u00b6","text":"<ul> <li>Individual scenarios are Datasets</li> <li>Collections of scenarios are Datasets</li> <li>Scenario comparisons are Datasets</li> <li>Collections of comparisons are Datasets</li> </ul> <p>This means you interact with all entities through a consistent interface, regardless of whether you're working with a single scenario or a complex collection of scenario comparisons.</p>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#working-with-datasets","title":"Working with Datasets\u00b6","text":"<p>Let's explore the fundamental operations with Datasets:</p>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#accessing-a-single-dataset","title":"Accessing a Single Dataset\u00b6","text":""},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#fetching-data","title":"Fetching Data\u00b6","text":"<p>The primary method for interacting with Datasets is the <code>fetch()</code> method:</p>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#discovering-available-data","title":"Discovering Available Data\u00b6","text":"<p>To see what data is available in a Dataset:</p>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#from-simple-to-powerful-scenario-collections","title":"From Simple to Powerful: Scenario Collections\u00b6","text":"<p>While the individual Dataset interface is useful, MESCAL's true power emerges when working with multiple scenarios.</p> <p>Let's fetch the marginal price data for all scenarios at once:</p>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#scenario-comparisons","title":"Scenario Comparisons\u00b6","text":"<p>Similarly, we can get comparison data (deltas between scenarios):</p>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#visualization-example","title":"Visualization Example\u00b6","text":"<p>Now let's see this in action with a visualization. We'll create a unified analysis of average generation by carrier, control area, and scenario:</p>"},{"location":"mescal-study-01/notebooks/mescal_101_study_manager_and_basic_fetching/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ul> <li>Unified Interface: Whether working with individual scenarios or complex collections, the same methods apply</li> <li>Efficient Analysis: Analyze multiple scenarios with the same code you'd use for one</li> <li>Automatic Comparison: Calculate scenario deltas without manual calculations</li> <li>Hierarchical Organization: Study \u2192 Scenarios \u2192 Individual Datasets provides a logical structure</li> <li>Consistency Across Platforms: The same code works regardless of your modeling platform</li> </ul> <p>In the next notebook, we'll explore more advanced data fetching and transformation techniques that build on these fundamentals.</p>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/","title":"MESCAL 102: Advanced Data Fetching Techniques","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git\n        %cd mescal-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mescal -U\n        !pip install git+https://github.com/helgeesch/mescal-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git         %cd mescal-vanilla-studies/          !pip install git+https://github.com/helgeesch/mescal -U         !pip install git+https://github.com/helgeesch/mescal-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>Running locally, let's continue.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mescal-vanilla-studies')\n    os.chdir('/content/mescal-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mescal import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mescal-vanilla-studies')     os.chdir('/content/mescal-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mescal import StudyManager except ImportError:     raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import plotly.express as px\n\nfrom mescal.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query\nfrom mescal.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nPlotlyTheme().apply()\nrenderer = ConditionalRenderer()\n</pre> import plotly.express as px  from mescal.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query from mescal.utils.plotly_utils.plotly_theme import PlotlyTheme from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() PlotlyTheme().apply() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Same study setup as in 101, only this time it's from a method so we can re-use it\nfrom studies.study_01_intro_to_mescal.scripts.setup_study_manager import get_scigrid_de_study_manager\n\nstudy = get_scigrid_de_study_manager()\n</pre> # Same study setup as in 101, only this time it's from a method so we can re-use it from studies.study_01_intro_to_mescal.scripts.setup_study_manager import get_scigrid_de_study_manager  study = get_scigrid_de_study_manager()  In\u00a0[5]: Copied! <pre>df_price_change = study.comp.fetch('buses_t.marginal_price')\ndf_price_change_mean = df_price_change.mean().unstack('dataset')\nprint(df_price_change_mean)\n</pre> df_price_change = study.comp.fetch('buses_t.marginal_price') df_price_change_mean = df_price_change.mean().unstack('dataset') print(df_price_change_mean) <pre>dataset    solar_150 vs base  solar_200 vs base  wind_150 vs base  wind_200 vs base\nBus                                                                                \n1                       0.21               0.23             -6.64             -7.29\n10                      0.25               0.21             -6.22             -8.21\n100                    -0.71              -1.28             -1.50             -3.40\n100_220kV              -0.72              -1.29             -1.50             -3.41\n101                     0.14               0.10             -6.12             -7.51\n...                      ...                ...               ...               ...\n96_220kV               -0.17              -0.41             -4.03             -4.81\n97                     -0.69              -1.24             -1.39             -2.56\n98                      0.17               0.13             -6.16             -7.61\n99                     -0.71              -1.28             -1.32             -3.21\n99_220kV               -0.71              -1.28             -1.34             -3.23\n\n[585 rows x 4 columns]\n</pre> <p>The result is a DataFrame showing the average price change per bus for each scenario comparison.</p> In\u00a0[6]: Copied! <pre>df_price_all = study.scen_comp.fetch('buses_t.marginal_price')\nprint(df_price_all.head())\n</pre> df_price_all = study.scen_comp.fetch('buses_t.marginal_price') print(df_price_all.head()) <pre>type                scenario               ...       comparison               \ndataset                 base               ... wind_200 vs base               \nBus                        1    10    100  ...               98    99 99_220kV\nsnapshot                                   ...                                \n2011-01-01 00:00:00    -0.44  5.77  23.12  ...            -1.82  0.10     0.11\n2011-01-01 01:00:00    -0.58  6.10  22.53  ...            -2.02 -0.81    -0.82\n2011-01-01 02:00:00    -0.58  6.07  22.11  ...            -2.01 -2.26    -2.26\n2011-01-01 03:00:00    -0.60  6.14  21.50  ...            -2.15 -3.48    -3.48\n2011-01-01 04:00:00    -0.61  6.16  20.39  ...            -2.15 -5.06    -5.05\n\n[5 rows x 5265 columns]\n</pre> <p>This DataFrame contains both the scenario data and comparison data, distinguished by the 'type' level in the MultiIndex.</p> In\u00a0[7]: Copied! <pre>df_price_all_mean = df_price_all.mean().droplevel('type').unstack('dataset').sort_index(axis=1)\nprint(df_price_all_mean)\n</pre> df_price_all_mean = df_price_all.mean().droplevel('type').unstack('dataset').sort_index(axis=1) print(df_price_all_mean) <pre>dataset     base  solar_150  solar_150 vs base  ...  wind_150 vs base  wind_200  wind_200 vs base\nBus                                             ...                                              \n1           7.68       7.89               0.21  ...             -6.64      0.38             -7.29\n10         10.99      11.25               0.25  ...             -6.22      2.78             -8.21\n100        21.88      21.17              -0.71  ...             -1.50     18.48             -3.40\n100_220kV  21.98      21.26              -0.72  ...             -1.50     18.57             -3.41\n101         8.25       8.39               0.14  ...             -6.12      0.74             -7.51\n...          ...        ...                ...  ...               ...       ...               ...\n96_220kV   13.21      13.04              -0.17  ...             -4.03      8.40             -4.81\n97         21.29      20.59              -0.69  ...             -1.39     18.73             -2.56\n98          8.58       8.74               0.17  ...             -6.16      0.97             -7.61\n99         22.22      21.51              -0.71  ...             -1.32     19.01             -3.21\n99_220kV   22.20      21.49              -0.71  ...             -1.34     18.97             -3.23\n\n[585 rows x 9 columns]\n</pre> <p>This creates a table with average prices and deltas side by side, sorted alphabetically by dataset name.</p> In\u00a0[8]: Copied! <pre>buses_model_df = study.scen.get_dataset('base').fetch('buses')\nprint(buses_model_df)\n</pre> buses_model_df = study.scen.get_dataset('base').fetch('buses') print(buses_model_df) <pre>           v_nom type      x  ...                                      wkt_srid_4326                   location control_area\nBus                           ...                                                                                           \n1          220.0        9.52  ...  SRID=4326;POINT(9.52257596986262 52.3604090557...   POINT (9.52258 52.36041)     TenneTDE\n2          380.0        9.11  ...  SRID=4326;POINT(9.11321007472722 52.5438533223...   POINT (9.11321 52.54385)     TenneTDE\n3          380.0        9.39  ...  SRID=4326;POINT(9.38974509624863 52.0263130660...   POINT (9.38975 52.02631)     TenneTDE\n4          380.0        9.13  ...  SRID=4326;POINT(9.12526570294975 52.5382640870...   POINT (9.12527 52.53826)     TenneTDE\n5          380.0       10.37  ...  SRID=4326;POINT(10.3662749375017 52.2846467462...  POINT (10.36627 52.28465)     TenneTDE\n...          ...  ...    ...  ...                                                ...                        ...          ...\n404_220kV  220.0        8.23  ...  SRID=4326;POINT(8.23209442033504 47.5561434831...   POINT (8.23209 47.55614)   TransnetBW\n413_220kV  220.0        8.67  ...  SRID=4326;POINT(8.67371728898445 49.2904393851...   POINT (8.67372 49.29044)   TransnetBW\n421_220kV  220.0        9.09  ...  SRID=4326;POINT(9.09183529743294 49.2946188662...   POINT (9.09184 49.29462)   TransnetBW\n450_220kV  220.0        7.42  ...  SRID=4326;POINT(7.41670791195847 51.4570521583...   POINT (7.41671 51.45705)      Amprion\n458_220kV  220.0        7.42  ...  SRID=4326;POINT(7.41946378046051 51.4575100936...   POINT (7.41946 51.45751)      Amprion\n\n[585 rows x 21 columns]\n</pre> In\u00a0[9]: Copied! <pre>filtered_price_df = filter_by_model_query(df_price_all, buses_model_df, 'v_nom &gt;= 380')\nprint(filtered_price_df.head())\n</pre> filtered_price_df = filter_by_model_query(df_price_all, buses_model_df, 'v_nom &gt;= 380') print(filtered_price_df.head()) <pre>type                scenario               ...       comparison            \ndataset                 base               ... wind_200 vs base            \nBus                       10    100   101  ...               97    98    99\nsnapshot                                   ...                             \n2011-01-01 00:00:00     5.77  23.12  1.36  ...            -1.10 -1.82  0.10\n2011-01-01 01:00:00     6.10  22.53  1.35  ...             0.51 -2.02 -0.81\n2011-01-01 02:00:00     6.07  22.11  1.34  ...            -1.21 -2.01 -2.26\n2011-01-01 03:00:00     6.14  21.50  1.38  ...            -2.35 -2.15 -3.48\n2011-01-01 04:00:00     6.16  20.39  1.41  ...            -5.12 -2.15 -5.06\n\n[5 rows x 2592 columns]\n</pre> <p>The <code>filter_by_model_query</code> utility applies pandas query syntax to filter time series based on model properties.</p> In\u00a0[10]: Copied! <pre>price_with_control_area = prepend_model_prop_levels(filtered_price_df, buses_model_df, 'control_area')\nprint(price_with_control_area.head())\n</pre> price_with_control_area = prepend_model_prop_levels(filtered_price_df, buses_model_df, 'control_area') print(price_with_control_area.head()) <pre>control_area         50Hertz  Amprion  50Hertz  ...         TenneTDE          50Hertz          Amprion\ntype                scenario scenario scenario  ...       comparison       comparison       comparison\ndataset                 base     base     base  ... wind_200 vs base wind_200 vs base wind_200 vs base\nBus                       10      100      101  ...               97               98               99\nsnapshot                                        ...                                                   \n2011-01-01 00:00:00     5.77    23.12     1.36  ...            -1.10            -1.82             0.10\n2011-01-01 01:00:00     6.10    22.53     1.35  ...             0.51            -2.02            -0.81\n2011-01-01 02:00:00     6.07    22.11     1.34  ...            -1.21            -2.01            -2.26\n2011-01-01 03:00:00     6.14    21.50     1.38  ...            -2.35            -2.15            -3.48\n2011-01-01 04:00:00     6.16    20.39     1.41  ...            -5.12            -2.15            -5.06\n\n[5 rows x 2592 columns]\n</pre> <p>This operation adds the 'control_area' property from the bus model as a new level in our multi-index DataFrame.</p> In\u00a0[11]: Copied! <pre>price_by_control_area = price_with_control_area.mean().groupby(level=['control_area', 'dataset']).mean().unstack('control_area')\nprint(price_by_control_area)\n</pre> price_by_control_area = price_with_control_area.mean().groupby(level=['control_area', 'dataset']).mean().unstack('control_area') print(price_by_control_area) <pre>control_area       50Hertz  Amprion  TenneTDE  TransnetBW\ndataset                                                  \nbase                 11.66    18.88     15.09       23.33\nsolar_150            11.63    18.26     14.64       22.74\nsolar_150 vs base    -0.03    -0.63     -0.45       -0.59\nsolar_200            11.45    17.79     14.29       22.18\nsolar_200 vs base    -0.21    -1.09     -0.80       -1.16\nwind_150              7.09    17.08     12.08       22.66\nwind_150 vs base     -4.57    -1.81     -3.01       -0.67\nwind_200              4.77    15.64     10.42       21.04\nwind_200 vs base     -6.89    -3.24     -4.67       -2.29\n</pre> In\u00a0[12]: Copied! <pre>buses_model_df = study.scen.get_dataset('base').fetch('buses')\ndata = study.scen_comp.fetch('buses_t.marginal_price')\ndata = filter_by_model_query(data, buses_model_df, query='v_nom &gt;= 380')\ndata = prepend_model_prop_levels(data, buses_model_df, 'control_area')\ndata = data.T.groupby(level=['dataset', 'control_area']).mean().T\ndata_flat = flatten_df(data)\nfig = px.box(\n    data_frame=data_flat,\n    x='control_area',\n    color='dataset',\n    y='value',\n    category_orders={'control_area': ['Amprion', 'TransnetBW', 'TenneTDE', '50Hertz']},\n    labels={'value': 'Hourly marginal price [\u20ac/MWh]'},\n)\nfig.update_traces(boxmean=True)\nfig.update_layout(title='&lt;b&gt;Distribution of hourly price occurrences per scenario (price change per comparison)&lt;/b&gt;', width=1200)\nfig.update_xaxes(title=None)\n\nrenderer.show_plotly(fig)\n</pre> buses_model_df = study.scen.get_dataset('base').fetch('buses') data = study.scen_comp.fetch('buses_t.marginal_price') data = filter_by_model_query(data, buses_model_df, query='v_nom &gt;= 380') data = prepend_model_prop_levels(data, buses_model_df, 'control_area') data = data.T.groupby(level=['dataset', 'control_area']).mean().T data_flat = flatten_df(data) fig = px.box(     data_frame=data_flat,     x='control_area',     color='dataset',     y='value',     category_orders={'control_area': ['Amprion', 'TransnetBW', 'TenneTDE', '50Hertz']},     labels={'value': 'Hourly marginal price [\u20ac/MWh]'}, ) fig.update_traces(boxmean=True) fig.update_layout(title='Distribution of hourly price occurrences per scenario (price change per comparison)', width=1200) fig.update_xaxes(title=None)  renderer.show_plotly(fig) <pre>/Users/helgeesch/Documents/repositories/mescal-vanilla-studies/venv/lib/python3.12/site-packages/_plotly_utils/basevalidators.py:2596: DeprecationWarning:\n\n*scattermapbox* is deprecated! Use *scattermap* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n\n/Users/helgeesch/Documents/repositories/mescal-vanilla-studies/venv/lib/python3.12/site-packages/_plotly_utils/basevalidators.py:2596: DeprecationWarning:\n\n*scattermapbox* is deprecated! Use *scattermap* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n\n/Users/helgeesch/Documents/repositories/mescal-vanilla-studies/venv/lib/python3.12/site-packages/_plotly_utils/basevalidators.py:2596: DeprecationWarning:\n\n*scattermapbox* is deprecated! Use *scattermap* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n\n/Users/helgeesch/Documents/repositories/mescal-vanilla-studies/venv/lib/python3.12/site-packages/kaleido/scopes/base.py:188: DeprecationWarning:\n\nsetDaemon() is deprecated, set the daemon attribute instead\n\n</pre> In\u00a0[13]: Copied! <pre># Start with raw data\ndf_raw = study.scen_comp.fetch('buses_t.marginal_price')\n\n# Filter to high voltage buses\nbuses_model_df = study.scen.get_dataset('base').fetch('buses')\ndf_filtered = filter_by_model_query(df_raw, buses_model_df, 'v_nom &gt;= 380')\n\n# Add control area information\ndf_with_areas = prepend_model_prop_levels(df_filtered, buses_model_df, 'control_area')\n\n# Calculate hourly area prices\nhourly_area_prices = df_with_areas.T.groupby(['dataset', 'control_area']).mean().T.unstack()\n\n# Reshape for analysis\narea_price_stats = hourly_area_prices.groupby(['dataset', 'control_area']).agg(['mean', 'std', 'min', 'max'])\n\nprint(area_price_stats)\n</pre> # Start with raw data df_raw = study.scen_comp.fetch('buses_t.marginal_price')  # Filter to high voltage buses buses_model_df = study.scen.get_dataset('base').fetch('buses') df_filtered = filter_by_model_query(df_raw, buses_model_df, 'v_nom &gt;= 380')  # Add control area information df_with_areas = prepend_model_prop_levels(df_filtered, buses_model_df, 'control_area')  # Calculate hourly area prices hourly_area_prices = df_with_areas.T.groupby(['dataset', 'control_area']).mean().T.unstack()  # Reshape for analysis area_price_stats = hourly_area_prices.groupby(['dataset', 'control_area']).agg(['mean', 'std', 'min', 'max'])  print(area_price_stats) <pre>                                 mean   std    min    max\ndataset           control_area                           \nbase              50Hertz       11.66  4.90   7.54  22.37\n                  Amprion       18.88  3.17  14.31  23.37\n                  TenneTDE      15.09  4.93   8.25  22.19\n                  TransnetBW    23.33  2.80  19.34  25.85\nsolar_150         50Hertz       11.63  4.88   7.62  22.17\n                  Amprion       18.26  3.83  12.23  23.38\n                  TenneTDE      14.64  5.38   6.99  22.07\n                  TransnetBW    22.74  3.43  16.90  25.85\nsolar_150 vs base 50Hertz       -0.03  0.17  -0.48   0.26\n                  Amprion       -0.63  0.92  -2.59   0.15\n                  TenneTDE      -0.45  0.76  -2.27   0.51\n                  TransnetBW    -0.59  1.02  -4.12   0.07\nsolar_200         50Hertz       11.45  4.87   6.85  21.82\n                  Amprion       17.79  4.25  11.10  23.27\n                  TenneTDE      14.29  5.63   6.32  22.00\n                  TransnetBW    22.18  4.14  14.74  25.85\nsolar_200 vs base 50Hertz       -0.21  0.25  -0.81   0.27\n                  Amprion       -1.09  1.48  -4.50   0.09\n                  TenneTDE      -0.80  1.20  -3.94   0.55\n                  TransnetBW    -1.16  1.66  -4.96   0.18\nwind_150          50Hertz        7.09  2.57   4.44  14.42\n                  Amprion       17.08  2.48  12.77  20.28\n                  TenneTDE      12.08  3.11   7.18  17.09\n                  TransnetBW    22.66  3.35  18.08  25.88\nwind_150 vs base  50Hertz       -4.57  2.69  -8.67  -1.05\n                  Amprion       -1.81  1.33  -3.49   0.88\n                  TenneTDE      -3.01  2.22  -6.34  -0.06\n                  TransnetBW    -0.67  1.01  -4.21   0.14\nwind_200          50Hertz        4.77  2.16   2.01   8.85\n                  Amprion       15.64  3.20  10.32  19.47\n                  TenneTDE      10.42  3.47   4.93  15.57\n                  TransnetBW    21.04  5.06  12.37  25.94\nwind_200 vs base  50Hertz       -6.89  2.99 -13.52  -2.81\n                  Amprion       -3.24  1.21  -5.10   0.26\n                  TenneTDE      -4.67  2.20  -8.58  -0.60\n                  TransnetBW    -2.29  2.50  -7.52   0.19\n</pre> <p>What would normally take dozens of lines of data manipulation code is condensed into a few expressive operations that maintain the relationships between different types of data.</p>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#mescal-102-advanced-data-fetching-techniques","title":"MESCAL 102: Advanced Data Fetching Techniques\u00b6","text":""},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#introduction","title":"Introduction\u00b6","text":"<p>Building on the StudyManager fundamentals, this notebook demonstrates MESCAL's advanced data fetching and transformation capabilities. These techniques significantly streamline multi-scenario analysis by leveraging pandas' power while abstracting away common boilerplate code.</p> <p>MESCAL's data handling utilities provide efficient ways to:</p> <ul> <li>Work with multi-index DataFrames from scenario and comparison collections</li> <li>Filter data using model properties</li> <li>Aggregate data across complex dimensions</li> <li>Combine scenarios and comparisons in unified analyses</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mescal and all requirements are installed.</p>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#working-with-multi-index-dataframes","title":"Working with Multi-Index DataFrames\u00b6","text":"<p>Fetching data across multiple scenarios naturally creates multi-index DataFrames. Let's examine and transform these structures:</p>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#calculating-averages-per-scenario-comparison","title":"Calculating Averages Per Scenario Comparison\u00b6","text":""},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#combining-scenarios-and-comparisons","title":"Combining Scenarios and Comparisons\u00b6","text":"<p>To analyze both raw values and deltas in one operation:</p>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#unified-analysis","title":"Unified Analysis\u00b6","text":"<p>We can perform operations on this unified structure:</p>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#integrating-model-data-with-time-series","title":"Integrating Model Data with Time Series\u00b6","text":"<p>A powerful MESCAL capability is the integration of static model data with time series data.</p>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#accessing-model-data","title":"Accessing Model Data\u00b6","text":""},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#filtering-by-model-properties","title":"Filtering by Model Properties\u00b6","text":"<p>Let's filter our time series to include only high voltage buses (v_nom &gt;= 380 kV):</p>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#prepending-model-properties","title":"Prepending Model Properties\u00b6","text":"<p>We can add model properties as additional index levels:</p>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#aggregating-by-model-properties","title":"Aggregating by Model Properties\u00b6","text":"<p>Now we can efficiently aggregate by control area:</p>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#sophisticated-visualization-example","title":"Sophisticated Visualization Example\u00b6","text":"<p>Let's demonstrate these techniques with a more sophisticated visualization. We'll create a boxplot showing the distribution of hourly prices by control area and scenario:</p>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#advanced-transformation-pipeline","title":"Advanced Transformation Pipeline\u00b6","text":"<p>Let's trace through a complete data transformation pipeline that demonstrates MESCAL's efficiency:</p>"},{"location":"mescal-study-01/notebooks/mescal_102_mastering_data_fetching/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ul> <li>Unified Analysis: Work with scenarios and comparisons in the same analytical framework</li> <li>Model and Time-Series Data Fetching: Unified framework for fetching model and time-series data</li> <li>Useful Integration Utilities: MESCAL provides tools like <code>filter_by_model_query</code> and <code>prepend_model_prop_levels</code> that efficiently establish relationships between model properties and time series data and provide frequently needed filtering and property mapping features</li> </ul> <p>These capabilities make MESCAL particularly powerful for complex multi-scenario analyses that would otherwise require extensive custom code.</p>"},{"location":"mescal-study-01/notebooks/mescal_103_scenario_attributes/","title":"MESCAL 103: Scenario Attributes","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git\n        %cd mescal-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mescal -U\n        !pip install git+https://github.com/helgeesch/mescal-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git         %cd mescal-vanilla-studies/          !pip install git+https://github.com/helgeesch/mescal -U         !pip install git+https://github.com/helgeesch/mescal-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>Running locally, let's continue.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mescal-vanilla-studies')\n    os.chdir('/content/mescal-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mescal import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mescal-vanilla-studies')     os.chdir('/content/mescal-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mescal import StudyManager except ImportError:     raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport plotly.express as px\nimport pypsa\n\nfrom mescal import StudyManager\nfrom mescal.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query\nfrom mescal.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom mescal_pypsa import PyPSADataset\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nPlotlyTheme().apply()\nrenderer = ConditionalRenderer()\n</pre> import os import plotly.express as px import pypsa  from mescal import StudyManager from mescal.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query from mescal.utils.plotly_utils.plotly_theme import PlotlyTheme from mescal_pypsa import PyPSADataset from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() PlotlyTheme().apply() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Register study-specific interpreters (details on this will be covered in a later notebook)\nfrom studies.study_01_intro_to_mescal.src.study_specific_model_interpreters import ControlAreaModelInterpreter, ScigridDEBusModelInterpreter\n\nPyPSADataset.register_interpreter(ControlAreaModelInterpreter)\nPyPSADataset.register_interpreter(ScigridDEBusModelInterpreter)\n\n# setup up study same as in previous notebooks, but this time also set attributes\nstudy_folder = 'studies/study_01_intro_to_mescal'\nnetworks_folder = os.path.join(study_folder, 'data/networks_scigrid_de')\n\nn_base = pypsa.Network(os.path.join(networks_folder, 'base.nc'))\nn_solar_150 = pypsa.Network(os.path.join(networks_folder, 'solar_150.nc'))\nn_solar_200 = pypsa.Network(os.path.join(networks_folder, 'solar_200.nc'))\nn_wind_150 = pypsa.Network(os.path.join(networks_folder, 'wind_150.nc'))\nn_wind_200 = pypsa.Network(os.path.join(networks_folder, 'wind_200.nc'))\n\nstudy = StudyManager.factory_from_scenarios(\n    scenarios=[\n        PyPSADataset(n_base,        name='base'),\n        PyPSADataset(n_solar_150,   name='solar_150', attributes=dict(res_tech='solar', scaling_factor=150)),\n        PyPSADataset(n_solar_200,   name='solar_200', attributes=dict(res_tech='solar', scaling_factor=200)),\n        PyPSADataset(n_wind_150,    name='wind_150', attributes=dict(res_tech='wind', scaling_factor=150)),\n        PyPSADataset(n_wind_200,    name='wind_200', attributes=dict(res_tech='wind', scaling_factor=200)),\n    ],\n    comparisons=[('solar_150', 'base'), ('solar_200', 'base'), ('wind_150', 'base'), ('wind_200', 'base')],\n    export_folder=os.path.join(study_folder, 'non_versioned/output'),\n)\n</pre> # Register study-specific interpreters (details on this will be covered in a later notebook) from studies.study_01_intro_to_mescal.src.study_specific_model_interpreters import ControlAreaModelInterpreter, ScigridDEBusModelInterpreter  PyPSADataset.register_interpreter(ControlAreaModelInterpreter) PyPSADataset.register_interpreter(ScigridDEBusModelInterpreter)  # setup up study same as in previous notebooks, but this time also set attributes study_folder = 'studies/study_01_intro_to_mescal' networks_folder = os.path.join(study_folder, 'data/networks_scigrid_de')  n_base = pypsa.Network(os.path.join(networks_folder, 'base.nc')) n_solar_150 = pypsa.Network(os.path.join(networks_folder, 'solar_150.nc')) n_solar_200 = pypsa.Network(os.path.join(networks_folder, 'solar_200.nc')) n_wind_150 = pypsa.Network(os.path.join(networks_folder, 'wind_150.nc')) n_wind_200 = pypsa.Network(os.path.join(networks_folder, 'wind_200.nc'))  study = StudyManager.factory_from_scenarios(     scenarios=[         PyPSADataset(n_base,        name='base'),         PyPSADataset(n_solar_150,   name='solar_150', attributes=dict(res_tech='solar', scaling_factor=150)),         PyPSADataset(n_solar_200,   name='solar_200', attributes=dict(res_tech='solar', scaling_factor=200)),         PyPSADataset(n_wind_150,    name='wind_150', attributes=dict(res_tech='wind', scaling_factor=150)),         PyPSADataset(n_wind_200,    name='wind_200', attributes=dict(res_tech='wind', scaling_factor=200)),     ],     comparisons=[('solar_150', 'base'), ('solar_200', 'base'), ('wind_150', 'base'), ('wind_200', 'base')],     export_folder=os.path.join(study_folder, 'non_versioned/output'), ) In\u00a0[5]: Copied! <pre>scen_attributes_df = study.scen.get_attributes_concat_df()\nprint(scen_attributes_df)\n</pre> scen_attributes_df = study.scen.get_attributes_concat_df() print(scen_attributes_df) <pre>attribute res_tech scaling_factor\ndataset                          \nbase           NaN            NaN\nsolar_150    solar            150\nsolar_200    solar            200\nwind_150      wind            150\nwind_200      wind            200\n</pre> <p>Of course, the same method can be called for the collection of comparisons and returns a similar df.</p> In\u00a0[6]: Copied! <pre>comp_attributes_df = study.comp.get_attributes_concat_df()\nprint(comp_attributes_df)\n</pre> comp_attributes_df = study.comp.get_attributes_concat_df() print(comp_attributes_df) <pre>attribute         res_tech scaling_factor variation_dataset reference_dataset\ndataset                                                                      \nsolar_150 vs base    solar            150         solar_150              base\nsolar_200 vs base    solar            200         solar_200              base\nwind_150 vs base      wind            150          wind_150              base\nwind_200 vs base      wind            200          wind_200              base\n</pre> <p>As you can see, for DatasetComparisons, the method automatically uses the attributes of the variation dataset while also including the names to variation as well as reference dataset. You could, of course, also set attributes explicitly for every comparison dataset individually.</p> <p>From the previous notebooks, you might also remember that there is even a combined dataset for all scenarios + comparisons. The method even works for this one, as you can see below.</p> In\u00a0[7]: Copied! <pre>print(study.scen_comp.get_attributes_concat_df())\n</pre> print(study.scen_comp.get_attributes_concat_df()) <pre>attribute                    res_tech scaling_factor variation_dataset reference_dataset\ntype       dataset                                                                      \nscenario   base                   NaN            NaN               NaN               NaN\n           solar_150            solar            150               NaN               NaN\n           solar_200            solar            200               NaN               NaN\n           wind_150              wind            150               NaN               NaN\n           wind_200              wind            200               NaN               NaN\ncomparison solar_150 vs base    solar            150         solar_150              base\n           solar_200 vs base    solar            200         solar_200              base\n           wind_150 vs base      wind            150          wind_150              base\n           wind_200 vs base      wind            200          wind_200              base\n</pre> <p>You might be wondering, when would we need these attributes? Making use of Dataset attributes is entirely optional, but is extremely useful, for example when you want to categorize or group your assessment by different attributes.</p> <p>In the example below, we want to show the average price change per control area and categorize the scenarios by res_tech and scaling_factor, here we go:</p> In\u00a0[8]: Copied! <pre>comp_attributes_df = study.comp.get_attributes_concat_df()\nbuses_model_df = study.scen.get_dataset('base').fetch('buses')\ndata = study.comp.fetch('buses_t.marginal_price')\ndata = prepend_model_prop_levels(data, buses_model_df, 'control_area')\ndata = data.T.groupby(level=['dataset', 'control_area']).mean().T\ndata = data.mean().to_frame('value')\ndata = prepend_model_prop_levels(data, comp_attributes_df, 'res_tech', 'scaling_factor')\ndata = data.reset_index()\nfig = px.bar(\n    data_frame=data,\n    x='scaling_factor',\n    y='value',\n    color='res_tech',\n    facet_col='control_area',\n    labels={'scaling_factor': 'RES scaled to [%]', 'value': 'average price change compared to base [\u20ac/MWh]'},\n    barmode='group',\n)\nfig.update_layout(title='&lt;b&gt;Average price change per control area and scenario&lt;/b&gt;', width=1200)\n\nrenderer.show_plotly(fig)\n</pre> comp_attributes_df = study.comp.get_attributes_concat_df() buses_model_df = study.scen.get_dataset('base').fetch('buses') data = study.comp.fetch('buses_t.marginal_price') data = prepend_model_prop_levels(data, buses_model_df, 'control_area') data = data.T.groupby(level=['dataset', 'control_area']).mean().T data = data.mean().to_frame('value') data = prepend_model_prop_levels(data, comp_attributes_df, 'res_tech', 'scaling_factor') data = data.reset_index() fig = px.bar(     data_frame=data,     x='scaling_factor',     y='value',     color='res_tech',     facet_col='control_area',     labels={'scaling_factor': 'RES scaled to [%]', 'value': 'average price change compared to base [\u20ac/MWh]'},     barmode='group', ) fig.update_layout(title='Average price change per control area and scenario', width=1200)  renderer.show_plotly(fig) <p>You might have spottet that the prepend_model_prop_levels method is applied in combination with the comp_attributes_df. In this application, the attributes_df basically just functions as a model_df (mapping from object names to properties/memberships/attributes) for the index level with dataset names.</p> <p>In many cases, dataset attributes are actually retrievable from the raw-data folder / file and can be interpreted based on a rule. This is especially useful if you are managing more than just a hand full of scenarios with potentially many attribute dimensions.</p> <p>The used method in the example below utilizes the rule based attribute setting and sets up the study manager instance by a method call. Visit the source code if you want to see the implementation for yourself.</p> In\u00a0[9]: Copied! <pre>from studies.study_01_intro_to_mescal.scripts.setup_study_manager import get_scigrid_de_study_manager\n\nstudy = get_scigrid_de_study_manager()  # same study setup as from above, using rule based attribute attribution and a re-usable script.\n</pre> from studies.study_01_intro_to_mescal.scripts.setup_study_manager import get_scigrid_de_study_manager  study = get_scigrid_de_study_manager()  # same study setup as from above, using rule based attribute attribution and a re-usable script."},{"location":"mescal-study-01/notebooks/mescal_103_scenario_attributes/#mescal-103-scenario-attributes","title":"MESCAL 103: Scenario Attributes\u00b6","text":""},{"location":"mescal-study-01/notebooks/mescal_103_scenario_attributes/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mescal and all requirements are installed.</p>"},{"location":"mescal-study-01/notebooks/mescal_103_scenario_attributes/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ul> <li>Dataset Attribute Feature: Describe scenarios and scenario comparisons with attributes and have the information at hand during your assessment.</li> <li>Modularity of descriptive <code>model_df</code> framework: Treat the attributes_df as if it was a <code>model_df</code> for the dataset names and make use of tools like <code>filter_by_model_query</code> and <code>prepend_model_prop_levels</code> in combination with multi-scenario (multi-scenario-comparison) dataframes.</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/","title":"MESCAL 301: Time Series Dashboard","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git\n        %cd mescal-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mescal -U\n        !pip install git+https://github.com/helgeesch/mescal-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git         %cd mescal-vanilla-studies/          !pip install git+https://github.com/helgeesch/mescal -U         !pip install git+https://github.com/helgeesch/mescal-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>Running locally, let's continue.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mescal-vanilla-studies')\n    os.chdir('/content/mescal-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mescal import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mescal-vanilla-studies')     os.chdir('/content/mescal-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mescal import StudyManager except ImportError:     raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport pandas as pd\n\nfrom mescal.visualizations.plotly_figures import TimeSeriesDashboardGenerator\nfrom mescal.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom mescal.utils.pandas_utils import xs_df\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nPlotlyTheme().apply()\nrenderer = ConditionalRenderer(height=800, width=1600)\n</pre> import os import pandas as pd  from mescal.visualizations.plotly_figures import TimeSeriesDashboardGenerator from mescal.utils.plotly_utils.plotly_theme import PlotlyTheme from mescal.utils.pandas_utils import xs_df from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() PlotlyTheme().apply() renderer = ConditionalRenderer(height=800, width=1600) In\u00a0[4]: Copied! <pre># Load sample data\nurl = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\"\ndf_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)\n\n# Create scenario dataset with variations\ndf_raw_scenarios = pd.concat(\n    {\n        'base': df_raw,\n        'scen1': df_raw ** 0.5,  # numerical transformation to mock a \"scenario\"\n        'scen2': df_raw ** 0.2,  # numerical transformation to mock a \"scenario\"\n    },\n    axis=1,\n    names=['dataset']\n)\n\n# Create dataset with renewable generation (as percentages)\ndf_res = df_raw[['onwind', 'offwind', 'solar']].copy() * 100\n# Remove one data point to show handling of missing data\ndf_res_scenarios = xs_df(df_raw_scenarios.copy(), ['onwind', 'offwind', 'solar'], level='variable', axis=1).drop(('scen1', 'offwind'), axis=1)\n\n# Create mixed dataset with price, load, and solar data\ndf_mixed = df_raw[['prices', 'load', 'solar']].copy()\ndf_mixed['solar'] *= 100  # Convert solar to percentage\ndf_mixed_scenarios = xs_df(df_raw_scenarios.copy(), ['prices', 'load', 'solar'], level='variable', axis=1)\nfor c in df_mixed_scenarios.columns:\n    if 'solar' in c:\n        df_mixed_scenarios[c] *= 100\n</pre> # Load sample data url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\" df_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)  # Create scenario dataset with variations df_raw_scenarios = pd.concat(     {         'base': df_raw,         'scen1': df_raw ** 0.5,  # numerical transformation to mock a \"scenario\"         'scen2': df_raw ** 0.2,  # numerical transformation to mock a \"scenario\"     },     axis=1,     names=['dataset'] )  # Create dataset with renewable generation (as percentages) df_res = df_raw[['onwind', 'offwind', 'solar']].copy() * 100 # Remove one data point to show handling of missing data df_res_scenarios = xs_df(df_raw_scenarios.copy(), ['onwind', 'offwind', 'solar'], level='variable', axis=1).drop(('scen1', 'offwind'), axis=1)  # Create mixed dataset with price, load, and solar data df_mixed = df_raw[['prices', 'load', 'solar']].copy() df_mixed['solar'] *= 100  # Convert solar to percentage df_mixed_scenarios = xs_df(df_raw_scenarios.copy(), ['prices', 'load', 'solar'], level='variable', axis=1) for c in df_mixed_scenarios.columns:     if 'solar' in c:         df_mixed_scenarios[c] *= 100 In\u00a0[5]: Copied! <pre># Display RES ts\nprint(\"Renewable generation data shape:\", df_res.shape, \"\\n\")\nprint(df_res.head())\n</pre> # Display RES ts print(\"Renewable generation data shape:\", df_res.shape, \"\\n\") print(df_res.head()) <pre>Renewable generation data shape: (8760, 3) \n\nvariable             onwind  offwind  solar\n2015-01-01 00:00:00   15.66    70.30    0.0\n2015-01-01 01:00:00   16.59    68.75    0.0\n2015-01-01 02:00:00   17.46    65.35    0.0\n2015-01-01 03:00:00   17.45    68.03    0.0\n2015-01-01 04:00:00   18.26    72.72    0.0\n</pre> In\u00a0[6]: Copied! <pre>print(\"Mixed data shape:\", df_mixed.shape, \"\\n\")\nprint(df_mixed.tail())\n</pre> print(\"Mixed data shape:\", df_mixed.shape, \"\\n\") print(df_mixed.tail()) <pre>Mixed data shape: (8760, 3) \n\nvariable             prices   load  solar\n2015-12-31 19:00:00   36.79  47.72    0.0\n2015-12-31 20:00:00   28.81  45.91    0.0\n2015-12-31 21:00:00   26.27  45.61    0.0\n2015-12-31 22:00:00   29.99  43.76    0.0\n2015-12-31 23:00:00   31.59  41.91    0.0\n</pre> In\u00a0[7]: Copied! <pre>print(\"Scenarios RES data shape:\", df_res_scenarios.shape, \"\\n\")\nprint(df_res_scenarios.head())\n</pre> print(\"Scenarios RES data shape:\", df_res_scenarios.shape, \"\\n\") print(df_res_scenarios.head()) <pre>Scenarios RES data shape: (8760, 8) \n\ndataset               base                ...  scen2              \nvariable            onwind offwind solar  ... onwind offwind solar\n2015-01-01 00:00:00   0.16    0.70   0.0  ...   0.69    0.93   0.0\n2015-01-01 01:00:00   0.17    0.69   0.0  ...   0.70    0.93   0.0\n2015-01-01 02:00:00   0.17    0.65   0.0  ...   0.71    0.92   0.0\n2015-01-01 03:00:00   0.17    0.68   0.0  ...   0.71    0.93   0.0\n2015-01-01 04:00:00   0.18    0.73   0.0  ...   0.71    0.94   0.0\n\n[5 rows x 8 columns]\n</pre> In\u00a0[8]: Copied! <pre>print(\"Scenarios Mixed data shape:\", df_mixed_scenarios.shape, \"\\n\")\nprint(df_mixed_scenarios.head())\n</pre> print(\"Scenarios Mixed data shape:\", df_mixed_scenarios.shape, \"\\n\") print(df_mixed_scenarios.head()) <pre>Scenarios Mixed data shape: (8760, 9) \n\ndataset               base               ... scen2             \nvariable              load solar prices  ...  load solar prices\n2015-01-01 00:00:00  41.15   0.0    NaN  ...  2.10   0.0    NaN\n2015-01-01 01:00:00  40.13   0.0    NaN  ...  2.09   0.0    NaN\n2015-01-01 02:00:00  39.11   0.0    NaN  ...  2.08   0.0    NaN\n2015-01-01 03:00:00  38.77   0.0    NaN  ...  2.08   0.0    NaN\n2015-01-01 04:00:00  38.94   0.0    NaN  ...  2.08   0.0    NaN\n\n[5 rows x 9 columns]\n</pre> In\u00a0[9]: Copied! <pre># Basic visualization with variables as rows\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='date',                      # X-axis per date (other options are aggregations per: week, month, year_week, year_month)\n    color_continuous_scale='viridis',   # Color scale\n    facet_row='variable',               # Each variable gets its own row\n    facet_row_order=['solar', 'onwind', 'offwind']\n)\n\nfig = generator.get_figure(df_res, title='Renewable Generation Patterns')\n\nrenderer.show_plotly(fig)\n</pre> # Basic visualization with variables as rows generator = TimeSeriesDashboardGenerator(     x_axis='date',                      # X-axis per date (other options are aggregations per: week, month, year_week, year_month)     color_continuous_scale='viridis',   # Color scale     facet_row='variable',               # Each variable gets its own row     facet_row_order=['solar', 'onwind', 'offwind'] )  fig = generator.get_figure(df_res, title='Renewable Generation Patterns')  renderer.show_plotly(fig) <p>You can see a dashboard with three time-series visualizations as heatmaps and some KPIs right next to the heatmaps. The time-series heatmaps have the date on the x-axis and the time of the day on the y-axis. This enables you to quickly understand any intra-daily patterns, such as the one of the solar profile.</p> <p>The stats on the right-hand side are an efficient way to immediately understand some KPIs for each time-series, and how they differ between the different variables.</p> In\u00a0[10]: Copied! <pre>from mescal.visualizations.plotly_figures.timeseries_dashboard import DashboardConfig\n\nstats = DashboardConfig.DEFAULT_STATISTICS.copy()\nstats.pop('Datums')\nstats.pop('Abs max')\nstats.pop('Abs mean')\nstats['Median'] = DashboardConfig.STATISTICS_LIBRARY['Median']\nstats['% == 0'] = lambda x: (x == 0).sum() / len(x) * 100  # custom agg should be a callable of a pd.Series\nstats['% &gt; 50'] = lambda x: (x &gt; 50).sum() / len(x) * 100  # custom agg should be a callable of a pd.Series\n\n# Basic visualization with custom set of stats\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='date',                      # X-axis per date (other options are aggregations per: week, month, year_week, year_month)\n    color_continuous_scale='viridis',   # Color scale\n    facet_row='variable',               # Each variable gets its own row\n    facet_row_order=['solar', 'onwind', 'offwind'],\n    stat_aggs=stats,\n)\n\nfig = generator.get_figure(df_res, title='Renewable Generation Patterns with custom stats')\n\nrenderer.show_plotly(fig)\n</pre> from mescal.visualizations.plotly_figures.timeseries_dashboard import DashboardConfig  stats = DashboardConfig.DEFAULT_STATISTICS.copy() stats.pop('Datums') stats.pop('Abs max') stats.pop('Abs mean') stats['Median'] = DashboardConfig.STATISTICS_LIBRARY['Median'] stats['% == 0'] = lambda x: (x == 0).sum() / len(x) * 100  # custom agg should be a callable of a pd.Series stats['% &gt; 50'] = lambda x: (x &gt; 50).sum() / len(x) * 100  # custom agg should be a callable of a pd.Series  # Basic visualization with custom set of stats generator = TimeSeriesDashboardGenerator(     x_axis='date',                      # X-axis per date (other options are aggregations per: week, month, year_week, year_month)     color_continuous_scale='viridis',   # Color scale     facet_row='variable',               # Each variable gets its own row     facet_row_order=['solar', 'onwind', 'offwind'],     stat_aggs=stats, )  fig = generator.get_figure(df_res, title='Renewable Generation Patterns with custom stats')  renderer.show_plotly(fig) In\u00a0[11]: Copied! <pre># Organizing variables as columns with wrapping\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='date',\n    color_continuous_scale='viridis',\n    facet_col='variable',\n    facet_col_order=['solar', 'onwind', 'offwind'],\n    facet_col_wrap=2  # Wrap after 2 columns\n)\n\nfig = generator.get_figure(df_res, title='Variables as Columns (Wrapped)')\n\nrenderer.show_plotly(fig)\n</pre> # Organizing variables as columns with wrapping generator = TimeSeriesDashboardGenerator(     x_axis='date',     color_continuous_scale='viridis',     facet_col='variable',     facet_col_order=['solar', 'onwind', 'offwind'],     facet_col_wrap=2  # Wrap after 2 columns )  fig = generator.get_figure(df_res, title='Variables as Columns (Wrapped)')  renderer.show_plotly(fig) In\u00a0[12]: Copied! <pre># Creating a grid of scenarios (columns) and variables (rows)\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='date',\n    facet_col='dataset',  # Scenarios as columns\n    facet_row='variable', # Variables as rows\n    facet_col_order=['base', 'scen1', 'scen2'],\n    facet_row_order=['onwind', 'solar', 'offwind'],\n    color_continuous_scale='viridis',\n    subplots_horizontal_spacing=0.05  # Adjust spacing between columns\n)\n\nfig = generator.get_figure(df_res_scenarios, title='Variable Patterns by Scenario')\n\nrenderer.show_plotly(fig)\n</pre> # Creating a grid of scenarios (columns) and variables (rows) generator = TimeSeriesDashboardGenerator(     x_axis='date',     facet_col='dataset',  # Scenarios as columns     facet_row='variable', # Variables as rows     facet_col_order=['base', 'scen1', 'scen2'],     facet_row_order=['onwind', 'solar', 'offwind'],     color_continuous_scale='viridis',     subplots_horizontal_spacing=0.05  # Adjust spacing between columns )  fig = generator.get_figure(df_res_scenarios, title='Variable Patterns by Scenario')  renderer.show_plotly(fig) In\u00a0[13]: Copied! <pre># Custom color scales by row (variable)\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='date',\n    facet_col='dataset',\n    facet_row='variable',\n    facet_col_order=['base', 'scen1', 'scen2'],\n    facet_row_order=['solar', 'load', 'prices'],\n    per_facet_row_colorscale=True,              # Different color scale per row\n    facet_row_color_settings={\n        'solar': {'color_continuous_scale': 'Reds', 'range_color': [0, 100]},\n        'load': {'color_continuous_scale': 'Blues'},\n        'prices': {'color_continuous_scale': 'Portland', 'color_continuous_midpoint': 0},\n    },\n    subplots_horizontal_spacing=0.05  # Adjust spacing between columns\n)\n\nfig = generator.get_figure(df_mixed_scenarios, title='Variable-specific Color Scales')\n\nrenderer.show_plotly(fig)\n</pre> # Custom color scales by row (variable) generator = TimeSeriesDashboardGenerator(     x_axis='date',     facet_col='dataset',     facet_row='variable',     facet_col_order=['base', 'scen1', 'scen2'],     facet_row_order=['solar', 'load', 'prices'],     per_facet_row_colorscale=True,              # Different color scale per row     facet_row_color_settings={         'solar': {'color_continuous_scale': 'Reds', 'range_color': [0, 100]},         'load': {'color_continuous_scale': 'Blues'},         'prices': {'color_continuous_scale': 'Portland', 'color_continuous_midpoint': 0},     },     subplots_horizontal_spacing=0.05  # Adjust spacing between columns )  fig = generator.get_figure(df_mixed_scenarios, title='Variable-specific Color Scales')  renderer.show_plotly(fig) In\u00a0[14]: Copied! <pre># Multiple x-axis time aggregations with custom color scales\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis=['date', 'week', 'year_month'],      # Multiple time aggregations\n    color_continuous_scale='viridis',\n    facet_col='x_axis',                         # Aggregations as columns\n    facet_row='variable',\n    facet_row_order=['solar', 'load', 'prices'],\n    per_facet_row_colorscale=True,\n    facet_row_color_settings={\n        'solar': {'color_continuous_scale': 'Reds', 'range_color': [0, 100]},\n        'load': {'color_continuous_scale': 'Blues'},\n        'prices': {'color_continuous_scale': 'Portland', 'color_continuous_midpoint': 0},\n    },\n    subplots_horizontal_spacing=0.05  # Adjust spacing between columns\n)\n\nfig = generator.get_figure(df_mixed, title='Multiple Time Aggregations')\n\nrenderer.show_plotly(fig)\n</pre> # Multiple x-axis time aggregations with custom color scales generator = TimeSeriesDashboardGenerator(     x_axis=['date', 'week', 'year_month'],      # Multiple time aggregations     color_continuous_scale='viridis',     facet_col='x_axis',                         # Aggregations as columns     facet_row='variable',     facet_row_order=['solar', 'load', 'prices'],     per_facet_row_colorscale=True,     facet_row_color_settings={         'solar': {'color_continuous_scale': 'Reds', 'range_color': [0, 100]},         'load': {'color_continuous_scale': 'Blues'},         'prices': {'color_continuous_scale': 'Portland', 'color_continuous_midpoint': 0},     },     subplots_horizontal_spacing=0.05  # Adjust spacing between columns )  fig = generator.get_figure(df_mixed, title='Multiple Time Aggregations')  renderer.show_plotly(fig) In\u00a0[15]: Copied! <pre># Multiple statistical aggregations\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='year_month',                        # Aggregated per year and month\n    facet_col='groupby_aggregation',            # Different stats as columns\n    groupby_aggregation=['min', 'mean', 'max'], # Stats to calculate\n    color_continuous_scale='viridis',\n    facet_row='variable',\n    facet_row_order=['solar', 'onwind', 'offwind'],\n    subplots_horizontal_spacing=0.05  # Adjust spacing between columns\n)\n\nfig = generator.get_figure(df_res, title='Min/Mean/Max Aggregations by Month')\n\nrenderer.show_plotly(fig)\n</pre> # Multiple statistical aggregations generator = TimeSeriesDashboardGenerator(     x_axis='year_month',                        # Aggregated per year and month     facet_col='groupby_aggregation',            # Different stats as columns     groupby_aggregation=['min', 'mean', 'max'], # Stats to calculate     color_continuous_scale='viridis',     facet_row='variable',     facet_row_order=['solar', 'onwind', 'offwind'],     subplots_horizontal_spacing=0.05  # Adjust spacing between columns )  fig = generator.get_figure(df_res, title='Min/Mean/Max Aggregations by Month')  renderer.show_plotly(fig) In\u00a0[16]: Copied! <pre># Custom color scales per statistic type\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='month',\n    facet_col='groupby_aggregation',\n    groupby_aggregation=['min', 'mean', 'max'],\n    facet_row='variable',\n    facet_row_order=['solar', 'onwind', 'offwind'],\n    per_facet_col_colorscale=True,  # Different color scale per column\n    facet_col_color_settings={  # Color scales for columns\n        'min': {'color_continuous_scale': 'Blues', 'range_color': [0, 10]},\n        'mean': {'color_continuous_scale': 'Greens', 'range_color': [0, 70]},\n        'max': {'color_continuous_scale': 'Reds', 'range_color': [0, 100]}\n    },\n    subplots_horizontal_spacing=0.05  # Adjust spacing between columns\n)\n\nfig = generator.get_figure(df_res, title='Custom Colors by Statistic Type')\n\nrenderer.show_plotly(fig)\n</pre> # Custom color scales per statistic type generator = TimeSeriesDashboardGenerator(     x_axis='month',     facet_col='groupby_aggregation',     groupby_aggregation=['min', 'mean', 'max'],     facet_row='variable',     facet_row_order=['solar', 'onwind', 'offwind'],     per_facet_col_colorscale=True,  # Different color scale per column     facet_col_color_settings={  # Color scales for columns         'min': {'color_continuous_scale': 'Blues', 'range_color': [0, 10]},         'mean': {'color_continuous_scale': 'Greens', 'range_color': [0, 70]},         'max': {'color_continuous_scale': 'Reds', 'range_color': [0, 100]}     },     subplots_horizontal_spacing=0.05  # Adjust spacing between columns )  fig = generator.get_figure(df_res, title='Custom Colors by Statistic Type')  renderer.show_plotly(fig)"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/#mescal-301-time-series-dashboard","title":"MESCAL 301: Time Series Dashboard\u00b6","text":"<p>This notebook demonstrates the TimeSeriesDashboard module for effective visualization of complex time-series data across multiple dimensions, such as multi-scenario and multi-object time-series DataFrames.</p> <p>The TimeSeriesDashboard module can be used as a standalone module and does not require the use of any further MESCAL modules, so it can be used outside of the KPI framework, as well as outside of the StudyManager and Dataset framework. You can use this module with any time-series that you have in a pd DataFrame format with a DatetimeIndex. In fact, for the examples, we will use an example time-series from one of TU-Berlins public lecture documents.</p> <p>Key features of TimeSeriesDashboard:</p> <ul> <li>Heatmap visualization of time-series patterns</li> <li>Multi-faceted display for comparing scenarios and variables, following the plotly-express framework of facet_row and facet_col</li> <li>Flexible time-horizon aggregation (daily, weekly, monthly) combined with flexible statistical aggregations (e.g. mean, min, max, ...)</li> <li>Statistical summaries alongside visualizations, fully customizable</li> <li>Customizable color scales, optionally even per facet category</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mescal and all requirements are installed.</p>"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/#loading-example-data","title":"Loading Example Data\u00b6","text":"<p>We'll work with renewable energy generation and market data from TU-Berlin for our examples.</p>"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/#basic-timeseriesdashboard-visualization","title":"Basic TimeSeriesDashboard Visualization\u00b6","text":"<p>Let's visualize hourly patterns throughout different time periods, with variables organized as facet rows.</p>"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/#custom-statistics-in-stats-column","title":"Custom Statistics in stats column\u00b6","text":"<p>You can fully customize the statistics shown in the stats column on the dashbaord. The Config has a default set and a library of pre-implemented statistical aggregations (feel free to check out). Custom aggregations can be implemented as needed, you just need to provide a callable for a pandas series that returns the KPI.</p>"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/#facet-column-layouts","title":"Facet Column Layouts\u00b6","text":"<p>Instead of rows, we can also organize variables as columns, giving full control over the layout.</p>"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/#multi-dimensional-facets-scenarios-variables","title":"Multi-dimensional Facets (Scenarios \u00d7 Variables)\u00b6","text":"<p>Visualize multiple scenarios and variables in a grid layout.</p>"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/#custom-color-scales-per-variable","title":"Custom Color Scales Per Variable\u00b6","text":"<p>In many cases the different variables (or different objects / comparisons / scenarios) will have different orders-of-magnitude, making it useful to have a different color scale for the different categories.</p> <p>For this, just set <code>per_facet_row_colorscale</code> (or col) to True, and set the <code>facet_row_color_settings</code> (or col) accordingly. See an example below.</p>"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/#multiple-time-aggregations","title":"Multiple Time Aggregations\u00b6","text":"<p>For the x_axis, you can also set a list of time aggregation methods, and then show them side-by-side (or row-by-row in case of facet_row, respectively).</p>"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/#statistical-aggregations","title":"Statistical Aggregations\u00b6","text":"<p>The same principle of setting multiple x_axis aggregations and comparing them can also be applied to Compare different statistical aggregations (min, mean, max) across each time-horizon.</p>"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/#custom-colors-per-statistical-aggregation","title":"Custom Colors Per Statistical Aggregation\u00b6","text":"<p>The principle of custom color settings can, of course, also be applied per facet_col and in this case, for different statistical aggregations. Let's have a look.</p>"},{"location":"mescal-study-01/notebooks/mescal_301_time_series_dashboard/#conclusion","title":"Conclusion\u00b6","text":"<p>The TimeSeriesDashboard is a powerful tool for visualizing time-series data across multiple dimensions. Key benefits include:</p> <ul> <li>Revealing hourly patterns across days, weeks, or months</li> <li>Easy comparison of multiple scenarios, variables, objects, ...</li> <li>Easy detection of structural errors in your data</li> <li>KPI summaries alongside visualizations</li> <li>Highly customizable with flexible KPI summaries, categorical faceting, and color scales</li> </ul> <p>Keep in mind, the TimeSeriesDashboard can be used as a standalone module, as long as you have your data in a pandas DataFrame format with a DatetimeIndex.</p> <p>This module becomes especially powerful once you have your multi-scenario study loaded into a StudyManager, because the StudyManager gives you efficient access to the multi-scenario DataFrames in proper MultiIndex format.</p> <p>Try for yourself! :)</p>"},{"location":"mescal-study-01/notebooks/mescal_302_segmented_colormap/","title":"MESCAL 302: Segmented Colormap","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git\n        %cd mescal-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mescal -U\n        !pip install git+https://github.com/helgeesch/mescal-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git         %cd mescal-vanilla-studies/          !pip install git+https://github.com/helgeesch/mescal -U         !pip install git+https://github.com/helgeesch/mescal-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>Running locally, let's continue.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mescal-vanilla-studies')\n    os.chdir('/content/mescal-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mescal import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mescal-vanilla-studies')     os.chdir('/content/mescal-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mescal import StudyManager except ImportError:     raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\")  <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport numpy as np\nimport folium\nimport plotly.graph_objects as go\n\nfrom mescal.visualizations.value_mapping_system import SegmentedContinuousColorscale\nfrom mescal.visualizations.folium_legend_system import ContinuousColorscaleLegend\n\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\n\nconfigure_clean_output_for_jupyter_notebook()\nrenderer = ConditionalRenderer()\n</pre> import os import numpy as np import folium import plotly.graph_objects as go  from mescal.visualizations.value_mapping_system import SegmentedContinuousColorscale from mescal.visualizations.folium_legend_system import ContinuousColorscaleLegend  from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer   configure_clean_output_for_jupyter_notebook() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Define our market price colormap segments\nprice_segments = {\n    (-500, -150): ['#000080'],  # Navy for extreme negative prices\n    (-150, 0): ['#0000FF', '#87CEFA'],  # Blue to light blue for negative prices\n    (0, 150): ['#00C300', '#FFFB00'],  # Light green to yellow-green for low positive prices\n    (150, 500): ['#FFFB00', '#FF9300'],  # Yellow-green to orange for high positive prices\n    (500, 10000): ['#FF0000']  # Red for extreme positive prices\n}\n\n# Create a colormap without folium legend functionality \nprice_colorscale = SegmentedContinuousColorscale(price_segments)\n\n# Create a legend for Folium maps\nprice_legend = ContinuousColorscaleLegend(\n    mapping=price_colorscale,\n    title=\"Market Price (\u20ac/MWh)\",\n    width=350,\n    padding=20,\n    background_color=\"#FFFFFF\",\n    n_ticks_per_segment=2\n)\n\n# Show the legend in a Folium map\nm = folium.Map(location=[50.5, 10.5], zoom_start=6, tiles='CartoDB Positron')\nprice_legend.add_to(m)\n\n\nrenderer.show_folium(m)\n</pre> # Define our market price colormap segments price_segments = {     (-500, -150): ['#000080'],  # Navy for extreme negative prices     (-150, 0): ['#0000FF', '#87CEFA'],  # Blue to light blue for negative prices     (0, 150): ['#00C300', '#FFFB00'],  # Light green to yellow-green for low positive prices     (150, 500): ['#FFFB00', '#FF9300'],  # Yellow-green to orange for high positive prices     (500, 10000): ['#FF0000']  # Red for extreme positive prices }  # Create a colormap without folium legend functionality  price_colorscale = SegmentedContinuousColorscale(price_segments)  # Create a legend for Folium maps price_legend = ContinuousColorscaleLegend(     mapping=price_colorscale,     title=\"Market Price (\u20ac/MWh)\",     width=350,     padding=20,     background_color=\"#FFFFFF\",     n_ticks_per_segment=2 )  # Show the legend in a Folium map m = folium.Map(location=[50.5, 10.5], zoom_start=6, tiles='CartoDB Positron') price_legend.add_to(m)   renderer.show_folium(m) <p>This legend will be used in the next notebook (mescal_303_geospatial_visualization.ipynb) for our geospatial visualizations of market prices.</p> In\u00a0[5]: Copied! <pre># Get the colorscale for Plotly\nplotly_colorscale = price_colorscale.to_normalized_colorscale(num_reference_points_per_segment=5)\n\n# Create some sample data with a range of price values\nnp.random.seed(42)\nhours = 24\ndays = 7\nz = np.random.normal(loc=100, scale=150, size=(days, hours))\nz[2, 8:12] = -200  # Add some extreme negative values\nz[5, 15:20] = 700  # Add some extreme positive values\n\n# Create a heatmap with our custom colorscale\nfig = go.Figure(data=go.Heatmap(\n    z=z,\n    x=[f\"{h:02d}:00\" for h in range(hours)],\n    y=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"],\n    colorscale=plotly_colorscale,\n    zmin=-500,\n    zmax=10000,\n))\n\nfig.update_layout(\n    title=\"Example: Market Prices with Segmented Colormap\",\n    xaxis_title=\"Hour of Day\",\n    yaxis_title=\"Day of Week\",\n)\n\nrenderer.show_plotly(fig)\n</pre> # Get the colorscale for Plotly plotly_colorscale = price_colorscale.to_normalized_colorscale(num_reference_points_per_segment=5)  # Create some sample data with a range of price values np.random.seed(42) hours = 24 days = 7 z = np.random.normal(loc=100, scale=150, size=(days, hours)) z[2, 8:12] = -200  # Add some extreme negative values z[5, 15:20] = 700  # Add some extreme positive values  # Create a heatmap with our custom colorscale fig = go.Figure(data=go.Heatmap(     z=z,     x=[f\"{h:02d}:00\" for h in range(hours)],     y=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"],     colorscale=plotly_colorscale,     zmin=-500,     zmax=10000, ))  fig.update_layout(     title=\"Example: Market Prices with Segmented Colormap\",     xaxis_title=\"Hour of Day\",     yaxis_title=\"Day of Week\", )  renderer.show_plotly(fig)"},{"location":"mescal-study-01/notebooks/mescal_302_segmented_colormap/#mescal-302-segmented-colormap","title":"MESCAL 302: Segmented Colormap\u00b6","text":"<p>A useful module to merge multiple linear segments into a single cohesive colormap.</p>"},{"location":"mescal-study-01/notebooks/mescal_302_segmented_colormap/#introduction","title":"Introduction\u00b6","text":"<p>Visualizing data with complex color scales can be challenging. Some datasets have natural break points where you want different color behavior:</p> <ul> <li>Extreme negative values in one color range</li> <li>Near-zero values with fine gradients</li> <li>Positive values in another color range</li> <li>Extreme positive values with distinct colors</li> </ul> <p>Traditional linear colormaps don't handle these multi-segment needs well. The <code>SegmentedColorMap</code> and its companion <code>SegmentedColorMapLegend</code> solve this problem by allowing you to define custom segments with distinct color behaviors.</p>"},{"location":"mescal-study-01/notebooks/mescal_302_segmented_colormap/#key-features","title":"Key Features\u00b6","text":"<ul> <li>Define different color gradients for different value ranges</li> <li>Seamless transitions between segments</li> <li>Support for both continuous gradients and discrete colors</li> <li>Integration with both Folium (via legend) and Plotly (via normalized colorscale)</li> <li>Configurable ticks, labels, and visual styling</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_302_segmented_colormap/#creating-a-market-price-colormap","title":"Creating a Market Price Colormap\u00b6","text":"<p>Let's create a colormap for electricity market prices (\u20ac/MWh) with these segments:</p> <ul> <li>Extreme negative prices: -500 to -150 \u20ac/MWh (navy blue)</li> <li>Negative prices: -150 to 0 \u20ac/MWh (gradient from blue to light blue)</li> <li>Low positive prices: 0 to 150 \u20ac/MWh (gradient from light green to yellow-green)</li> <li>High positive prices: 150 to 500 \u20ac/MWh (gradient from yellow-green to orange)</li> <li>Extreme positive prices: 500 to 10000 \u20ac/MWh (red)</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_302_segmented_colormap/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mescal and all requirements are installed.</p>"},{"location":"mescal-study-01/notebooks/mescal_302_segmented_colormap/#using-the-colormap-with-plotly","title":"Using the Colormap with Plotly\u00b6","text":"<p>The <code>to_normalized_colorscale()</code> method converts our segmented colormap to a format compatible with Plotly:</p>"},{"location":"mescal-study-01/notebooks/mescal_302_segmented_colormap/#benefit-of-segmented-colormaps","title":"Benefit of Segmented Colormaps\u00b6","text":"<p>With our segmented colormap, we can:</p> <ol> <li>Define a color scale split in custom segments</li> <li>Assign distinct colors to extreme values</li> <li>Create fine color gradients for the most common value ranges</li> <li>Use appropriate colors for different segments (e.g., red for extreme positive prices)</li> <li>Maintain consistent coloring across different visualizations</li> </ol> <p>This approach is particularly useful for electricity market data, where extreme prices are significant but rare, and the common price range needs detailed color differentiation.</p>"},{"location":"mescal-study-01/notebooks/mescal_302_segmented_colormap/#conclusion","title":"Conclusion\u00b6","text":"<p>The <code>SegmentedColorMap</code> module provides a powerful tool for creating color scales that match the natural segments in your data. When combined with folium and plotly, it enables consistent, informative visualizations across different platforms and chart types.</p> <p>In the next notebook (mescal_303), we'll apply this colormap to geospatial visualizations of electricity market data.</p>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/","title":"MESCAL 401: Folium Model Data Visualization - Areas, Points, and Lines","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git\n        %cd mescal-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mescal -U\n        !pip install git+https://github.com/helgeesch/mescal-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git         %cd mescal-vanilla-studies/          !pip install git+https://github.com/helgeesch/mescal -U         !pip install git+https://github.com/helgeesch/mescal-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mescal-vanilla-studies')\n    os.chdir('/content/mescal-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mescal import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let's go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mescal-vanilla-studies')     os.chdir('/content/mescal-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mescal import StudyManager except ImportError:     raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let's go!\") <pre>\u2705 Environment ready. Let's go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport folium\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\n\nfrom mescal import StudyManager\nfrom mescal_pypsa import PyPSADataset\nfrom mescal.visualizations.folium_viz_system import (\n    PropertyMapper, \n    AreaGenerator, AreaFeatureResolver,\n    CircleMarkerGenerator, CircleMarkerFeatureResolver,\n    LineGenerator, LineFeatureResolver,\n    TextOverlayGenerator, TextOverlayFeatureResolver\n)\nfrom mescal.utils.folium_utils.background_color import set_background_color_of_map\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\n# Register study-specific interpreters\nfrom studies.study_01_intro_to_mescal.src.study_specific_model_interpreters import (\n    ControlAreaModelInterpreter, \n    ScigridDEBusModelInterpreter\n)\n\nPyPSADataset.register_interpreter(ControlAreaModelInterpreter)\nPyPSADataset.register_interpreter(ScigridDEBusModelInterpreter)\n\nconfigure_clean_output_for_jupyter_notebook()\nrenderer = ConditionalRenderer()\n</pre> import os import folium import geopandas as gpd import pandas as pd import numpy as np  from mescal import StudyManager from mescal_pypsa import PyPSADataset from mescal.visualizations.folium_viz_system import (     PropertyMapper,      AreaGenerator, AreaFeatureResolver,     CircleMarkerGenerator, CircleMarkerFeatureResolver,     LineGenerator, LineFeatureResolver,     TextOverlayGenerator, TextOverlayFeatureResolver ) from mescal.utils.folium_utils.background_color import set_background_color_of_map from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  # Register study-specific interpreters from studies.study_01_intro_to_mescal.src.study_specific_model_interpreters import (     ControlAreaModelInterpreter,      ScigridDEBusModelInterpreter )  PyPSADataset.register_interpreter(ControlAreaModelInterpreter) PyPSADataset.register_interpreter(ScigridDEBusModelInterpreter)  configure_clean_output_for_jupyter_notebook() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Load the StudyManager with PyPSA Scigrid-DE network data\nfrom studies.study_01_intro_to_mescal.scripts.setup_study_manager import get_scigrid_de_study_manager\nstudy = get_scigrid_de_study_manager()\n\n# Get the base dataset for our visualizations\nds_base = study.scen.get_dataset('base')\n\nprint(\"Available data flags:\")\nfor flag in sorted(ds_base.accepted_flags):\n    if any(geo_term in flag for geo_term in ['control_areas', 'buses', 'lines']):\n        print(f\"  \ud83d\udccd {flag}\")\n    else:\n        print(f\"  \ud83d\udcca {flag}\")\n</pre> # Load the StudyManager with PyPSA Scigrid-DE network data from studies.study_01_intro_to_mescal.scripts.setup_study_manager import get_scigrid_de_study_manager study = get_scigrid_de_study_manager()  # Get the base dataset for our visualizations ds_base = study.scen.get_dataset('base')  print(\"Available data flags:\") for flag in sorted(ds_base.accepted_flags):     if any(geo_term in flag for geo_term in ['control_areas', 'buses', 'lines']):         print(f\"  \ud83d\udccd {flag}\")     else:         print(f\"  \ud83d\udcca {flag}\") <pre>Available data flags:\n  \ud83d\udccd buses\n  \ud83d\udccd buses_t.marginal_price\n  \ud83d\udccd buses_t.p\n  \ud83d\udccd buses_t.q\n  \ud83d\udccd buses_t.v_ang\n  \ud83d\udccd buses_t.v_mag_pu\n  \ud83d\udccd buses_t.v_mag_pu_set\n  \ud83d\udcca carriers\n  \ud83d\udccd control_areas\n  \ud83d\udccd control_areas_t.vol_weighted_marginal_price\n  \ud83d\udcca generators\n  \ud83d\udcca generators_t.efficiency\n  \ud83d\udcca generators_t.marginal_cost\n  \ud83d\udcca generators_t.marginal_cost_quadratic\n  \ud83d\udcca generators_t.mu_lower\n  \ud83d\udcca generators_t.mu_p_set\n  \ud83d\udcca generators_t.mu_ramp_limit_down\n  \ud83d\udcca generators_t.mu_ramp_limit_up\n  \ud83d\udcca generators_t.mu_upper\n  \ud83d\udcca generators_t.p\n  \ud83d\udcca generators_t.p_max_pu\n  \ud83d\udcca generators_t.p_min_pu\n  \ud83d\udcca generators_t.p_set\n  \ud83d\udcca generators_t.q\n  \ud83d\udcca generators_t.q_set\n  \ud83d\udcca generators_t.ramp_limit_down\n  \ud83d\udcca generators_t.ramp_limit_up\n  \ud83d\udcca generators_t.stand_by_cost\n  \ud83d\udcca generators_t.status\n  \ud83d\udcca global_constraints\n  \ud83d\udcca line_types\n  \ud83d\udccd lines\n  \ud83d\udccd lines_t.mu_lower\n  \ud83d\udccd lines_t.mu_upper\n  \ud83d\udccd lines_t.p0\n  \ud83d\udccd lines_t.p1\n  \ud83d\udccd lines_t.q0\n  \ud83d\udccd lines_t.q1\n  \ud83d\udccd lines_t.s_max_pu\n  \ud83d\udcca links\n  \ud83d\udcca links_t.efficiency\n  \ud83d\udcca links_t.marginal_cost\n  \ud83d\udcca links_t.marginal_cost_quadratic\n  \ud83d\udcca links_t.mu_lower\n  \ud83d\udcca links_t.mu_p_set\n  \ud83d\udcca links_t.mu_ramp_limit_down\n  \ud83d\udcca links_t.mu_ramp_limit_up\n  \ud83d\udcca links_t.mu_upper\n  \ud83d\udcca links_t.p0\n  \ud83d\udcca links_t.p1\n  \ud83d\udcca links_t.p_max_pu\n  \ud83d\udcca links_t.p_min_pu\n  \ud83d\udcca links_t.p_set\n  \ud83d\udcca links_t.ramp_limit_down\n  \ud83d\udcca links_t.ramp_limit_up\n  \ud83d\udcca links_t.stand_by_cost\n  \ud83d\udcca links_t.status\n  \ud83d\udcca loads\n  \ud83d\udcca loads_t.p\n  \ud83d\udcca loads_t.p_set\n  \ud83d\udcca loads_t.q\n  \ud83d\udcca loads_t.q_set\n  \ud83d\udcca networks\n  \ud83d\udcca objective\n  \ud83d\udcca shapes\n  \ud83d\udcca shunt_impedances\n  \ud83d\udcca shunt_impedances_t.p\n  \ud83d\udcca shunt_impedances_t.q\n  \ud83d\udcca storage_units\n  \ud83d\udcca storage_units_t.efficiency_dispatch\n  \ud83d\udcca storage_units_t.efficiency_store\n  \ud83d\udcca storage_units_t.inflow\n  \ud83d\udcca storage_units_t.marginal_cost\n  \ud83d\udcca storage_units_t.marginal_cost_quadratic\n  \ud83d\udcca storage_units_t.marginal_cost_storage\n  \ud83d\udcca storage_units_t.mu_energy_balance\n  \ud83d\udcca storage_units_t.mu_lower\n  \ud83d\udcca storage_units_t.mu_state_of_charge_set\n  \ud83d\udcca storage_units_t.mu_upper\n  \ud83d\udcca storage_units_t.p\n  \ud83d\udcca storage_units_t.p_dispatch\n  \ud83d\udcca storage_units_t.p_max_pu\n  \ud83d\udcca storage_units_t.p_min_pu\n  \ud83d\udcca storage_units_t.p_set\n  \ud83d\udcca storage_units_t.p_store\n  \ud83d\udcca storage_units_t.q\n  \ud83d\udcca storage_units_t.q_set\n  \ud83d\udcca storage_units_t.spill\n  \ud83d\udcca storage_units_t.spill_cost\n  \ud83d\udcca storage_units_t.standing_loss\n  \ud83d\udcca storage_units_t.state_of_charge\n  \ud83d\udcca storage_units_t.state_of_charge_set\n  \ud83d\udcca stores\n  \ud83d\udcca stores_t.e\n  \ud83d\udcca stores_t.e_max_pu\n  \ud83d\udcca stores_t.e_min_pu\n  \ud83d\udcca stores_t.marginal_cost\n  \ud83d\udcca stores_t.marginal_cost_quadratic\n  \ud83d\udcca stores_t.marginal_cost_storage\n  \ud83d\udcca stores_t.mu_energy_balance\n  \ud83d\udcca stores_t.mu_lower\n  \ud83d\udcca stores_t.mu_upper\n  \ud83d\udcca stores_t.p\n  \ud83d\udcca stores_t.p_set\n  \ud83d\udcca stores_t.q\n  \ud83d\udcca stores_t.q_set\n  \ud83d\udcca stores_t.standing_loss\n  \ud83d\udcca sub_networks\n  \ud83d\udcca transformer_types\n  \ud83d\udcca transformers\n  \ud83d\udcca transformers_t.mu_lower\n  \ud83d\udcca transformers_t.mu_upper\n  \ud83d\udcca transformers_t.p0\n  \ud83d\udcca transformers_t.p1\n  \ud83d\udcca transformers_t.q0\n  \ud83d\udcca transformers_t.q1\n  \ud83d\udcca transformers_t.s_max_pu\n</pre> In\u00a0[5]: Copied! <pre># Load control areas directly from GeoJSON file\ngeojson_path = 'studies/study_01_intro_to_mescal/data/DE_control_areas.geojson'\ncontrol_areas_gdf = gpd.read_file(geojson_path)\ncontrol_areas_gdf = control_areas_gdf.rename(columns={'tso': 'control_area'})\ncontrol_areas_gdf = control_areas_gdf.set_index('control_area')\ncontrol_areas_gdf = control_areas_gdf.set_crs(epsg=4326)\n\nprint(\"Control Areas from GeoJSON:\")\nprint(control_areas_gdf.head())\nprint(f\"\\nGeometry types: {control_areas_gdf.geometry.geom_type.unique()}\")\nprint(f\"CRS: {control_areas_gdf.crs}\")\n</pre> # Load control areas directly from GeoJSON file geojson_path = 'studies/study_01_intro_to_mescal/data/DE_control_areas.geojson' control_areas_gdf = gpd.read_file(geojson_path) control_areas_gdf = control_areas_gdf.rename(columns={'tso': 'control_area'}) control_areas_gdf = control_areas_gdf.set_index('control_area') control_areas_gdf = control_areas_gdf.set_crs(epsg=4326)  print(\"Control Areas from GeoJSON:\") print(control_areas_gdf.head()) print(f\"\\nGeometry types: {control_areas_gdf.geometry.geom_type.unique()}\") print(f\"CRS: {control_areas_gdf.crs}\") <pre>Control Areas from GeoJSON:\n                                                       geometry\ncontrol_area                                                   \n50Hertz       MULTIPOLYGON (((11.2639 50.48074, 11.2474 50.4...\nTenneTDE      MULTIPOLYGON (((8.45592 50.41449, 8.38936 50.4...\nTransnetBW    MULTIPOLYGON (((8.62585 47.64422, 8.59569 47.6...\nAmprion       MULTIPOLYGON (((7.09992 49.15556, 7.0803 49.14...\n\nGeometry types: ['MultiPolygon']\nCRS: EPSG:4326\n</pre> In\u00a0[6]: Copied! <pre># Create a basic map with control areas\nm1 = folium.Map(\n    location=[51, 11],  # Center of Germany\n    zoom_start=6,\n    tiles=None\n)\nm1 = set_background_color_of_map(m1, color='#f8f9fa')\n\n# Create a simple area generator with static styling\nbasic_area_generator = AreaGenerator(\n    AreaFeatureResolver(\n        fill_color='#e3f2fd',\n        fill_opacity=0.6,\n        border_color='#1976d2',\n        border_width=2,\n        tooltip=True  # Auto-generate tooltips from data\n    )\n)\n\n# Create a feature group and add areas\nfeature_group = folium.FeatureGroup(name='Control Areas (Direct GeoJSON)')\nbasic_area_generator.generate_objects_for_model_df(control_areas_gdf, feature_group)\nfeature_group.add_to(m1)\n\nfolium.LayerControl().add_to(m1)\nrenderer.show_folium(m1)\n</pre> # Create a basic map with control areas m1 = folium.Map(     location=[51, 11],  # Center of Germany     zoom_start=6,     tiles=None ) m1 = set_background_color_of_map(m1, color='#f8f9fa')  # Create a simple area generator with static styling basic_area_generator = AreaGenerator(     AreaFeatureResolver(         fill_color='#e3f2fd',         fill_opacity=0.6,         border_color='#1976d2',         border_width=2,         tooltip=True  # Auto-generate tooltips from data     ) )  # Create a feature group and add areas feature_group = folium.FeatureGroup(name='Control Areas (Direct GeoJSON)') basic_area_generator.generate_objects_for_model_df(control_areas_gdf, feature_group) feature_group.add_to(m1)  folium.LayerControl().add_to(m1) renderer.show_folium(m1) In\u00a0[7]: Copied! <pre># Fetch control areas using MESCAL dataset (via ControlAreaModelInterpreter)\ncontrol_areas_mescal = ds_base.fetch('control_areas')\n\nprint(\"Control Areas from MESCAL Dataset:\")\nprint(control_areas_mescal.head())\nprint(f\"\\nSame data? {control_areas_mescal.equals(control_areas_gdf)}\")\n</pre> # Fetch control areas using MESCAL dataset (via ControlAreaModelInterpreter) control_areas_mescal = ds_base.fetch('control_areas')  print(\"Control Areas from MESCAL Dataset:\") print(control_areas_mescal.head()) print(f\"\\nSame data? {control_areas_mescal.equals(control_areas_gdf)}\") <pre>Control Areas from MESCAL Dataset:\n                                                       geometry\ncontrol_area                                                   \n50Hertz       MULTIPOLYGON (((11.2639 50.48074, 11.2474 50.4...\nTenneTDE      MULTIPOLYGON (((8.45592 50.41449, 8.38936 50.4...\nTransnetBW    MULTIPOLYGON (((8.62585 47.64422, 8.59569 47.6...\nAmprion       MULTIPOLYGON (((7.09992 49.15556, 7.0803 49.14...\n\nSame data? True\n</pre> In\u00a0[8]: Copied! <pre>from mescal.visualizations.value_mapping_system import DiscreteColorMapping\n\n# Create a more sophisticated visualization with color mapping\nm2 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm2 = set_background_color_of_map(m2, color='#ffffff')\n\n# Define explicit colors for control areas\ncontrol_area_colors = {\n    'Amprion': '#FF6B6B',      # Red\n    'TenneTDE': '#4ECDC4',     # Teal  \n    'TransnetBW': '#96CEB4',   # Blue\n    # '50Hertz': '#45B7D1'     # outcommented to showcase the \"auto_assign\" feature of the DiscreteColorMapping class\n}\n\n#\ncontrol_areas_mescal['fill_opacity'] = [1.0, 0.5, 1.0, 1.0]\n\ncolor_mapping = DiscreteColorMapping(control_area_colors, mode=\"auto_assign\")\n\n# Create area generator with dynamic coloring\ncolored_area_generator = AreaGenerator(\n    AreaFeatureResolver(\n        fill_color=PropertyMapper.from_item_attr('control_area', color_mapping),\n        fill_opacity=0.7,\n        border_color='#2c3e50',\n        border_width=2,\n        highlight_border_width=3,\n        tooltip=True\n    )\n)\n\n# Add colored areas\ncolored_feature_group = folium.FeatureGroup(name='Control Areas (Colored)')\ncolored_area_generator.generate_objects_for_model_df(control_areas_mescal, colored_feature_group)\ncolored_feature_group.add_to(m2)\n\nfolium.LayerControl().add_to(m2)\nrenderer.show_folium(m2)\n</pre> from mescal.visualizations.value_mapping_system import DiscreteColorMapping  # Create a more sophisticated visualization with color mapping m2 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m2 = set_background_color_of_map(m2, color='#ffffff')  # Define explicit colors for control areas control_area_colors = {     'Amprion': '#FF6B6B',      # Red     'TenneTDE': '#4ECDC4',     # Teal       'TransnetBW': '#96CEB4',   # Blue     # '50Hertz': '#45B7D1'     # outcommented to showcase the \"auto_assign\" feature of the DiscreteColorMapping class }  # control_areas_mescal['fill_opacity'] = [1.0, 0.5, 1.0, 1.0]  color_mapping = DiscreteColorMapping(control_area_colors, mode=\"auto_assign\")  # Create area generator with dynamic coloring colored_area_generator = AreaGenerator(     AreaFeatureResolver(         fill_color=PropertyMapper.from_item_attr('control_area', color_mapping),         fill_opacity=0.7,         border_color='#2c3e50',         border_width=2,         highlight_border_width=3,         tooltip=True     ) )  # Add colored areas colored_feature_group = folium.FeatureGroup(name='Control Areas (Colored)') colored_area_generator.generate_objects_for_model_df(control_areas_mescal, colored_feature_group) colored_feature_group.add_to(m2)  folium.LayerControl().add_to(m2) renderer.show_folium(m2) In\u00a0[9]: Copied! <pre># Fetch buses using MESCAL (includes control area assignment via ScigridDEBusModelInterpreter)\nbuses_gdf = ds_base.fetch('buses')\n\nprint(\"Bus data structure:\")\nprint(buses_gdf.head())\nprint(f\"\\nColumns: {list(buses_gdf.columns)}\")\nprint(f\"Geometry type: {buses_gdf.geometry.geom_type.unique()}\")\nprint(f\"Total buses: {len(buses_gdf)}\")\nprint(f\"Buses per control area:\\n{buses_gdf['control_area'].value_counts()}\")\n</pre> # Fetch buses using MESCAL (includes control area assignment via ScigridDEBusModelInterpreter) buses_gdf = ds_base.fetch('buses')  print(\"Bus data structure:\") print(buses_gdf.head()) print(f\"\\nColumns: {list(buses_gdf.columns)}\") print(f\"Geometry type: {buses_gdf.geometry.geom_type.unique()}\") print(f\"Total buses: {len(buses_gdf)}\") print(f\"Buses per control area:\\n{buses_gdf['control_area'].value_counts()}\") <pre>Bus data structure:\n     v_nom type      x  ...  ref                   location control_area\nBus                     ...                                             \n1    220.0        9.52  ...        POINT (9.52258 52.36041)     TenneTDE\n2    380.0        9.11  ...        POINT (9.11321 52.54385)     TenneTDE\n3    380.0        9.39  ...        POINT (9.38975 52.02631)     TenneTDE\n4    380.0        9.13  ...        POINT (9.12527 52.53826)     TenneTDE\n5    380.0       10.37  ...       POINT (10.36627 52.28465)     TenneTDE\n\n[5 rows x 21 columns]\n\nColumns: ['v_nom', 'type', 'x', 'y', 'carrier', 'unit', 'v_mag_pu_set', 'v_mag_pu_min', 'v_mag_pu_max', 'control', 'generator', 'sub_network', 'frequency', 'wkt_srid_4326', 'operator', 'osm_name', 'voltage', 'typ', 'ref', 'location', 'control_area']\nGeometry type: ['Point']\nTotal buses: 585\nBuses per control area:\ncontrol_area\nAmprion       241\nTenneTDE      176\n50Hertz        87\nTransnetBW     81\nName: count, dtype: int64\n</pre> In\u00a0[10]: Copied! <pre># Create map with buses as points\nm3 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm3 = set_background_color_of_map(m3, color='#f8f9fa')\n\n# Add control areas as background\nbg_areas = folium.FeatureGroup(name='Control Areas (Background)')\nbasic_area_generator = AreaGenerator(\n    AreaFeatureResolver(\n        fill_color=PropertyMapper.from_static_value('#e8e8e8'),\n        fill_opacity=0.3,\n        border_color=PropertyMapper.from_static_value('#666666'),\n        border_width=1\n    )\n)\nbasic_area_generator.generate_objects_for_model_df(control_areas_mescal, bg_areas)\nbg_areas.add_to(m3)\n\n# Function to get bus color based on voltage level\ndef get_bus_color_by_voltage(data_item):\n    \"\"\"Color buses by voltage level\"\"\"\n    v_nom = data_item.get_object_attribute('v_nom')\n    if v_nom &gt;= 380:\n        return '#d32f2f'  # Red for extra high voltage\n    elif v_nom &gt;= 220:\n        return '#f57c00'  # Orange for high voltage  \n    elif v_nom &gt;= 110:\n        return '#fbc02d'  # Yellow for medium voltage\n    else:\n        return '#388e3c'  # Green for low voltage\n\n# Function to get bus size based on voltage level\ndef get_bus_size_by_voltage(data_item):\n    \"\"\"Size buses by voltage level\"\"\"\n    v_nom = data_item.get_object_attribute('v_nom')\n    if v_nom &gt;= 380:\n        return 8\n    elif v_nom &gt;= 220:\n        return 6\n    elif v_nom &gt;= 110:\n        return 4\n    else:\n        return 2\n\n# Create circle marker generator for buses\nbus_generator = CircleMarkerGenerator(\n    CircleMarkerFeatureResolver(\n        radius=PropertyMapper(get_bus_size_by_voltage),\n        fill_color=PropertyMapper(get_bus_color_by_voltage),\n        color=PropertyMapper.from_static_value('#2c3e50'),\n        weight=1,\n        fill_opacity=0.8,\n        tooltip=True\n    )\n)\n\n# Add buses to map\nbus_feature_group = folium.FeatureGroup(name='Buses (by Voltage Level)')\nbus_generator.generate_objects_for_model_df(buses_gdf, bus_feature_group)\nbus_feature_group.add_to(m3)\n\n# Add a custom legend\nlegend_html = '''\n&lt;div style=\"position: fixed; \n            bottom: 50px; right: 50px; width: 200px; height: 160px;\n            background-color: white; border:2px solid grey; z-index:9999; \n            font-size:14px; padding: 10px\n            \"&gt;\n&lt;h4&gt;Bus Voltage Levels&lt;/h4&gt;\n&lt;p&gt;&lt;i class=\"fa fa-circle\" style=\"color:#d32f2f\"&gt;&lt;/i&gt; \u2265380 kV (Extra High)&lt;/p&gt;\n&lt;p&gt;&lt;i class=\"fa fa-circle\" style=\"color:#f57c00\"&gt;&lt;/i&gt; \u2265220 kV (High)&lt;/p&gt;\n&lt;p&gt;&lt;i class=\"fa fa-circle\" style=\"color:#fbc02d\"&gt;&lt;/i&gt; \u2265110 kV (Medium)&lt;/p&gt;\n&lt;p&gt;&lt;i class=\"fa fa-circle\" style=\"color:#388e3c\"&gt;&lt;/i&gt; &lt;110 kV (Low)&lt;/p&gt;\n&lt;/div&gt;\n'''\nm3.get_root().html.add_child(folium.Element(legend_html))\n\nfolium.LayerControl().add_to(m3)\nrenderer.show_folium(m3)\n</pre> # Create map with buses as points m3 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m3 = set_background_color_of_map(m3, color='#f8f9fa')  # Add control areas as background bg_areas = folium.FeatureGroup(name='Control Areas (Background)') basic_area_generator = AreaGenerator(     AreaFeatureResolver(         fill_color=PropertyMapper.from_static_value('#e8e8e8'),         fill_opacity=0.3,         border_color=PropertyMapper.from_static_value('#666666'),         border_width=1     ) ) basic_area_generator.generate_objects_for_model_df(control_areas_mescal, bg_areas) bg_areas.add_to(m3)  # Function to get bus color based on voltage level def get_bus_color_by_voltage(data_item):     \"\"\"Color buses by voltage level\"\"\"     v_nom = data_item.get_object_attribute('v_nom')     if v_nom &gt;= 380:         return '#d32f2f'  # Red for extra high voltage     elif v_nom &gt;= 220:         return '#f57c00'  # Orange for high voltage       elif v_nom &gt;= 110:         return '#fbc02d'  # Yellow for medium voltage     else:         return '#388e3c'  # Green for low voltage  # Function to get bus size based on voltage level def get_bus_size_by_voltage(data_item):     \"\"\"Size buses by voltage level\"\"\"     v_nom = data_item.get_object_attribute('v_nom')     if v_nom &gt;= 380:         return 8     elif v_nom &gt;= 220:         return 6     elif v_nom &gt;= 110:         return 4     else:         return 2  # Create circle marker generator for buses bus_generator = CircleMarkerGenerator(     CircleMarkerFeatureResolver(         radius=PropertyMapper(get_bus_size_by_voltage),         fill_color=PropertyMapper(get_bus_color_by_voltage),         color=PropertyMapper.from_static_value('#2c3e50'),         weight=1,         fill_opacity=0.8,         tooltip=True     ) )  # Add buses to map bus_feature_group = folium.FeatureGroup(name='Buses (by Voltage Level)') bus_generator.generate_objects_for_model_df(buses_gdf, bus_feature_group) bus_feature_group.add_to(m3)  # Add a custom legend legend_html = '''  Bus Voltage Levels <p> \u2265380 kV (Extra High)</p> <p> \u2265220 kV (High)</p> <p> \u2265110 kV (Medium)</p> <p> &lt;110 kV (Low)</p>  ''' m3.get_root().html.add_child(folium.Element(legend_html))  folium.LayerControl().add_to(m3) renderer.show_folium(m3) In\u00a0[11]: Copied! <pre># Fetch transmission lines\nlines_gdf = ds_base.fetch('lines')\n\nprint(\"Transmission lines data:\")\nprint(lines_gdf.head())\n</pre> # Fetch transmission lines lines_gdf = ds_base.fetch('lines')  print(\"Transmission lines data:\") print(lines_gdf.head()) <pre>Transmission lines data:\n         bus0     bus1                         type  ...  bus_control_area_combo_opposite  bus_type_combo_opposite                                           geometry\nLine                                                 ...                                                                                                             \n1           1  2_220kV  Al/St 240/40 2-bundle 220.0  ...              TenneTDE - TenneTDE                       -   LINESTRING (9.52257596986 52.3604090558, 9.113...\n2           3        4  Al/St 240/40 4-bundle 380.0  ...              TenneTDE - TenneTDE                       -   LINESTRING (9.38974509625 52.026313066, 9.1252...\n3     5_220kV        6  Al/St 240/40 2-bundle 220.0  ...              TenneTDE - TenneTDE                       -   LINESTRING (10.3662749375 52.2846467462, 9.918...\n4           7        5  Al/St 240/40 4-bundle 380.0  ...              TenneTDE - TenneTDE                       -   LINESTRING (9.91717971972 52.2781686139, 10.36...\n5           8        9  Al/St 240/40 4-bundle 380.0  ...              TenneTDE - TenneTDE                       -   LINESTRING (10.4149923382 53.412606883, 10.378...\n\n[5 rows x 155 columns]\n</pre> In\u00a0[12]: Copied! <pre># Create comprehensive network visualization\nm4 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm4 = set_background_color_of_map(m4, color='#ffffff')\n\n# Add control areas as background (lighter)\nbg_areas = folium.FeatureGroup(name='Control Areas')\narea_bg_generator = AreaGenerator(\n    AreaFeatureResolver(\n        fill_color=PropertyMapper.from_static_value('#f5f5f5'),\n        fill_opacity=0.4,\n        border_color=PropertyMapper.from_static_value('#bdbdbd'),\n        border_width=1.5\n    )\n)\narea_bg_generator.generate_objects_for_model_df(control_areas_mescal, bg_areas)\nbg_areas.add_to(m4)\n\n# Function to style lines by voltage\ndef get_line_color_by_voltage(data_item):\n    \"\"\"Color lines by voltage level\"\"\"\n    v_nom = data_item.get_object_attribute('v_nom')\n    if v_nom &gt;= 380:\n        return '#c62828'  # Dark red for 380+ kV\n    elif v_nom &gt;= 220:\n        return '#ef6c00'  # Orange for 220+ kV\n    else:\n        return '#2e7d32'  # Green for &lt;220 kV\n\ndef get_line_width_by_voltage(data_item):\n    \"\"\"Width lines by voltage level\"\"\"\n    v_nom = data_item.get_object_attribute('v_nom')\n    if v_nom &gt;= 380:\n        return 4\n    elif v_nom &gt;= 220:\n        return 3\n    else:\n        return 2\n\n# Create line generator\nline_generator = LineGenerator(\n    LineFeatureResolver(\n        color=PropertyMapper(get_line_color_by_voltage),\n        weight=PropertyMapper(get_line_width_by_voltage),\n        opacity=0.8,\n        tooltip=True\n    )\n)\n\n# Add transmission lines\nlines_feature_group = folium.FeatureGroup(name='Transmission Lines')\nline_generator.generate_objects_for_model_df(lines_gdf, lines_feature_group)\nlines_feature_group.add_to(m4)\n\n# Add key buses (high voltage only)\nhigh_voltage_buses = buses_gdf[buses_gdf['v_nom'] &gt;= 220]\nkey_bus_generator = CircleMarkerGenerator(\n    CircleMarkerFeatureResolver(\n        radius=PropertyMapper(lambda item: 5 if item.get_object_attribute('v_nom') &gt;= 380 else 3),\n        fill_color='#1a237e',\n        color='#ffffff',\n        weight=2,\n        fill_opacity=0.9,\n        tooltip=True\n    )\n)\n\nkey_buses_group = folium.FeatureGroup(name='Key Buses (\u2265220kV)')\nkey_bus_generator.generate_objects_for_model_df(high_voltage_buses, key_buses_group)\nkey_buses_group.add_to(m4)\n\n# Add network legend\nnetwork_legend_html = '''\n&lt;div style=\"position: fixed; \n            bottom: 50px; right: 50px; width: 220px; height: 180px;\n            background-color: white; border:2px solid grey; z-index:9999; \n            font-size:14px; padding: 10px\n            \"&gt;\n&lt;h4&gt;Network Elements&lt;/h4&gt;\n&lt;p&gt;&lt;span style=\"color:#c62828; font-weight:bold;\"&gt;\u2501\u2501\u2501&lt;/span&gt; \u2265380 kV Lines&lt;/p&gt;\n&lt;p&gt;&lt;span style=\"color:#ef6c00; font-weight:bold;\"&gt;\u2501\u2501\u2501&lt;/span&gt; \u2265220 kV Lines&lt;/p&gt;\n&lt;p&gt;&lt;span style=\"color:#2e7d32; font-weight:bold;\"&gt;\u2501\u2501\u2501&lt;/span&gt; &lt;220 kV Lines&lt;/p&gt;\n&lt;p&gt;&lt;i class=\"fa fa-circle\" style=\"color:#1a237e\"&gt;&lt;/i&gt; Key Buses (\u2265220kV)&lt;/p&gt;\n&lt;p style=\"color:#bdbdbd;\"&gt;\u25a2 Control Areas&lt;/p&gt;\n&lt;/div&gt;\n'''\nm4.get_root().html.add_child(folium.Element(network_legend_html))\n\nfolium.LayerControl().add_to(m4)\nrenderer.show_folium(m4)\n</pre> # Create comprehensive network visualization m4 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m4 = set_background_color_of_map(m4, color='#ffffff')  # Add control areas as background (lighter) bg_areas = folium.FeatureGroup(name='Control Areas') area_bg_generator = AreaGenerator(     AreaFeatureResolver(         fill_color=PropertyMapper.from_static_value('#f5f5f5'),         fill_opacity=0.4,         border_color=PropertyMapper.from_static_value('#bdbdbd'),         border_width=1.5     ) ) area_bg_generator.generate_objects_for_model_df(control_areas_mescal, bg_areas) bg_areas.add_to(m4)  # Function to style lines by voltage def get_line_color_by_voltage(data_item):     \"\"\"Color lines by voltage level\"\"\"     v_nom = data_item.get_object_attribute('v_nom')     if v_nom &gt;= 380:         return '#c62828'  # Dark red for 380+ kV     elif v_nom &gt;= 220:         return '#ef6c00'  # Orange for 220+ kV     else:         return '#2e7d32'  # Green for &lt;220 kV  def get_line_width_by_voltage(data_item):     \"\"\"Width lines by voltage level\"\"\"     v_nom = data_item.get_object_attribute('v_nom')     if v_nom &gt;= 380:         return 4     elif v_nom &gt;= 220:         return 3     else:         return 2  # Create line generator line_generator = LineGenerator(     LineFeatureResolver(         color=PropertyMapper(get_line_color_by_voltage),         weight=PropertyMapper(get_line_width_by_voltage),         opacity=0.8,         tooltip=True     ) )  # Add transmission lines lines_feature_group = folium.FeatureGroup(name='Transmission Lines') line_generator.generate_objects_for_model_df(lines_gdf, lines_feature_group) lines_feature_group.add_to(m4)  # Add key buses (high voltage only) high_voltage_buses = buses_gdf[buses_gdf['v_nom'] &gt;= 220] key_bus_generator = CircleMarkerGenerator(     CircleMarkerFeatureResolver(         radius=PropertyMapper(lambda item: 5 if item.get_object_attribute('v_nom') &gt;= 380 else 3),         fill_color='#1a237e',         color='#ffffff',         weight=2,         fill_opacity=0.9,         tooltip=True     ) )  key_buses_group = folium.FeatureGroup(name='Key Buses (\u2265220kV)') key_bus_generator.generate_objects_for_model_df(high_voltage_buses, key_buses_group) key_buses_group.add_to(m4)  # Add network legend network_legend_html = '''  Network Elements <p>\u2501\u2501\u2501 \u2265380 kV Lines</p> <p>\u2501\u2501\u2501 \u2265220 kV Lines</p> <p>\u2501\u2501\u2501 &lt;220 kV Lines</p> <p> Key Buses (\u2265220kV)</p> <p>\u25a2 Control Areas</p>  ''' m4.get_root().html.add_child(folium.Element(network_legend_html))  folium.LayerControl().add_to(m4) renderer.show_folium(m4) In\u00a0[13]: Copied! <pre># Create final comprehensive map with text overlays\nm5 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm5 = set_background_color_of_map(m5, color='#ffffff')\n\n# Add colored control areas\nareas_group = folium.FeatureGroup(name='Control Areas')\ncolored_area_generator.generate_objects_for_model_df(control_areas_mescal, areas_group)\nareas_group.add_to(m5)\n\n# Add control area labels using TextOverlayGenerator\ntext_generator = TextOverlayGenerator(\n    TextOverlayFeatureResolver(\n        text=PropertyMapper.from_item_attr('control_area'),  # Use area name\n        font_size='16pt',\n        font_weight='bold',\n        color='#2c3e50',\n        background_color='#ffffff',\n        border_color='#bdc3c7',\n        border_width=1,\n        padding=5\n    )\n)\n\nlabels_group = folium.FeatureGroup(name='Area Labels')\ntext_generator.generate_objects_for_model_df(control_areas_mescal, labels_group)\nlabels_group.add_to(m5)\n\n# Add transmission lines (extra high voltage only for cleaner view)\nehv_lines = lines_gdf[lines_gdf['v_nom'] &gt;= 380]\nehv_line_generator = LineGenerator(\n    LineFeatureResolver(\n        color='#c62828',\n        weight=3,\n        opacity=0.8,\n        tooltip=True\n    )\n)\n\nehv_lines_group = folium.FeatureGroup(name='Extra High Voltage Lines (\u2265380kV)')\nehv_line_generator.generate_objects_for_model_df(ehv_lines, ehv_lines_group)\nehv_lines_group.add_to(m5)\n\n# Add major bus nodes\nmajor_buses = buses_gdf[buses_gdf['v_nom'] &gt;= 380]\nmajor_bus_generator = CircleMarkerGenerator(\n    CircleMarkerFeatureResolver(\n        radius=6,\n        fill_color=PropertyMapper.from_static_value('#1a237e'),\n        color=PropertyMapper.from_static_value('#ffffff'),\n        weight=2,\n        fill_opacity=1.0,\n        tooltip=True\n    )\n)\n\nmajor_buses_group = folium.FeatureGroup(name='Major Bus Nodes (\u2265380kV)')\nmajor_bus_generator.generate_objects_for_model_df(major_buses, major_buses_group)\nmajor_buses_group.add_to(m5)\n\nfolium.LayerControl().add_to(m5)\nrenderer.show_folium(m5)\n</pre> # Create final comprehensive map with text overlays m5 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m5 = set_background_color_of_map(m5, color='#ffffff')  # Add colored control areas areas_group = folium.FeatureGroup(name='Control Areas') colored_area_generator.generate_objects_for_model_df(control_areas_mescal, areas_group) areas_group.add_to(m5)  # Add control area labels using TextOverlayGenerator text_generator = TextOverlayGenerator(     TextOverlayFeatureResolver(         text=PropertyMapper.from_item_attr('control_area'),  # Use area name         font_size='16pt',         font_weight='bold',         color='#2c3e50',         background_color='#ffffff',         border_color='#bdc3c7',         border_width=1,         padding=5     ) )  labels_group = folium.FeatureGroup(name='Area Labels') text_generator.generate_objects_for_model_df(control_areas_mescal, labels_group) labels_group.add_to(m5)  # Add transmission lines (extra high voltage only for cleaner view) ehv_lines = lines_gdf[lines_gdf['v_nom'] &gt;= 380] ehv_line_generator = LineGenerator(     LineFeatureResolver(         color='#c62828',         weight=3,         opacity=0.8,         tooltip=True     ) )  ehv_lines_group = folium.FeatureGroup(name='Extra High Voltage Lines (\u2265380kV)') ehv_line_generator.generate_objects_for_model_df(ehv_lines, ehv_lines_group) ehv_lines_group.add_to(m5)  # Add major bus nodes major_buses = buses_gdf[buses_gdf['v_nom'] &gt;= 380] major_bus_generator = CircleMarkerGenerator(     CircleMarkerFeatureResolver(         radius=6,         fill_color=PropertyMapper.from_static_value('#1a237e'),         color=PropertyMapper.from_static_value('#ffffff'),         weight=2,         fill_opacity=1.0,         tooltip=True     ) )  major_buses_group = folium.FeatureGroup(name='Major Bus Nodes (\u2265380kV)') major_bus_generator.generate_objects_for_model_df(major_buses, major_buses_group) major_buses_group.add_to(m5)  folium.LayerControl().add_to(m5) renderer.show_folium(m5)"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#mescal-401-folium-model-data-visualization-areas-points-and-lines","title":"MESCAL 401: Folium Model Data Visualization - Areas, Points, and Lines\u00b6","text":"<p>This notebook demonstrates how to visualize model DataFrames using MESCAL's folium visualization system. We'll explore how to create interactive maps showing network topology, geographic data, and model properties across different geometric object types.</p>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#introduction","title":"Introduction\u00b6","text":"<p>MESCAL's folium visualization system provides a powerful way to create interactive geospatial visualizations of energy system data. In this notebook, we'll cover:</p> <ol> <li>Area Visualization: Control areas and regions using AreaGenerator</li> <li>Point Visualization: Buses and generators using CircleMarkerGenerator</li> <li>Line Visualization: Transmission lines using LineGenerator</li> <li>Text Overlays: Labels and annotations using TextOverlayGenerator</li> <li>PropertyMapper System: Dynamic styling based on model properties</li> </ol> <p>We'll use both external geospatial data (GeoJSON files) and integrated MESCAL datasets with geographic information.</p>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mescal and all requirements are installed.</p>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#load-study-data","title":"Load Study Data\u00b6","text":"<p>Let's load our PyPSA study data which includes networks with geographic information:</p>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#part-1-area-visualization-control-areas","title":"Part 1: Area Visualization - Control Areas\u00b6","text":"<p>Let's start by visualizing control areas using both direct GeoJSON loading and the integrated MESCAL dataset approach.</p>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#approach-1-direct-geojson-loading","title":"Approach 1: Direct GeoJSON Loading\u00b6","text":""},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#approach-2-using-mescal-dataset-integration","title":"Approach 2: Using MESCAL Dataset Integration\u00b6","text":""},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#part-2-point-visualization-buses-and-generators","title":"Part 2: Point Visualization - Buses and Generators\u00b6","text":"<p>Now let's visualize buses as points on the map, showing how the ScigridDEBusModelInterpreter enriches bus data with control area information.</p>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#part-3-line-visualization-transmission-lines","title":"Part 3: Line Visualization - Transmission Lines\u00b6","text":"<p>Let's visualize the transmission lines connecting the buses in our network.</p>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#part-4-text-overlays-and-labels","title":"Part 4: Text Overlays and Labels\u00b6","text":"<p>Finally, let's add text overlays to label our control areas and key network elements.</p>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#summary-propertymapper-system","title":"Summary: PropertyMapper System\u00b6","text":"<p>Throughout this notebook, we've demonstrated the power of MESCAL's PropertyMapper system:</p> <ol> <li><p>Static Values:</p> <p>PropertyMapper.from_static_value('#e3f2fd')</p> <p>\u2192 Always returns the same value</p> <p>\u2192 In the FeatureResolver, you can also set the feature attribute=static_value and the FeatureResolver will automatically create a static mapping.</p> </li> <li><p>Object Attributes and Attribute Mappings:</p> <p>PropertyMapper.from_item_attr('name') \u2192 Extracts 'name' attribute from data objects</p> <p>PropertyMapper.from_item_attr('name', custom_name_mapping_function) \u2192 Extracts 'name' attribute and maps it through custom_name_mapping_function</p> </li> <li><p>Custom Functions:</p> <p>PropertyMapper(get_area_color) \u2192 Applies custom logic directly to the data-item to determine properties and map them according to your logic. Especially useful when you have conditional logic, like if object_value &gt; object_threshold then ... else ...</p> </li> <li><p>Default Mappers:</p> <p>\u2192 Geometries, text content, and other feature-properties will be intelligently generated based on default if no specifics are defined</p> </li> <li><p>Dynamic Styling Examples:</p> <ul> <li>Bus colors/sizes based on voltage levels</li> <li>Line colors/widths based on voltage levels</li> <li>Area colors based on control area names</li> <li>Text labels from object attributes</li> </ul> </li> </ol>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook, we've explored MESCAL's folium visualization system for model DataFrames:</p>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#key-concepts-covered","title":"Key Concepts Covered:\u00b6","text":"<ol> <li><p>Multiple Data Sources: Both direct GeoJSON loading and integrated MESCAL datasets</p> </li> <li><p>Object Type Visualization:</p> <ul> <li>Areas: Control regions with AreaGenerator</li> <li>Points: Buses and generators with CircleMarkerGenerator</li> <li>Lines: Transmission lines with LineGenerator</li> <li>Text: Labels and annotations with TextOverlayGenerator</li> </ul> </li> <li><p>PropertyMapper System: Dynamic styling based on data attributes</p> </li> <li><p>FeatureResolver Pattern: Configurable styling and behavior</p> </li> <li><p>Layer Management: Organized visualization with folium FeatureGroups</p> </li> </ol>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#next-steps","title":"Next Steps:\u00b6","text":"<p>In the next notebook (mescal_304), we'll explore how to visualize KPI data using the same system, showing how computed metrics can be mapped to geographic visualizations with automatic coloring, legends, and interactive features.</p>"},{"location":"mescal-study-01/notebooks/mescal_303_folium_model_df_map/#key-takeaways","title":"Key Takeaways:\u00b6","text":"<ul> <li>MESCAL's folium system provides a unified approach to geospatial visualization</li> <li>The PropertyMapper system enables highly flexible, data-driven styling</li> <li>Integration with MESCAL datasets allows seamless visualization of model data</li> <li>Multiple object types can be combined to create comprehensive network visualizations</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/","title":"MESCAL 402: Folium KPI Visualization System","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git\n        %cd mescal-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mescal -U\n        !pip install git+https://github.com/helgeesch/mescal-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git         %cd mescal-vanilla-studies/          !pip install git+https://github.com/helgeesch/mescal -U         !pip install git+https://github.com/helgeesch/mescal-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mescal-vanilla-studies')\n    os.chdir('/content/mescal-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mescal import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let's go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mescal-vanilla-studies')     os.chdir('/content/mescal-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mescal import StudyManager except ImportError:     raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let's go!\") <pre>\u2705 Environment ready. Let's go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport folium\nimport pandas as pd\nimport numpy as np\n\nfrom mescal import StudyManager, kpis\nfrom mescal_pypsa import PyPSADataset\nfrom mescal.visualizations.folium_viz_system import (\n    PropertyMapper, KPICollectionMapVisualizer,\n    AreaGenerator, AreaFeatureResolver,\n    CircleMarkerGenerator, CircleMarkerFeatureResolver,\n    TextOverlayGenerator, TextOverlayFeatureResolver\n)\nfrom mescal.visualizations.value_mapping_system import SegmentedContinuousColorscale\nfrom mescal.visualizations.folium_legend_system import ContinuousColorscaleLegend\nfrom mescal.utils.folium_utils.background_color import set_background_color_of_map\nfrom mescal.utils.plotly_utils.plotly_theme import colors\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\n# Register study-specific interpreters\nfrom studies.study_01_intro_to_mescal.src.study_specific_model_interpreters import (\n    ControlAreaModelInterpreter, \n    ScigridDEBusModelInterpreter\n)\n\nPyPSADataset.register_interpreter(ControlAreaModelInterpreter)\nPyPSADataset.register_interpreter(ScigridDEBusModelInterpreter)\n\nconfigure_clean_output_for_jupyter_notebook()\nrenderer = ConditionalRenderer()\n</pre> import os import folium import pandas as pd import numpy as np  from mescal import StudyManager, kpis from mescal_pypsa import PyPSADataset from mescal.visualizations.folium_viz_system import (     PropertyMapper, KPICollectionMapVisualizer,     AreaGenerator, AreaFeatureResolver,     CircleMarkerGenerator, CircleMarkerFeatureResolver,     TextOverlayGenerator, TextOverlayFeatureResolver ) from mescal.visualizations.value_mapping_system import SegmentedContinuousColorscale from mescal.visualizations.folium_legend_system import ContinuousColorscaleLegend from mescal.utils.folium_utils.background_color import set_background_color_of_map from mescal.utils.plotly_utils.plotly_theme import colors from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  # Register study-specific interpreters from studies.study_01_intro_to_mescal.src.study_specific_model_interpreters import (     ControlAreaModelInterpreter,      ScigridDEBusModelInterpreter )  PyPSADataset.register_interpreter(ControlAreaModelInterpreter) PyPSADataset.register_interpreter(ScigridDEBusModelInterpreter)  configure_clean_output_for_jupyter_notebook() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Load the StudyManager with PyPSA Scigrid-DE network data\nfrom studies.study_01_intro_to_mescal.scripts.setup_study_manager import get_scigrid_de_study_manager\nstudy = get_scigrid_de_study_manager()\n\nprint(\"Study scenarios:\")\nfor dataset in study.scen.datasets:\n    print(f\"  \ud83d\udcca {dataset.name}\")\n    \nprint(\"\\nComparisons:\")\nfor dataset in study.comp.datasets:\n    print(f\"  \ud83d\udd04 {dataset.name}\")\n</pre> # Load the StudyManager with PyPSA Scigrid-DE network data from studies.study_01_intro_to_mescal.scripts.setup_study_manager import get_scigrid_de_study_manager study = get_scigrid_de_study_manager()  print(\"Study scenarios:\") for dataset in study.scen.datasets:     print(f\"  \ud83d\udcca {dataset.name}\")      print(\"\\nComparisons:\") for dataset in study.comp.datasets:     print(f\"  \ud83d\udd04 {dataset.name}\") <pre>Study scenarios:\n  \ud83d\udcca base\n  \ud83d\udcca solar_150\n  \ud83d\udcca solar_200\n  \ud83d\udcca wind_150\n  \ud83d\udcca wind_200\n\nComparisons:\n  \ud83d\udd04 solar_150 vs base\n  \ud83d\udd04 solar_200 vs base\n  \ud83d\udd04 wind_150 vs base\n  \ud83d\udd04 wind_200 vs base\n</pre> In\u00a0[22]: Copied! <pre># Clear any existing KPIs\nstudy.scen.clear_kpi_collection_for_all_sub_datasets()\nstudy.comp.clear_kpi_collection_for_all_sub_datasets()\n\n# Get base dataset to access control areas\nds_base = study.scen.get_dataset('base')\ncontrol_areas = ds_base.fetch('control_areas').index.to_list()\ngenerators = ds_base.fetch('generators').query(\"carrier.isin(['Gas', 'Hard Coal', 'Brown Coal'])\").index.to_list()\n\n# Define scenario KPIs\nscenario_kpis = []\n\n# 1. Market price KPIs for each control area\nmarket_price_kpis = [\n    kpis.FlagAggKPIFactory(\n        'control_areas_t.vol_weighted_marginal_price',\n        aggregation=kpis.aggregations.Mean,\n        column_subset=area,\n    )\n    for area in control_areas\n]\nscenario_kpis.extend(market_price_kpis)\n\n# 2. Generation capacity utilization KPIs\ngeneration_kpis = [\n    kpis.FlagAggKPIFactory(\n        'generators_t.p',\n        aggregation=kpis.aggregations.Mean,\n        column_subset=g,\n    )\n    for g in generators\n]\nscenario_kpis.extend(generation_kpis)\n\n# Define comparison KPIs (calculate changes between scenarios)\ncomparison_kpis = []\n\n# Price changes\nprice_change_kpis = [\n    kpis.ComparisonKPIFactory(\n        factory,\n        kpis.value_comparisons.Increase  # Absolute change\n    )\n    for factory in market_price_kpis\n]\ncomparison_kpis.extend(price_change_kpis)\n\n# Generation changes\ngeneration_change_kpis = [\n    kpis.ComparisonKPIFactory(\n        factory,\n        kpis.value_comparisons.Delta\n    )\n    for factory in generation_kpis\n]\ncomparison_kpis.extend(generation_change_kpis)\n\n# Add KPIs to datasets and compute\nstudy.scen.add_kpis_to_all_sub_datasets(scenario_kpis)\nstudy.comp.add_kpis_to_all_sub_datasets(comparison_kpis)\n\nprint(f\"\\nAdded {len(scenario_kpis)} scenario KPIs and {len(comparison_kpis)} comparison KPIs\")\n\n# Compute all KPIs\nprint(\"\\nComputing scenario KPIs...\")\nstudy.scen.get_merged_kpi_collection().compute_all()\n\nprint(\"Computing comparison KPIs...\")\nstudy.comp.get_merged_kpi_collection().compute_all()\n\nprint(\"\u2705 All KPIs computed successfully!\")\n</pre> # Clear any existing KPIs study.scen.clear_kpi_collection_for_all_sub_datasets() study.comp.clear_kpi_collection_for_all_sub_datasets()  # Get base dataset to access control areas ds_base = study.scen.get_dataset('base') control_areas = ds_base.fetch('control_areas').index.to_list() generators = ds_base.fetch('generators').query(\"carrier.isin(['Gas', 'Hard Coal', 'Brown Coal'])\").index.to_list()  # Define scenario KPIs scenario_kpis = []  # 1. Market price KPIs for each control area market_price_kpis = [     kpis.FlagAggKPIFactory(         'control_areas_t.vol_weighted_marginal_price',         aggregation=kpis.aggregations.Mean,         column_subset=area,     )     for area in control_areas ] scenario_kpis.extend(market_price_kpis)  # 2. Generation capacity utilization KPIs generation_kpis = [     kpis.FlagAggKPIFactory(         'generators_t.p',         aggregation=kpis.aggregations.Mean,         column_subset=g,     )     for g in generators ] scenario_kpis.extend(generation_kpis)  # Define comparison KPIs (calculate changes between scenarios) comparison_kpis = []  # Price changes price_change_kpis = [     kpis.ComparisonKPIFactory(         factory,         kpis.value_comparisons.Increase  # Absolute change     )     for factory in market_price_kpis ] comparison_kpis.extend(price_change_kpis)  # Generation changes generation_change_kpis = [     kpis.ComparisonKPIFactory(         factory,         kpis.value_comparisons.Delta     )     for factory in generation_kpis ] comparison_kpis.extend(generation_change_kpis)  # Add KPIs to datasets and compute study.scen.add_kpis_to_all_sub_datasets(scenario_kpis) study.comp.add_kpis_to_all_sub_datasets(comparison_kpis)  print(f\"\\nAdded {len(scenario_kpis)} scenario KPIs and {len(comparison_kpis)} comparison KPIs\")  # Compute all KPIs print(\"\\nComputing scenario KPIs...\") study.scen.get_merged_kpi_collection().compute_all()  print(\"Computing comparison KPIs...\") study.comp.get_merged_kpi_collection().compute_all()  print(\"\u2705 All KPIs computed successfully!\") <pre>\nAdded 253 scenario KPIs and 253 comparison KPIs\n\nComputing scenario KPIs...\nComputing comparison KPIs...\n\u2705 All KPIs computed successfully!\n</pre> In\u00a0[23]: Copied! <pre># Get KPI collections\nscenario_kpi_collection = study.scen.get_merged_kpi_collection()\ncomparison_kpi_collection = study.comp.get_merged_kpi_collection()\n\nprint(f\"Total scenario KPIs: {len(scenario_kpi_collection)}\")\nprint(f\"Total comparison KPIs: {len(comparison_kpi_collection)}\")\n\n# Show example KPIs\nprint(\"\\nExample scenario KPIs:\")\nfor i, kpi in enumerate(scenario_kpi_collection):\n    print(f\"  {i+1}. {kpi.name} = {kpi.value:.2f} {kpi.unit}\")\n    print(f\"     Dataset: {kpi.dataset.name}\")\n    print(f\"     Object: {getattr(kpi.attributes, 'object_name', 'N/A')}\")\n    if i == 2:\n        break\n\nprint(\"\\nExample comparison KPIs:\")\nfor i, kpi in enumerate(comparison_kpi_collection):\n    print(f\"  {i+1}. {kpi.name} = {kpi.value:.2f} {kpi.unit}\")\n    print(f\"     Dataset: {kpi.dataset.name}\")\n    print(f\"     Object: {getattr(kpi.attributes, 'object_name', 'N/A')}\")\n    if i == 2:\n        break\n\n# Show KPI attributes for filtering\nprint(\"\\nAvailable KPI attributes for filtering:\")\nsample_kpi = list(scenario_kpi_collection)[0]\nfor attr_name, attr_value in sample_kpi.attributes.as_dict(primitive_values=True).items():\n    print(f\"  {attr_name}: {attr_value}\")\n</pre> # Get KPI collections scenario_kpi_collection = study.scen.get_merged_kpi_collection() comparison_kpi_collection = study.comp.get_merged_kpi_collection()  print(f\"Total scenario KPIs: {len(scenario_kpi_collection)}\") print(f\"Total comparison KPIs: {len(comparison_kpi_collection)}\")  # Show example KPIs print(\"\\nExample scenario KPIs:\") for i, kpi in enumerate(scenario_kpi_collection):     print(f\"  {i+1}. {kpi.name} = {kpi.value:.2f} {kpi.unit}\")     print(f\"     Dataset: {kpi.dataset.name}\")     print(f\"     Object: {getattr(kpi.attributes, 'object_name', 'N/A')}\")     if i == 2:         break  print(\"\\nExample comparison KPIs:\") for i, kpi in enumerate(comparison_kpi_collection):     print(f\"  {i+1}. {kpi.name} = {kpi.value:.2f} {kpi.unit}\")     print(f\"     Dataset: {kpi.dataset.name}\")     print(f\"     Object: {getattr(kpi.attributes, 'object_name', 'N/A')}\")     if i == 2:         break  # Show KPI attributes for filtering print(\"\\nAvailable KPI attributes for filtering:\") sample_kpi = list(scenario_kpi_collection)[0] for attr_name, attr_value in sample_kpi.attributes.as_dict(primitive_values=True).items():     print(f\"  {attr_name}: {attr_value}\") <pre>Total scenario KPIs: 1265\nTotal comparison KPIs: 2277\n\nExample scenario KPIs:\n  1. Mean generators_t.p (167 Gas) = 0.00 NaU\n     Dataset: wind_150\n     Object: 167 Gas\n  2. Mean generators_t.p (422 Gas) = 0.00 NaU\n     Dataset: solar_200\n     Object: 422 Gas\n  3. Mean generators_t.p (92 Gas) = 0.00 NaU\n     Dataset: wind_200\n     Object: 92 Gas\n\nExample comparison KPIs:\n  1. Mean generators_t.p (184_220kV Gas) Delta = 0.00 NaU\n     Dataset: solar_200 vs base\n     Object: 184_220kV Gas\n  2. Mean generators_t.p (422 Gas) = 0.00 NaU\n     Dataset: solar_200\n     Object: 422 Gas\n  3. Mean generators_t.p (167 Gas) = 0.00 NaU\n     Dataset: wind_150\n     Object: 167 Gas\n\nAvailable KPI attributes for filtering:\n  name: Mean generators_t.p (167 Gas)\n  dataset: wind_150\n  dataset_type: ScigridDEDataset\n  unit: NaU\n  base_unit: NaU\n  flag: generators_t.p\n  object_name: 167 Gas\n  model_flag: generators\n  aggregation: Mean\n  column_subset: 167 Gas\n</pre> In\u00a0[24]: Copied! <pre># Create colorscale for market prices\nprice_colorscale = SegmentedContinuousColorscale(\n    segments={\n        (-25, 0): colors.sequential.shades_of_blue,\n        (0, 25): colors.sequential.shades_of_pink[::-1],\n    },\n    nan_fallback='#CCCCCC'\n)\n\n# Create map\nm1 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm1 = set_background_color_of_map(m1, color='#ffffff')\n\n# Add legend\nContinuousColorscaleLegend(\n    mapping=price_colorscale,\n    title=\"Market Price (\u20ac/MWh)\",\n    width=300,\n    position={'bottom': 50, 'right': 50}\n).add_to(m1)\n\n# Create area generator with KPI-based coloring\nprice_area_generator = AreaGenerator(\n    AreaFeatureResolver(\n        fill_color=PropertyMapper.from_kpi_value(price_colorscale),\n        fill_opacity=0.8,\n        border_color=PropertyMapper.from_static_value('#2c3e50'),\n        border_width=2,\n        tooltip=True  # Auto-generates tooltip with KPI info\n    )\n)\n\n# Filter KPIs to show only market prices\nprice_kpis = scenario_kpi_collection.get_filtered_kpi_collection_by_attributes(\n    attr_query=\"flag.str.contains('price')\",\n)\n\nprint(f\"Found {len(price_kpis)} price KPIs\")\n\n# Use KPICollectionMapVisualizer for automatic visualization\nKPICollectionMapVisualizer(\n    generators=[price_area_generator, TextOverlayGenerator()]\n).generate_and_add_feature_groups_to_map(price_kpis, m1, show='last')\n\nfolium.LayerControl(collapsed=False).add_to(m1)\nrenderer.show_folium(m1)\n</pre> # Create colorscale for market prices price_colorscale = SegmentedContinuousColorscale(     segments={         (-25, 0): colors.sequential.shades_of_blue,         (0, 25): colors.sequential.shades_of_pink[::-1],     },     nan_fallback='#CCCCCC' )  # Create map m1 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m1 = set_background_color_of_map(m1, color='#ffffff')  # Add legend ContinuousColorscaleLegend(     mapping=price_colorscale,     title=\"Market Price (\u20ac/MWh)\",     width=300,     position={'bottom': 50, 'right': 50} ).add_to(m1)  # Create area generator with KPI-based coloring price_area_generator = AreaGenerator(     AreaFeatureResolver(         fill_color=PropertyMapper.from_kpi_value(price_colorscale),         fill_opacity=0.8,         border_color=PropertyMapper.from_static_value('#2c3e50'),         border_width=2,         tooltip=True  # Auto-generates tooltip with KPI info     ) )  # Filter KPIs to show only market prices price_kpis = scenario_kpi_collection.get_filtered_kpi_collection_by_attributes(     attr_query=\"flag.str.contains('price')\", )  print(f\"Found {len(price_kpis)} price KPIs\")  # Use KPICollectionMapVisualizer for automatic visualization KPICollectionMapVisualizer(     generators=[price_area_generator, TextOverlayGenerator()] ).generate_and_add_feature_groups_to_map(price_kpis, m1, show='last')  folium.LayerControl(collapsed=False).add_to(m1) renderer.show_folium(m1) <pre>Found 20 price KPIs\n</pre> <pre>KPICollectionMapVisualizer: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:00&lt;00:00, 471.78it/s]\n</pre> In\u00a0[25]: Copied! <pre># Create diverging colorscale for price changes\nprice_change_colormap = SegmentedContinuousColorscale(\n    segments={\n        (-10, 0): colors.sequential.shades_of_blue,\n        (0, 10): colors.sequential.shades_of_pink[::-1],\n    },\n    nan_fallback='#CCCCCC'\n)\n\n# Create comparison visualization map\nm2 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm2 = set_background_color_of_map(m2, color='#ffffff')\n\n# Add legend for price changes\nContinuousColorscaleLegend(\n    mapping=price_change_colormap,\n    title=\"Price Change (\u20ac/MWh)\",\n    width=300,\n    position={'bottom': 50, 'right': 50}\n).add_to(m2)\n\n# Create area generator for price changes\nprice_change_generator = AreaGenerator(\n    AreaFeatureResolver(\n        fill_color=PropertyMapper.from_kpi_value(price_change_colormap),\n        fill_opacity=0.8,\n        border_color=PropertyMapper.from_static_value('#34495e'),\n        border_width=2,\n        tooltip=True\n    )\n)\n\n# Filter comparison KPIs for price changes\nprice_change_kpis = comparison_kpi_collection.get_filtered_kpi_collection_by_attributes(\n    attr_query=\"flag.str.contains('price')\",\n)\n\nprint(f\"Found {len(price_change_kpis)} price change KPIs\")\n\n# Visualize price changes\nKPICollectionMapVisualizer(\n    generators=[price_change_generator, TextOverlayGenerator()]\n).generate_and_add_feature_groups_to_map(price_change_kpis, m2, show='last')\n\nfolium.LayerControl(collapsed=False).add_to(m2)\nrenderer.show_folium(m2)\n</pre> # Create diverging colorscale for price changes price_change_colormap = SegmentedContinuousColorscale(     segments={         (-10, 0): colors.sequential.shades_of_blue,         (0, 10): colors.sequential.shades_of_pink[::-1],     },     nan_fallback='#CCCCCC' )  # Create comparison visualization map m2 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m2 = set_background_color_of_map(m2, color='#ffffff')  # Add legend for price changes ContinuousColorscaleLegend(     mapping=price_change_colormap,     title=\"Price Change (\u20ac/MWh)\",     width=300,     position={'bottom': 50, 'right': 50} ).add_to(m2)  # Create area generator for price changes price_change_generator = AreaGenerator(     AreaFeatureResolver(         fill_color=PropertyMapper.from_kpi_value(price_change_colormap),         fill_opacity=0.8,         border_color=PropertyMapper.from_static_value('#34495e'),         border_width=2,         tooltip=True     ) )  # Filter comparison KPIs for price changes price_change_kpis = comparison_kpi_collection.get_filtered_kpi_collection_by_attributes(     attr_query=\"flag.str.contains('price')\", )  print(f\"Found {len(price_change_kpis)} price change KPIs\")  # Visualize price changes KPICollectionMapVisualizer(     generators=[price_change_generator, TextOverlayGenerator()] ).generate_and_add_feature_groups_to_map(price_change_kpis, m2, show='last')  folium.LayerControl(collapsed=False).add_to(m2) renderer.show_folium(m2) <pre>Found 36 price change KPIs\n</pre> <pre>KPICollectionMapVisualizer: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 36/36 [00:01&lt;00:00, 24.59it/s]\n</pre> In\u00a0[32]: Copied! <pre># Create comprehensive map with multiple KPI layers\nm3 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm3 = set_background_color_of_map(m3, color='#ffffff')\n\n# Define different colorscales for different KPI types\ncolorscales = {\n    'price': price_colorscale,\n    'change': price_change_colormap,\n    'generation': SegmentedContinuousColorscale.single_segment_autoscale_factory_from_array(\n        values=[0, 100],\n        color_range='viridis',\n    ),\n}\n\n# Add legends for each KPI type\nlegend_positions = [\n    {'bottom': 50, 'left': 50},\n    {'bottom': 200, 'left': 50},\n    {'bottom': 350, 'left': 50}\n]\n\nContinuousColorscaleLegend(\n    mapping=colorscales['price'],\n    title=\"Average Market Price (\u20ac/MWh)\",\n    width=280,\n    position=legend_positions[0]\n).add_to(m3)\n\nContinuousColorscaleLegend(\n    mapping=colorscales['change'],\n    title=\"Price Change (\u20ac/MWh)\",\n    width=280,\n    position=legend_positions[1]\n).add_to(m3)\n\nContinuousColorscaleLegend(\n    mapping=colorscales['generation'],\n    title=\"Average Generation (MW)\",\n    width=280,\n    position=legend_positions[2]\n).add_to(m3)\n\nfrom shapely import Point\nfrom mescal.visualizations.folium_viz_system import VisualizableDataItem\n\n# In this case, we don't have a \"location\" column ready in the generator model df, so we create the location on demand\ndef _get_location_of_generator(gen_data_item: VisualizableDataItem) -&gt; Point:\n    lat = gen_data_item.get_object_attribute('bus_y')\n    lon = gen_data_item.get_object_attribute('bus_x')\n    return Point([lon, lat])\n\n# Create generators for different KPI types\ngenerators = {\n    'price': AreaGenerator(\n        AreaFeatureResolver(\n            fill_color=PropertyMapper.from_kpi_value(colorscales['price']),\n            fill_opacity=0.8,\n            border_width=2,\n            tooltip=True\n        )\n    ),\n    'generation': CircleMarkerGenerator(\n        CircleMarkerFeatureResolver(\n            fill_color=PropertyMapper.from_kpi_value(colorscales['generation']),\n            # radius=PropertyMapper.from_kpi_value(lambda p: max(min(20, p/50), 3)),\n            radius=PropertyMapper.from_item_attr('p_nom', lambda p: max(min(20, p/50), 3)),\n            fill_opacity=0.8,\n            border_width=1,\n            tooltip=True,\n            location=PropertyMapper(_get_location_of_generator)\n        )\n    ),\n    'change': AreaGenerator(\n        AreaFeatureResolver(\n            fill_color=PropertyMapper.from_kpi_value(colorscales['change']),\n            fill_opacity=0.8,\n            border_width=2,\n            tooltip=True\n        )\n    )\n}\n\n# Visualize market prices for scenarios\nprice_kpis = scenario_kpi_collection.get_filtered_kpi_collection_by_attributes(\n    attr_query='flag.str.contains(\"price\")'\n)\nprice_fgs = KPICollectionMapVisualizer(\n    generators=[generators['price'], TextOverlayGenerator()]\n).generate_and_add_feature_groups_to_map(price_kpis, m3, show=\"none\")\n\n# Visualize price changes for comparisons\nprice_change_kpis = comparison_kpi_collection.get_filtered_kpi_collection_by_attributes(\n    attr_query='flag.str.contains(\"price\")',\n    value_operation='Increase',\n)\nprice_change_fgs = KPICollectionMapVisualizer(\n    generators=[generators['change'], TextOverlayGenerator()]\n).generate_and_add_feature_groups_to_map(price_change_kpis, m3, show='first')\n\n# Visualize generation for scenarios\ngeneration_kpis = scenario_kpi_collection.get_filtered_kpi_collection_by_attributes(\n    attr_query='flag.str.contains(\"generators_t.p\")',\n)\ngeneration_fgs = KPICollectionMapVisualizer(\n    generators=[generators['generation'], TextOverlayGenerator()]\n).generate_and_add_feature_groups_to_map(generation_kpis, m3, show='first')\n\n\nfolium.LayerControl(collapsed=False).add_to(m3)\nfrom folium.plugins import GroupedLayerControl\n\nGroupedLayerControl(\n    groups={'Area Fgs': price_fgs + price_change_fgs, 'Generator Fgs': generation_fgs},\n    exclusive_groups=True,\n    collapsed=False,\n).add_to(m3)\n\nrenderer.show_folium(m3)\n</pre> # Create comprehensive map with multiple KPI layers m3 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m3 = set_background_color_of_map(m3, color='#ffffff')  # Define different colorscales for different KPI types colorscales = {     'price': price_colorscale,     'change': price_change_colormap,     'generation': SegmentedContinuousColorscale.single_segment_autoscale_factory_from_array(         values=[0, 100],         color_range='viridis',     ), }  # Add legends for each KPI type legend_positions = [     {'bottom': 50, 'left': 50},     {'bottom': 200, 'left': 50},     {'bottom': 350, 'left': 50} ]  ContinuousColorscaleLegend(     mapping=colorscales['price'],     title=\"Average Market Price (\u20ac/MWh)\",     width=280,     position=legend_positions[0] ).add_to(m3)  ContinuousColorscaleLegend(     mapping=colorscales['change'],     title=\"Price Change (\u20ac/MWh)\",     width=280,     position=legend_positions[1] ).add_to(m3)  ContinuousColorscaleLegend(     mapping=colorscales['generation'],     title=\"Average Generation (MW)\",     width=280,     position=legend_positions[2] ).add_to(m3)  from shapely import Point from mescal.visualizations.folium_viz_system import VisualizableDataItem  # In this case, we don't have a \"location\" column ready in the generator model df, so we create the location on demand def _get_location_of_generator(gen_data_item: VisualizableDataItem) -&gt; Point:     lat = gen_data_item.get_object_attribute('bus_y')     lon = gen_data_item.get_object_attribute('bus_x')     return Point([lon, lat])  # Create generators for different KPI types generators = {     'price': AreaGenerator(         AreaFeatureResolver(             fill_color=PropertyMapper.from_kpi_value(colorscales['price']),             fill_opacity=0.8,             border_width=2,             tooltip=True         )     ),     'generation': CircleMarkerGenerator(         CircleMarkerFeatureResolver(             fill_color=PropertyMapper.from_kpi_value(colorscales['generation']),             # radius=PropertyMapper.from_kpi_value(lambda p: max(min(20, p/50), 3)),             radius=PropertyMapper.from_item_attr('p_nom', lambda p: max(min(20, p/50), 3)),             fill_opacity=0.8,             border_width=1,             tooltip=True,             location=PropertyMapper(_get_location_of_generator)         )     ),     'change': AreaGenerator(         AreaFeatureResolver(             fill_color=PropertyMapper.from_kpi_value(colorscales['change']),             fill_opacity=0.8,             border_width=2,             tooltip=True         )     ) }  # Visualize market prices for scenarios price_kpis = scenario_kpi_collection.get_filtered_kpi_collection_by_attributes(     attr_query='flag.str.contains(\"price\")' ) price_fgs = KPICollectionMapVisualizer(     generators=[generators['price'], TextOverlayGenerator()] ).generate_and_add_feature_groups_to_map(price_kpis, m3, show=\"none\")  # Visualize price changes for comparisons price_change_kpis = comparison_kpi_collection.get_filtered_kpi_collection_by_attributes(     attr_query='flag.str.contains(\"price\")',     value_operation='Increase', ) price_change_fgs = KPICollectionMapVisualizer(     generators=[generators['change'], TextOverlayGenerator()] ).generate_and_add_feature_groups_to_map(price_change_kpis, m3, show='first')  # Visualize generation for scenarios generation_kpis = scenario_kpi_collection.get_filtered_kpi_collection_by_attributes(     attr_query='flag.str.contains(\"generators_t.p\")', ) generation_fgs = KPICollectionMapVisualizer(     generators=[generators['generation'], TextOverlayGenerator()] ).generate_and_add_feature_groups_to_map(generation_kpis, m3, show='first')   folium.LayerControl(collapsed=False).add_to(m3) from folium.plugins import GroupedLayerControl  GroupedLayerControl(     groups={'Area Fgs': price_fgs + price_change_fgs, 'Generator Fgs': generation_fgs},     exclusive_groups=True,     collapsed=False, ).add_to(m3)  renderer.show_folium(m3) <pre>KPICollectionMapVisualizer: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:00&lt;00:00, 484.93it/s]\nKPICollectionMapVisualizer: 0it [00:00, ?it/s]\nKPICollectionMapVisualizer: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1245/1245 [00:15&lt;00:00, 80.85it/s]\n</pre>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#mescal-402-folium-kpi-visualization-system","title":"MESCAL 402: Folium KPI Visualization System\u00b6","text":"<p>This notebook demonstrates how to visualize KPI data using MESCAL's folium system. We'll explore how computed metrics and indicators can be mapped to interactive geographic visualizations with automatic coloring, legends, and multi-scenario analysis.</p>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#introduction","title":"Introduction\u00b6","text":"<p>Building on the model data visualization from mescal_401, this notebook focuses on visualizing computed KPIs (Key Performance Indicators). We'll cover:</p> <ol> <li>KPI Data Items: How KPIs integrate with the folium visualization system</li> <li>KPI Collection Visualization: Automatic mapping of KPI collections to maps</li> <li>Dynamic Coloring: Using PropertyMapper.from_kpi_value() for data-driven styling</li> <li>Multi-Scenario KPI Maps: Comparing KPIs across different scenarios</li> <li>KPI Filtering and Grouping: Selective visualization of KPI subsets</li> <li>Interactive Features: Tooltips, popups, and layer controls for KPI exploration</li> </ol> <p>This approach enables sophisticated analysis of energy system performance metrics with geographic context.</p>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mescal and all requirements are installed.</p>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#load-study-data-and-setup-kpis","title":"Load Study Data and Setup KPIs\u00b6","text":"<p>Let's load our study data and define comprehensive KPIs for visualization:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#explore-kpi-structure","title":"Explore KPI Structure\u00b6","text":"<p>Let's examine the structure of our KPI collections to understand what we can visualize:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#part-1-basic-kpi-visualization","title":"Part 1: Basic KPI Visualization\u00b6","text":"<p>Let's start with a simple visualization of market prices across control areas for all scenarios:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#part-2-comparison-visualization","title":"Part 2: Comparison Visualization\u00b6","text":"<p>Now let's visualize price changes between scenarios using comparison KPIs:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#part-3-multi-kpi-visualization","title":"Part 3: Multi-KPI Visualization\u00b6","text":"<p>Let's create a comprehensive map showing multiple KPI types with layer controls:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#summary-kpi-visualization-system-architecture","title":"Summary: KPI Visualization System Architecture\u00b6","text":"<p>Let's summarize the key components and patterns we've demonstrated:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#key-components-demonstrated","title":"Key Components Demonstrated:\u00b6","text":"<ul> <li>\ud83d\udcca KPIDataItem: Wraps KPI objects for visualization pipeline</li> <li>\ud83c\udfa8 PropertyMapper.from_kpi_value(): Maps KPI values to visual properties</li> <li>\ud83d\udd0d KPI Collection Filtering: By attributes, scenarios, and types</li> <li>\ud83d\uddfa\ufe0f KPICollectionMapVisualizer: Automatic map generation</li> <li>\ud83c\udf9b\ufe0f Multi-layer visualization: Different KPI types on same map</li> <li>\ud83d\udcc8 Custom styling: Based on KPI magnitude and significance</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#visualization-patterns","title":"Visualization Patterns:\u00b6","text":"<ul> <li>\u2022 Scenario Comparison: Side-by-side analysis of different cases</li> <li>\u2022 Change Analysis: Delta visualization with diverging colors</li> <li>\u2022 Multi-KPI Dashboards: Layer controls for different metrics</li> <li>\u2022 Significance Encoding: Opacity/border based on value magnitude</li> <li>\u2022 Interactive Features: Tooltips, popups, and layer controls</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#advanced-features","title":"Advanced Features:\u00b6","text":"<ul> <li>\u2728 Automatic tooltip generation from KPI attributes</li> <li>\ud83c\udfaf Regex-based KPI filtering for flexible selection</li> <li>\ud83d\udccf Custom colorscales for different data ranges</li> <li>\ud83d\udd04 Dynamic styling based on KPI properties</li> <li>\ud83d\udcca Integration with MESCAL's scenario comparison framework</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook, we've demonstrated MESCAL's powerful KPI visualization system:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#key-achievements","title":"Key Achievements:\u00b6","text":"<ol> <li>Automated KPI Mapping: Direct integration between computed KPIs and geographic visualizations</li> <li>Multi-Scenario Analysis: Simultaneous visualization of different scenarios and their comparisons</li> <li>Flexible Filtering: Sophisticated KPI selection based on multiple criteria</li> <li>Advanced Styling: Custom visual encoding based on KPI properties and significance</li> <li>Interactive Analysis: Layer controls, tooltips, and popups for detailed exploration</li> </ol>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#architecture-benefits","title":"Architecture Benefits:\u00b6","text":"<ul> <li>Unified Pipeline: Same visualization system works for any KPI type</li> <li>Automatic Integration: KPIs automatically carry geographic context from their source datasets</li> <li>Scalable Design: Easily handles large numbers of KPIs across multiple scenarios</li> <li>Customizable Styling: PropertyMapper system enables any visual encoding pattern</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_304_folium_area_kpi_map/#practical-applications","title":"Practical Applications:\u00b6","text":"<p>This visualization approach is particularly valuable for:</p> <ul> <li>Scenario Impact Assessment: Understanding regional effects of policy changes</li> <li>Market Analysis: Visualizing price patterns and market dynamics</li> <li>Infrastructure Planning: Identifying areas needing investment or attention</li> <li>Stakeholder Communication: Clear visual communication of complex analysis results</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_304_simple_area_kpi_map/","title":"MESCAL 303: Interactive Geospatial with Folium","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git\n        %cd mescal-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mescal -U\n        !pip install git+https://github.com/helgeesch/mescal-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git         %cd mescal-vanilla-studies/          !pip install git+https://github.com/helgeesch/mescal -U         !pip install git+https://github.com/helgeesch/mescal-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>Running locally, let's continue.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mescal-vanilla-studies')\n    os.chdir('/content/mescal-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mescal import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mescal-vanilla-studies')     os.chdir('/content/mescal-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mescal import StudyManager except ImportError:     raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport folium\n\nfrom mescal import kpis\nfrom mescal.visualizations.value_mapping_system import SegmentedContinuousColorscale\nfrom mescal.visualizations.folium_legend_system import ContinuousColorscaleLegend\nfrom mescal.visualizations.folium_viz_system import (\n    PropertyMapper, KPICollectionMapVisualizer,\n    AreaFeatureResolver, AreaGenerator,\n    TextOverlayFeatureResolver, TextOverlayGenerator,\n)\nfrom mescal.utils.folium_utils.background_color import set_background_color_of_map\nfrom mescal.utils.plotly_utils.plotly_theme import colors\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nrenderer = ConditionalRenderer()\n</pre> import os import folium  from mescal import kpis from mescal.visualizations.value_mapping_system import SegmentedContinuousColorscale from mescal.visualizations.folium_legend_system import ContinuousColorscaleLegend from mescal.visualizations.folium_viz_system import (     PropertyMapper, KPICollectionMapVisualizer,     AreaFeatureResolver, AreaGenerator,     TextOverlayFeatureResolver, TextOverlayGenerator, ) from mescal.utils.folium_utils.background_color import set_background_color_of_map from mescal.utils.plotly_utils.plotly_theme import colors from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Load the StudyManager with PyPSA Scigrid-DE network data we created in previous notebooks\nfrom studies.study_01_intro_to_mescal.scripts.setup_study_manager import get_scigrid_de_study_manager\nstudy = get_scigrid_de_study_manager()\n\n# Check available datasets\nprint(\"Available scenarios:\")\nfor dataset in study.scen.datasets:\n    print(f\"- {dataset.name}\")\n</pre> # Load the StudyManager with PyPSA Scigrid-DE network data we created in previous notebooks from studies.study_01_intro_to_mescal.scripts.setup_study_manager import get_scigrid_de_study_manager study = get_scigrid_de_study_manager()  # Check available datasets print(\"Available scenarios:\") for dataset in study.scen.datasets:     print(f\"- {dataset.name}\") <pre>Available scenarios:\n- base\n- solar_150\n- solar_200\n- wind_150\n- wind_200\n</pre> In\u00a0[5]: Copied! <pre># Clear any existing KPIs\nstudy.scen.clear_kpi_collection_for_all_sub_datasets()\nstudy.comp.clear_kpi_collection_for_all_sub_datasets()\n\n# Initialize KPI lists\nmy_scen_kpis = []\nmy_comp_kpis = []\n\n# Get base dataset to access control areas\nds_base = study.scen.get_dataset('base')\n\n# Create volume-weighted market price KPIs for each control area\nscen_kpis_market_price = [\n    kpis.FlagAggKPIFactory(\n        'control_areas_t.vol_weighted_marginal_price', # Time series flag\n        aggregation=kpis.aggregations.Mean,            # Calculate mean across time\n        column_subset=control_area                     # For each control area\n    )\n    for control_area in ds_base.fetch('control_areas').index.to_list()\n]\n\n# Add the price KPIs to our scenario KPIs list\nmy_scen_kpis += scen_kpis_market_price\n\n# Create comparison KPIs to calculate price increases between scenarios\ncomp_vars_increase = [\n    kpis.ComparisonKPIFactory(\n        factory,                         # Use the same KPI factories as scenarios\n        kpis.value_comparisons.Increase  # Calculate simple increase (var - ref)\n    )\n    for factory in my_scen_kpis\n]\n\n# Add the comparison KPIs to our comparison KPIs list\nmy_comp_kpis += comp_vars_increase\n\n# Add all KPIs to respective datasets\nstudy.scen.add_kpis_to_all_sub_datasets(my_scen_kpis)\nstudy.comp.add_kpis_to_all_sub_datasets(my_comp_kpis)\n\n# Compute all KPIs (with progress bar)\nstudy.scen.get_merged_kpi_collection().compute_all()\nstudy.comp.get_merged_kpi_collection().compute_all()\n</pre> # Clear any existing KPIs study.scen.clear_kpi_collection_for_all_sub_datasets() study.comp.clear_kpi_collection_for_all_sub_datasets()  # Initialize KPI lists my_scen_kpis = [] my_comp_kpis = []  # Get base dataset to access control areas ds_base = study.scen.get_dataset('base')  # Create volume-weighted market price KPIs for each control area scen_kpis_market_price = [     kpis.FlagAggKPIFactory(         'control_areas_t.vol_weighted_marginal_price', # Time series flag         aggregation=kpis.aggregations.Mean,            # Calculate mean across time         column_subset=control_area                     # For each control area     )     for control_area in ds_base.fetch('control_areas').index.to_list() ]  # Add the price KPIs to our scenario KPIs list my_scen_kpis += scen_kpis_market_price  # Create comparison KPIs to calculate price increases between scenarios comp_vars_increase = [     kpis.ComparisonKPIFactory(         factory,                         # Use the same KPI factories as scenarios         kpis.value_comparisons.Increase  # Calculate simple increase (var - ref)     )     for factory in my_scen_kpis ]  # Add the comparison KPIs to our comparison KPIs list my_comp_kpis += comp_vars_increase  # Add all KPIs to respective datasets study.scen.add_kpis_to_all_sub_datasets(my_scen_kpis) study.comp.add_kpis_to_all_sub_datasets(my_comp_kpis)  # Compute all KPIs (with progress bar) study.scen.get_merged_kpi_collection().compute_all() study.comp.get_merged_kpi_collection().compute_all() In\u00a0[6]: Copied! <pre># Create a map centered on Germany\nm = folium.Map(\n    location=[51, 11],  # Center of Germany\n    tiles=None,         # No background tiles initially\n    zoom_start=6,       # Initial zoom level\n    zoom_snap=0.25      # Allow finer zoom control\n)\n\n# Set a clean white background\nm = set_background_color_of_map(m, color='#ffffff')\n</pre> # Create a map centered on Germany m = folium.Map(     location=[51, 11],  # Center of Germany     tiles=None,         # No background tiles initially     zoom_start=6,       # Initial zoom level     zoom_snap=0.25      # Allow finer zoom control )  # Set a clean white background m = set_background_color_of_map(m, color='#ffffff') In\u00a0[7]: Copied! <pre># Define the market price colormap legend (same as in mescal_302)\nprice_segments = {\n    (-500, -25): ['#000080'],               # Navy for extreme negative prices\n    (-25, 0): ['#0000FF', '#87CEFA'],       # Blue to light blue for negative prices\n    (0, 25): ['#00C300', '#FFFB00'],        # Light green to yellow for low positive prices\n    (25, 500): ['#FFFB00', '#FF9300'],      # Yellow to orange for high positive prices\n    (500, 10000): ['#FF0000']                # Red for extreme positive prices\n}\n\nprice_colorscale = SegmentedContinuousColorscale(segments=price_segments)\n\n# Add the colormap legend to the map\nContinuousColorscaleLegend(\n    mapping=price_colorscale,\n    title=\"Market Price (\u20ac/MWh)\",\n    width=350,\n    padding=20,\n    background_color=\"#FFFFFF\",\n    n_ticks_per_segment=2,\n    position={'bottom': 20, 'right': 20}\n).add_to(m)\n\n# Create area KPI area_price_increase_visualizer with the colormap\narea_generator = AreaGenerator(\n    AreaFeatureResolver(\n        fill_color=PropertyMapper.from_kpi_value(price_colorscale),\n        fill_opacity=1,\n        border_width=1.5,\n        highlight_border_width=2.0,\n    )\n)\n\n# Get KPI collection for scenarios\nkpi_collection = study.scen.get_merged_kpi_collection()\n\n# Generate feature groups for the map\nKPICollectionMapVisualizer(\n    generators=[\n        area_generator,\n        TextOverlayGenerator(),\n    ],\n).generate_and_add_feature_groups_to_map(kpi_collection, m)\n</pre> # Define the market price colormap legend (same as in mescal_302) price_segments = {     (-500, -25): ['#000080'],               # Navy for extreme negative prices     (-25, 0): ['#0000FF', '#87CEFA'],       # Blue to light blue for negative prices     (0, 25): ['#00C300', '#FFFB00'],        # Light green to yellow for low positive prices     (25, 500): ['#FFFB00', '#FF9300'],      # Yellow to orange for high positive prices     (500, 10000): ['#FF0000']                # Red for extreme positive prices }  price_colorscale = SegmentedContinuousColorscale(segments=price_segments)  # Add the colormap legend to the map ContinuousColorscaleLegend(     mapping=price_colorscale,     title=\"Market Price (\u20ac/MWh)\",     width=350,     padding=20,     background_color=\"#FFFFFF\",     n_ticks_per_segment=2,     position={'bottom': 20, 'right': 20} ).add_to(m)  # Create area KPI area_price_increase_visualizer with the colormap area_generator = AreaGenerator(     AreaFeatureResolver(         fill_color=PropertyMapper.from_kpi_value(price_colorscale),         fill_opacity=1,         border_width=1.5,         highlight_border_width=2.0,     ) )  # Get KPI collection for scenarios kpi_collection = study.scen.get_merged_kpi_collection()  # Generate feature groups for the map KPICollectionMapVisualizer(     generators=[         area_generator,         TextOverlayGenerator(),     ], ).generate_and_add_feature_groups_to_map(kpi_collection, m) <pre>KPICollectionMapVisualizer: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:00&lt;00:00, 515.44it/s]\n</pre> In\u00a0[8]: Copied! <pre># Define a new colormap for price increases\nprice_increase_colorscale = SegmentedContinuousColorscale(\n    segments={\n        (0, 10): colors.sequential.shades_of_pink,    # Pink for price increases\n        (-10, 0): colors.sequential.shades_of_cyan[::-1],  # Cyan for price decreases\n    },\n    nan_fallback='#A2A2A2',  # Grey for NA values\n)\n\n# Add the second colormap legend to the map\nContinuousColorscaleLegend(\n    mapping=price_increase_colorscale,\n    title='Price Increase (Vol Weighted) [\u20ac/MWh]',\n    background_color='white',\n    width=350,\n    position=dict(bottom=150, right=20),  # Position below the first legend\n    padding=20,\n).add_to(m)\n\n# Create a new area_price_increase_visualizer for comparison data\narea_price_increase_visualizer = AreaGenerator(\n    AreaFeatureResolver(\n        fill_color=PropertyMapper.from_kpi_value(price_increase_colorscale),\n        border_width=1.5,\n        highlight_border_width=1.5,\n        fill_opacity=1.0,\n    ),\n)\n\n# Filter KPI collection to include only \"Increase\" values\nkpi_collection = study.comp.get_merged_kpi_collection().get_filtered_kpi_collection_by_attributes(\n    value_operation='Increase'\n)\n\n# Generate feature groups for comparisons\nKPICollectionMapVisualizer(\n    generators=[\n        area_price_increase_visualizer,\n        TextOverlayGenerator(),\n    ]\n).generate_and_add_feature_groups_to_map(kpi_collection, m, show='last')\n</pre> # Define a new colormap for price increases price_increase_colorscale = SegmentedContinuousColorscale(     segments={         (0, 10): colors.sequential.shades_of_pink,    # Pink for price increases         (-10, 0): colors.sequential.shades_of_cyan[::-1],  # Cyan for price decreases     },     nan_fallback='#A2A2A2',  # Grey for NA values )  # Add the second colormap legend to the map ContinuousColorscaleLegend(     mapping=price_increase_colorscale,     title='Price Increase (Vol Weighted) [\u20ac/MWh]',     background_color='white',     width=350,     position=dict(bottom=150, right=20),  # Position below the first legend     padding=20, ).add_to(m)  # Create a new area_price_increase_visualizer for comparison data area_price_increase_visualizer = AreaGenerator(     AreaFeatureResolver(         fill_color=PropertyMapper.from_kpi_value(price_increase_colorscale),         border_width=1.5,         highlight_border_width=1.5,         fill_opacity=1.0,     ), )  # Filter KPI collection to include only \"Increase\" values kpi_collection = study.comp.get_merged_kpi_collection().get_filtered_kpi_collection_by_attributes(     value_operation='Increase' )  # Generate feature groups for comparisons KPICollectionMapVisualizer(     generators=[         area_price_increase_visualizer,         TextOverlayGenerator(),     ] ).generate_and_add_feature_groups_to_map(kpi_collection, m, show='last') <pre>KPICollectionMapVisualizer: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00&lt;00:00, 264.30it/s]\n</pre> In\u00a0[9]: Copied! <pre># Add layer controls\nfolium.LayerControl(collapsed=False, draggable=True).add_to(m)\n</pre> # Add layer controls folium.LayerControl(collapsed=False, draggable=True).add_to(m) Out[9]: <pre>&lt;folium.map.LayerControl at 0x31a44bb60&gt;</pre> In\u00a0[10]: Copied! <pre>renderer.show_folium(m)\n</pre> renderer.show_folium(m)"},{"location":"mescal-study-01/notebooks/mescal_304_simple_area_kpi_map/#mescal-303-interactive-geospatial-areakpimapvisualizer-with-folium","title":"MESCAL 303: Interactive Geospatial <code>AreaKPIMapVisualizer</code> with Folium\u00b6","text":"<p>This notebook demonstrates how to create interactive geospatial visualizations of electricity market data using the SegmentedColorMap module we explored in the previous notebook.</p>"},{"location":"mescal-study-01/notebooks/mescal_304_simple_area_kpi_map/#introduction","title":"Introduction\u00b6","text":"<p>Geospatial visualization is one of the most effective ways to communicate regional patterns in electricity market data. In this notebook, we'll create a map showing:</p> <ol> <li>Market prices by control area</li> <li>Price increases between scenarios</li> </ol> <p>We'll leverage our custom segmented colormaps to ensure clear visual differentiation between positive and negative values.</p>"},{"location":"mescal-study-01/notebooks/mescal_304_simple_area_kpi_map/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mescal and all requirements are installed.</p>"},{"location":"mescal-study-01/notebooks/mescal_304_simple_area_kpi_map/#load-study-data","title":"Load Study Data\u00b6","text":"<p>First, we load our study data which contains multiple scenarios of the German electricity system:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_simple_area_kpi_map/#adding-kpis-to-scenarios-and-comparisons","title":"Adding KPIs to Scenarios and Comparisons\u00b6","text":"<p>To create our visualizations, we need to calculate relevant KPIs for each scenario and for the comparisons between scenarios:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_simple_area_kpi_map/#setting-up-the-map","title":"Setting up the Map\u00b6","text":"<p>Now we'll create a basic Folium map centered on Germany:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_simple_area_kpi_map/#visualizing-market-prices-by-control-area","title":"Visualizing Market Prices by Control Area\u00b6","text":"<p>Using our custom segmented colormap from the previous notebook, we'll create a visualization of market prices across Germany's control areas:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_simple_area_kpi_map/#visualizing-price-increases-between-scenarios","title":"Visualizing Price Increases Between Scenarios\u00b6","text":"<p>Next, we'll add another layer showing the price differences between scenarios:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_simple_area_kpi_map/#adding-layer-controls-and-finalizing-the-map","title":"Adding Layer Controls and Finalizing the Map\u00b6","text":"<p>Finally, we'll add layer controls so users can toggle between different visualizations:</p>"},{"location":"mescal-study-01/notebooks/mescal_304_simple_area_kpi_map/#what-do-we-see","title":"What do we see?\u00b6","text":"<p>If you are watching the static image in the Jupyter Notebook, it is showing the \"Price Increase\" for the last scenario-comparison in the study (wind_200 vs base). Since all values are negative, we know that the increase to 200% wind lead to a price decrease in all control_areas, with the highest price reduction in 50Hertz control_area (North-East), and the smallest reduction in the EnBW control area (South-West).</p> <p>If you open the interactive version (either through showing m in the notebook, or by saving the map as an html and opening it in the browser), you can select the FeatureGroup in the map's menu on the top right and select the dataset (comparison) and indicator you want to project on the map. Besides projecting the \"Price Increase\" for each scenario-comparison, you can also show the \"Market Price\" per single scenario.</p>"},{"location":"mescal-study-01/notebooks/mescal_304_simple_area_kpi_map/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook, we've demonstrated how to create advanced geospatial visualizations of electricity market data using Folium and our custom SegmentedColorMap module. The key advantages of this approach are:</p> <ol> <li>Clear visual differentiation between positive and negative values</li> <li>Multiple map layers for different types of analysis (absolute prices and price changes)</li> <li>Custom legends that help interpret the data</li> <li>Interactive features like hover tooltips and layer toggling</li> </ol> <p>These techniques can be applied to any geospatial electricity market data, allowing for intuitive visualization of complex market dynamics across regions.</p> <p>In the next notebook, we'll explore how to create animated visualizations to show temporal patterns in market data.</p>"},{"location":"mescal-study-01/notebooks/mescal_307_country_plotter_util/","title":"MESCAL 307: Country Plotter Utility Module","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git\n        %cd mescal-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mescal -U\n        !pip install git+https://github.com/helgeesch/mescal-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git         %cd mescal-vanilla-studies/          !pip install git+https://github.com/helgeesch/mescal -U         !pip install git+https://github.com/helgeesch/mescal-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>Running locally, let's continue.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mescal-vanilla-studies')\n    os.chdir('/content/mescal-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mescal import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mescal-vanilla-studies')     os.chdir('/content/mescal-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mescal import StudyManager except ImportError:     raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport folium\n\nfrom mescal.utils.folium_utils import set_background_color_of_map, MapCountryPlotter\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nrenderer = ConditionalRenderer()\n</pre> import os import folium  from mescal.utils.folium_utils import set_background_color_of_map, MapCountryPlotter from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Create a basic map centered on Europe\nm = folium.Map(location=[46, 10], zoom_start=4.75, tiles=None)\n\n# Set a clean white background\nm = set_background_color_of_map(m, color='#ffffff')\n\n# Initialize the plotter (uses default geojson)\nplotter = MapCountryPlotter()\n\n# Create a feature group for our countries\ncountries_fg = folium.FeatureGroup(name=\"Selected Countries\")\n\n# Add some countries with default styling\nplotter.add_countries_to_feature_group(\n    countries_fg,\n    countries=[\"DE\", \"FR\", \"IT\", \"ES\"],\n)\ncountries_fg.add_to(m)\n\n# Let's add layer control\nfolium.LayerControl().add_to(m)\n\nrenderer.show_folium(m)\n</pre> # Create a basic map centered on Europe m = folium.Map(location=[46, 10], zoom_start=4.75, tiles=None)  # Set a clean white background m = set_background_color_of_map(m, color='#ffffff')  # Initialize the plotter (uses default geojson) plotter = MapCountryPlotter()  # Create a feature group for our countries countries_fg = folium.FeatureGroup(name=\"Selected Countries\")  # Add some countries with default styling plotter.add_countries_to_feature_group(     countries_fg,     countries=[\"DE\", \"FR\", \"IT\", \"ES\"], ) countries_fg.add_to(m)  # Let's add layer control folium.LayerControl().add_to(m)  renderer.show_folium(m) <p>The plotter looks up countries using standard ISO country codes (ISO-A2, ISO-A3, SOV-A3). This is particularly useful when working with data that uses these standard codes.</p> In\u00a0[5]: Copied! <pre># Create a map\ncustom_map = folium.Map(location=[60, 14], zoom_start=5, tiles=None)\n\n# Create feature group with custom styling\nnordic_fg = folium.FeatureGroup(name=\"Nordic Countries\")\n\n# Add Nordic countries with blue styling\nplotter.add_countries_to_feature_group(\n    nordic_fg,\n    countries=[\"NOR\", \"SE\", \"FI\", \"DK\"],\n    style={\n        \"fillColor\": \"#3388ff\",\n        \"color\": \"black\",\n        \"weight\": 1,\n        \"fillOpacity\": 0.4\n    }\n)\nnordic_fg.add_to(custom_map)\n\nfolium.LayerControl().add_to(custom_map)\n\nrenderer.show_folium(m)\n</pre> # Create a map custom_map = folium.Map(location=[60, 14], zoom_start=5, tiles=None)  # Create feature group with custom styling nordic_fg = folium.FeatureGroup(name=\"Nordic Countries\")  # Add Nordic countries with blue styling plotter.add_countries_to_feature_group(     nordic_fg,     countries=[\"NOR\", \"SE\", \"FI\", \"DK\"],     style={         \"fillColor\": \"#3388ff\",         \"color\": \"black\",         \"weight\": 1,         \"fillOpacity\": 0.4     } ) nordic_fg.add_to(custom_map)  folium.LayerControl().add_to(custom_map)  renderer.show_folium(m) <p>The style dictionary accepts the same parameters as Folium's GeoJSON styling. You can customize:</p> <ul> <li><code>fillColor</code>: The color to fill the country</li> <li><code>color</code>: The border color</li> <li><code>weight</code>: The border thickness</li> <li><code>fillOpacity</code>: The transparency of the fill color</li> </ul> <p>This flexibility allows you to create visually distinct regions or apply data-driven styling, which is perfect for market zonal analysis.Excluding Countries</p> In\u00a0[6]: Copied! <pre># Get GeoJSON for Germany\ngermany_gdf = plotter.get_geojson_for_country(\"DE\")\n\n# Create a focused map\ngermany_map = folium.Map(location=[51, 10], zoom_start=6, tiles=None)\n\n# Add Germany with custom styling\nfolium.GeoJson(\n    germany_gdf,\n    style_function=lambda x: {\n        \"fillColor\": \"#ff0000\",\n        \"color\": \"black\",\n        \"weight\": 2,\n        \"fillOpacity\": 0.6\n    }\n).add_to(germany_map)\n\nrenderer.show_folium(germany_map)\n</pre> # Get GeoJSON for Germany germany_gdf = plotter.get_geojson_for_country(\"DE\")  # Create a focused map germany_map = folium.Map(location=[51, 10], zoom_start=6, tiles=None)  # Add Germany with custom styling folium.GeoJson(     germany_gdf,     style_function=lambda x: {         \"fillColor\": \"#ff0000\",         \"color\": \"black\",         \"weight\": 2,         \"fillOpacity\": 0.6     } ).add_to(germany_map)  renderer.show_folium(germany_map) <p>This direct access to country GeoJSON is useful when you need to:</p> <ul> <li>Perform spatial analysis with country boundaries</li> <li>Combine country shapes with other geometries</li> <li>Add custom interactive elements to specific countries</li> <li>Calculate centroids or other geometric properties for positioning labels or markers</li> </ul> <p>Feel free to combine this module with the map created in any of the previous modules (<code>mescal_303, mescal_304, mescal_305, mescal_306</code>) to show neighboring countries in the same map.</p>"},{"location":"mescal-study-01/notebooks/mescal_307_country_plotter_util/#mescal-307-country-plotter-utility-module","title":"MESCAL 307: Country Plotter Utility Module\u00b6","text":"<p>This notebook demonstrates a simple but useful module to project countries from the country library to a folium map.</p>"},{"location":"mescal-study-01/notebooks/mescal_307_country_plotter_util/#module-use-case","title":"Module Use-Case\u00b6","text":"<p>There are two primary use-cases for the module:</p> <p>Using it as a library with geo data for countries. Using it to project countries that are not the focus of your study (e.g. neighboring countries) in a neutral color to your map.</p> <p>Whenever you are creating a KPI Map for your study (e.g. a map with European prices or anything alike), you often want to project the countries that are not part of your study itself (e.g. neighboring countries) to the map as well. You can, of course, also use Folium's native tile layers. However, those are often overloaded with unnecessary texts and geo information that distracts from your actual data. The module enables you to include other countries that are not the focus of your study, without distracting from the information you are trying to communicate.</p>"},{"location":"mescal-study-01/notebooks/mescal_307_country_plotter_util/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mescal and all requirements are installed.</p>"},{"location":"mescal-study-01/notebooks/mescal_307_country_plotter_util/#basic-usage","title":"Basic Usage\u00b6","text":"<p>The MapCountryPlotter class provides a simple interface to add country geometries to folium maps. It handles the loading of GeoJSON data and provides methods to easily add countries to feature groups.</p>"},{"location":"mescal-study-01/notebooks/mescal_307_country_plotter_util/#custom-styling","title":"Custom Styling\u00b6","text":"<p>One of the key benefits of using this plotter is the ability to customize the styling of countries. This is useful when you want to highlight specific regions or apply styling based on data values.</p>"},{"location":"mescal-study-01/notebooks/mescal_307_country_plotter_util/#working-with-individual-countries","title":"Working with Individual Countries\u00b6","text":"<p>Sometimes you need more direct access to country geometries. The `get_geojson_for_country method allows you to retrieve the raw GeoJSON for specific processing.</p>"},{"location":"mescal-study-01/notebooks/mescal_307_country_plotter_util/#conclusion","title":"Conclusion\u00b6","text":"<p>The MapCountryPlotter module is a simple but powerful tool for extending clean, informative geographical visualizations. By providing an easy way to add country shapes to your maps, it helps you to a) provide geo shapes for countries; b) focus on communicating your data rather than relying on the rather distractive default tile_layers in folium.</p>"},{"location":"mescal-study-01/notebooks/mescal_308_html_dashboards/","title":"MESCAL 305: HTML Dashboard Utility","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git\n        %cd mescal-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mescal -U\n        !pip install git+https://github.com/helgeesch/mescal-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mescal-vanilla-studies\") and is_module_available(\"mescal\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mescal-vanilla-studies.git         %cd mescal-vanilla-studies/          !pip install git+https://github.com/helgeesch/mescal -U         !pip install git+https://github.com/helgeesch/mescal-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>Running locally, let's continue.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mescal-vanilla-studies')\n    os.chdir('/content/mescal-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mescal import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mescal-vanilla-studies')     os.chdir('/content/mescal-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mescal import StudyManager except ImportError:     raise ImportError(\"\u274c 'mescal' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mescal-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport plotly.express as px\nimport folium\n\nfrom mescal.visualizations.html_dashboard import HTMLDashboard\nfrom mescal.utils.folium_utils import MapCountryPlotter\nfrom mescal.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nPlotlyTheme().apply()\nrenderer = ConditionalRenderer()\n</pre> import os import plotly.express as px import folium  from mescal.visualizations.html_dashboard import HTMLDashboard from mescal.utils.folium_utils import MapCountryPlotter from mescal.utils.plotly_utils.plotly_theme import PlotlyTheme from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() PlotlyTheme().apply() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Sample dataset\ndata = px.data.gapminder().query(\"year == 2007\")\n\n# Create first visualization: GDP vs Life Expectancy\nfig1 = px.scatter(\n    data, \n    x=\"gdpPercap\", \n    y=\"lifeExp\", \n    size=\"pop\", \n    color=\"continent\",\n    hover_name=\"country\", \n    log_x=True, \n    size_max=60,\n    title=\"GDP vs Life Expectancy (2007)\"\n)\nfig1.update_layout(height=500)\n\n# Create second visualization: Population by Continent\nfig2 = px.bar(\n    data.groupby(\"continent\")[\"pop\"].sum().reset_index(), \n    x=\"continent\", \n    y=\"pop\", \n    color=\"continent\",\n    title=\"Population by Continent (2007)\"\n)\nfig2.update_layout(height=500)\n\n# Create third visualization: Life Expectancy Distribution\nfig3 = px.box(\n    data, \n    x=\"continent\", \n    y=\"lifeExp\", \n    color=\"continent\",\n    title=\"Life Expectancy Distribution by Continent (2007)\"\n)\nfig3.update_layout(height=500);\n</pre> # Sample dataset data = px.data.gapminder().query(\"year == 2007\")  # Create first visualization: GDP vs Life Expectancy fig1 = px.scatter(     data,      x=\"gdpPercap\",      y=\"lifeExp\",      size=\"pop\",      color=\"continent\",     hover_name=\"country\",      log_x=True,      size_max=60,     title=\"GDP vs Life Expectancy (2007)\" ) fig1.update_layout(height=500)  # Create second visualization: Population by Continent fig2 = px.bar(     data.groupby(\"continent\")[\"pop\"].sum().reset_index(),      x=\"continent\",      y=\"pop\",      color=\"continent\",     title=\"Population by Continent (2007)\" ) fig2.update_layout(height=500)  # Create third visualization: Life Expectancy Distribution fig3 = px.box(     data,      x=\"continent\",      y=\"lifeExp\",      color=\"continent\",     title=\"Life Expectancy Distribution by Continent (2007)\" ) fig3.update_layout(height=500); In\u00a0[5]: Copied! <pre># Create a map centered in Europe\nm = folium.Map(location=[50, 10], zoom_start=3)\n\n# Initialize country plotter\nplotter = MapCountryPlotter()\n\n# Create a simple choropleth-like map with country data\ncountries = [\"DE\", \"FR\", \"IT\", \"ES\", \"GB\", \"SE\", \"NO\", \"FI\", \"PL\", \"RO\"]\ncolors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \n          \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\"]\n\n# Create feature group\nfg = folium.FeatureGroup(name=\"Europe\")\n\n# Add countries with different colors\nfor country, color in zip(countries, colors):\n    plotter.add_countries_to_feature_group(\n        fg,\n        countries=[country],\n        style={\n            \"fillColor\": color,\n            \"color\": \"white\",\n            \"weight\": 1,\n            \"fillOpacity\": 0.7\n        }\n    )\n\n# Add to map\nfg.add_to(m)\n\n# Add background countries\nbackground_fg = folium.FeatureGroup(name=\"Background\")\nplotter.add_all_countries_except(\n    background_fg,\n    excluded_countries=countries,\n    style={\"fillColor\": \"#f8f8f8\", \"color\": \"#e0e0e0\", \"weight\": 0.5, \"fillOpacity\": 0.5}\n)\nbackground_fg.add_to(m)\n\n# Add layer control\nfolium.LayerControl().add_to(m);\n</pre> # Create a map centered in Europe m = folium.Map(location=[50, 10], zoom_start=3)  # Initialize country plotter plotter = MapCountryPlotter()  # Create a simple choropleth-like map with country data countries = [\"DE\", \"FR\", \"IT\", \"ES\", \"GB\", \"SE\", \"NO\", \"FI\", \"PL\", \"RO\"] colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\",            \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\"]  # Create feature group fg = folium.FeatureGroup(name=\"Europe\")  # Add countries with different colors for country, color in zip(countries, colors):     plotter.add_countries_to_feature_group(         fg,         countries=[country],         style={             \"fillColor\": color,             \"color\": \"white\",             \"weight\": 1,             \"fillOpacity\": 0.7         }     )  # Add to map fg.add_to(m)  # Add background countries background_fg = folium.FeatureGroup(name=\"Background\") plotter.add_all_countries_except(     background_fg,     excluded_countries=countries,     style={\"fillColor\": \"#f8f8f8\", \"color\": \"#e0e0e0\", \"weight\": 0.5, \"fillOpacity\": 0.5} ) background_fg.add_to(m)  # Add layer control folium.LayerControl().add_to(m); In\u00a0[6]: Copied! <pre>from mescal.visualizations.html_table import HTMLTable\n\ntable = HTMLTable(\n    df=data,\n    title=\"Gapminder Dataset (2007)\",\n    height=\"500px\",\n    theme=\"modern\",\n    pagination=\"local\",\n    page_size=15,\n)\n</pre> from mescal.visualizations.html_table import HTMLTable  table = HTMLTable(     df=data,     title=\"Gapminder Dataset (2007)\",     height=\"500px\",     theme=\"modern\",     pagination=\"local\",     page_size=15, ) In\u00a0[7]: Copied! <pre># Create a new dashboard\ndashboard = HTMLDashboard(name=\"Global Development Dashboard\")\n\n# Add title section\ndashboard.add_section_divider(\n    title=\"Global Development Dashboard\",\n    subtitle=\"Analysis of development indicators across countries and continents\",\n    background_color=\"#f0f7fa\",\n    padding=\"30px\",\n    border_bottom=\"3px solid #3498db\"\n)\n\n# Add plotly figures\ndashboard.add_plotly_figure(fig1, height=\"500px\", name=\"gdp_vs_lifeexp\")\ndashboard.add_plotly_figure(fig2, height=\"500px\", name=\"population_by_continent\")\ndashboard.add_plotly_figure(fig3, height=\"500px\", name=\"lifeexp_distribution\")\n\n# Add geographic section\ndashboard.add_section_divider(\n    title=\"Geographic Visualization\",\n    subtitle=\"European country map example\",\n    background_color=\"#e8f4f9\"\n)\n\n# Add folium map directly using the new method\ndashboard.add_folium_map(m)\n\n# Add data section\ndashboard.add_section_divider(\n    title=\"Data Overview\",\n    subtitle=\"Sample of the dataset used for analysis\",\n    background_color=\"#f0f7ed\",\n    border_left=\"5px solid #2ecc71\"\n)\n\n# Add data table\ndashboard.add_table(table, name=\"data_table\")\n\n# Add footer\ndashboard.add_section_divider(\n    title=\"\",\n    subtitle=\"Created with MESCAL HTMLDashboard | 2025\",\n    background_color=\"#f9f9f9\",\n    padding=\"10px\",\n    subtitle_color=\"#888\",\n    border_top=\"1px solid #ddd\"\n)\n\n# Save the dashboard\n# dashboard.save('your/dashboard/export/path.html')\n\n# View dashboard\nrenderer.show_html_dashboard(dashboard)\n</pre> # Create a new dashboard dashboard = HTMLDashboard(name=\"Global Development Dashboard\")  # Add title section dashboard.add_section_divider(     title=\"Global Development Dashboard\",     subtitle=\"Analysis of development indicators across countries and continents\",     background_color=\"#f0f7fa\",     padding=\"30px\",     border_bottom=\"3px solid #3498db\" )  # Add plotly figures dashboard.add_plotly_figure(fig1, height=\"500px\", name=\"gdp_vs_lifeexp\") dashboard.add_plotly_figure(fig2, height=\"500px\", name=\"population_by_continent\") dashboard.add_plotly_figure(fig3, height=\"500px\", name=\"lifeexp_distribution\")  # Add geographic section dashboard.add_section_divider(     title=\"Geographic Visualization\",     subtitle=\"European country map example\",     background_color=\"#e8f4f9\" )  # Add folium map directly using the new method dashboard.add_folium_map(m)  # Add data section dashboard.add_section_divider(     title=\"Data Overview\",     subtitle=\"Sample of the dataset used for analysis\",     background_color=\"#f0f7ed\",     border_left=\"5px solid #2ecc71\" )  # Add data table dashboard.add_table(table, name=\"data_table\")  # Add footer dashboard.add_section_divider(     title=\"\",     subtitle=\"Created with MESCAL HTMLDashboard | 2025\",     background_color=\"#f9f9f9\",     padding=\"10px\",     subtitle_color=\"#888\",     border_top=\"1px solid #ddd\" )  # Save the dashboard # dashboard.save('your/dashboard/export/path.html')  # View dashboard renderer.show_html_dashboard(dashboard)"},{"location":"mescal-study-01/notebooks/mescal_308_html_dashboards/#mescal-305-html-dashboard-utility","title":"MESCAL 305: HTML Dashboard Utility\u00b6","text":"<p>This notebook demonstrates how to use the HTMLDashboard utility to combine multiple visualizations into a single HTML file for easy sharing and viewing.</p>"},{"location":"mescal-study-01/notebooks/mescal_308_html_dashboards/#introduction","title":"Introduction\u00b6","text":"<p>When working with multiple visualizations (Plotly figures, Folium maps, etc.), it can be cumbersome to have separate HTML files for each one. The HTMLDashboard utility provides a simple way to combine multiple visualizations into a single, well-structured HTML file that can be easily shared with stakeholders or published online.</p>"},{"location":"mescal-study-01/notebooks/mescal_308_html_dashboards/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mescal and all requirements are installed.</p>"},{"location":"mescal-study-01/notebooks/mescal_308_html_dashboards/#creating-plotly-visualizations","title":"Creating Plotly Visualizations\u00b6","text":"<p>Let's create some sample Plotly visualizations that we'll include in our dashboard:</p>"},{"location":"mescal-study-01/notebooks/mescal_308_html_dashboards/#creating-a-folium-map","title":"Creating a Folium Map\u00b6","text":"<p>Next, let's create a Folium map visualization:</p>"},{"location":"mescal-study-01/notebooks/mescal_308_html_dashboards/#creating-a-simple-data-table","title":"Creating a Simple Data Table\u00b6","text":"<p>Let's also create a simple HTML table to include in our dashboard:</p>"},{"location":"mescal-study-01/notebooks/mescal_308_html_dashboards/#building-the-dashboard","title":"Building the Dashboard\u00b6","text":"<p>Now let's combine all our visualizations into a single HTML dashboard:</p>"},{"location":"mescal-study-01/notebooks/mescal_308_html_dashboards/#use-cases-and-best-practices","title":"Use Cases and Best Practices\u00b6","text":"<p>The HTMLDashboard utility is particularly useful for:</p> <ol> <li>Sharing Results: Create comprehensive reports combining multiple visualizations</li> <li>Market Analysis: Build dashboards showing electricity prices, flows, and generation mixes</li> <li>Comparative Studies: Combine maps, charts, and tables to show scenario differences</li> <li>Static Reporting: Create standalone HTML files that don't require server infrastructure</li> </ol> <p>Best practices:</p> <ul> <li>Name your elements: Using meaningful names makes reordering easier</li> <li>Organize content logically: Group related visualizations together</li> <li>Consider load time: Many large visualizations may slow down page loading</li> <li>Use section headers: Divide your dashboard into clear sections with headings</li> <li>Add context: Include text explanations to help readers understand your visualizations</li> </ul>"},{"location":"mescal-study-01/notebooks/mescal_308_html_dashboards/#conclusion","title":"Conclusion\u00b6","text":"<p>The HTMLDashboard utility provides a simple yet powerful way to combine multiple visualizations into a single HTML file. This makes it easy to create comprehensive dashboards and reports that can be shared with stakeholders or published online.</p> <p>By leveraging HTML, Plotly, and Folium, you can create rich, interactive visualizations that work in any modern web browser without requiring special software or server infrastructure.</p>"}]}